[
  {
    "objectID": "chapter_4.html#引子",
    "href": "chapter_4.html#引子",
    "title": "4  数据的描述",
    "section": "4.1 引子",
    "text": "4.1 引子\n  记不太清楚是从什么时候开始，在很多青年学者的认识里描述性统计工作的重要性逐渐下降，越来越追求包含复杂模型的“高大上”研究，当然也包括我自己。简单分析之后，发现可能是这些年因果推断革命在社会科学领域的兴起，以及学术期刊对于“高大上”研究的偏好所引导的吧。\n  尽管这是现阶段很常见的一种现象，但是这并不代表数据描述在科研研究中就不重要了。通过回归模型获得因果推断证据的首要前提就是在现实世界中这个因果关系是客观存在的，而这就需要进行充分的数据描述和探索性分析去发现这种客观规律，如果数据描述过程中无法显示出较明显的因果关联，那么因果推断模型提示的显著性将值得怀疑。\n  数据的描述，或者是描述性统计分析，包含的内容其实比较简洁，就是数据的集中趋势（Central Tendency）、离散趋势（Variability）和分布（Frequency Distribution）。学习过统计学的人都知道均数标准差，但是想要做好一项描述性统计分析工作确是需要丰富的经验以及一定的耐心和精力，不过高质量的数据描述也会给整个研究带来非常大的收获。因此描述性统计分析也是探索性数据分析（Exploratory Data Analysis）的重要组成部分。\n  当然了，数据描述不单单只是为了弄清楚单个变量的集中、离散和分布情况，而是从杂乱的数据中，找到不同的变量之间的关联性，按照数据科学中的术语即是寻找个体不同属性或特征之间的关联性。尽管相关性不是因果关系，但是相关性往往是因果关系的前提，并且发现事物之间新的相关性也是十分有价值的，比如大家所熟知的“尿布与啤酒”的关系。\n  在本章中，依据数据的展现形式，我将数据描述分为表和图两大部分进行介绍。本来打算将数据可视化（Data Visualization）单独介绍，但是考虑到内容实在太多，精力有限无法面面俱到，又因这方面的数据和学习资料已经十分丰富了，故而放弃了单独介绍的想法。然而，一图胜千言，字不如表，表不如图，图形对于信息的展示能力是明显优于表格的，因此正确掌握可视化方法是描述统计一项重要的技能。\n\n\n\n\n\n\n本章就主要内容：\n\n\n\n\n\n\n数据的表格描述\n数据的可视化\n回归模型表格和图形的输出"
  },
  {
    "objectID": "chapter_4.html#数据的表格描述",
    "href": "chapter_4.html#数据的表格描述",
    "title": "4  数据的描述",
    "section": "4.2 数据的表格描述",
    "text": "4.2 数据的表格描述\n  在描述性统计分析中，表格常用于展示样本的基本信息和社会人口学特征，通常是一篇医学或者社会科学方向学术论文的第一个表格，一般也称之为Table One或者Summary Table。此外，也常用于不同组别之间研究所关注的结局变量分布情况的比较，比如展示参保病人和未参保病人的年卫生花费。\n  这一类表格具有共同的特点，一是展示的信息基本是固定的，对于连续型变量通常展示均数±标准差、中位数（四分位间距）、最大值、最小值等，对于分类变量通常展示频数（百分比）；二是展示的结果通常是需要通过聚合运算之后才能得出；三是重复工作量大。\n  这三个特点就使得完成这部分的描述工作简单但是费时，我想很多人应该都是通过利用统计软件分别计算得出每个需要描述的变量结果之后，然后再在MS Word或者MS Excel中进行手动整理。这样会带来一个明显的弊端，如果数据库发生变化，那么所有的工作均需要重新来过，再就是对于数据清洗和探索性分析时极其不友好1。在本书中就将会介绍如何快速地完成这项工作。\n\n4.2.1 R中表格描述的推荐工具\n  根据我自己的使用经验，整理出了几个在R中进行表格描述非常便利的工具，各个Package的特点可以在官方文档中进行查看，这里不赘述。近期我个人最推荐modelsummary、table1、gtsummary这三个包，以下内容也将利用它们分别进行演示。表 4.1 中统计描述是指支持自动完成变量的描述。\n\n\n表 4.1: R中支持输出符合统计规范表格的第三方包情况一览\n\n\n包名称\n支持功能\n推荐指数\n\n\n\n\nmodelsummary\n统计描述，回归模型\n***\n\n\ngtsummary\n统计描述，回归模型、自定义表格\n***\n\n\ntable1\n统计描述、自定义表格\n**\n\n\nstargazer\n回归模型\n*\n\n\ngt\n统计描述、自定义表格\n**\n\n\nkableExtra\n自定义表格\n**\n\n\nflextable\n自定义表格\n**\n\n\n\n\n\n\n4.2.2 快速生成Summary Table\n  这部分用卫生政策研究中常使用的公开微观数据库CHARLS进行演示，使用的是美国南加州大学社会经济研究中心（CESR）对原始数据进行整理后所提供的Harmonized CHARLS数据库。\n  首先，可以利用 Section 2.2.4 中提到的datadictionary包生成Harmonized CHARLS数据库的数据字典，以便于对数据库的基本信息进行概览。当然了，在实际的分析过程中，数据字典应该是在分析之前就已经准备好了，而数据概览操作通常使用str()函数或者glimpse()函数来进行，在这里用str()函数进行演示。从结果可以看出有50658条记录，19296个受访者。\n\nlibrary(datadictionary)\nlibrary(readr)\n\nharm <- \n  read_csv(\"dataset/harmonized_data_long_sub220522.csv\")\n\nstr(harm[, 1:10])\n\ntibble [50,658 × 10] (S3: tbl_df/tbl/data.frame)\n $ ...1       : num [1:50658] 1 2 3 4 5 6 7 8 9 10 ...\n $ ID         : num [1:50658] 1.01e+10 1.01e+10 1.01e+10 1.01e+10 1.01e+10 ...\n $ householdID: num [1:50658] 1.01e+08 1.01e+08 1.01e+08 1.01e+08 1.01e+08 ...\n $ communityID: num [1:50658] 101041 101041 101041 101041 101041 ...\n $ rdob       : Date[1:50658], format: \"1965-05-01\" \"1965-05-01\" ...\n $ ragender   : num [1:50658] 2 2 2 2 1 2 2 2 2 1 ...\n $ raeducl    : num [1:50658] 1 1 1 1 1 1 1 1 1 1 ...\n $ marriage   : num [1:50658] 1 1 1 1 1 1 1 1 1 1 ...\n $ wave       : num [1:50658] 1 2 3 4 1 1 2 3 4 4 ...\n $ ruralcounty: num [1:50658] 1 1 1 1 1 1 1 1 1 1 ...\n\n\n  这里简单选择4个典型的分类变量（ragender、raeducl、wave、ruralcounty），3个数值型变量（self_health、cesd10、mmse、hosfee）作为示例。由于选择的这四个分类变量在数据库中的类型为numeric2，这一点可以从以上的codebook结果中看出，因此为了方便进行后续的统计描述，建议先将分类变量的类型转换为character，代码如下。\n\nlibrary(dplyr)\nharm <- harm %>%\n    mutate_at(vars(c(\"ragender\", \"raeducl\", \n                     \"wave\", \"ruralcounty\")), \n              as.character)\n\n\n4.2.2.1 仅分类变量\n  只对分类变量进行描述的需求可能在论文撰写过程中不会经常出现，但是在对数据进行概览时，还是会经常需要。 表 4.1 中提到的package基本都具备此功能，但更推荐使用table1和gtsummary包。\n  如果只用于数据概览，不考虑输出成规范的表格，推荐使用modelsummary包中的datasummary_skim()函数，结果见 表 4.2 。\n\nlibrary(modelsummary)\n\ndatasummary_skim(harm[, c(\"ragender\", \"raeducl\", \n                          \"wave\", \"ruralcounty\")], \n                 type=\"categorical\")\n\n\n\n表 4.2:  利用modelsummary包对分类变量进行描述 \n \n  \n      \n       \n    N \n    % \n  \n \n\n  \n    ragender \n    1 \n    12800 \n    25.3 \n  \n  \n     \n    2 \n    37858 \n    74.7 \n  \n  \n    raeducl \n    1 \n    43984 \n    86.8 \n  \n  \n     \n    2 \n    5369 \n    10.6 \n  \n  \n     \n    3 \n    1297 \n    2.6 \n  \n  \n     \n    NA \n    8 \n    0.0 \n  \n  \n    wave \n    1 \n    11005 \n    21.7 \n  \n  \n     \n    2 \n    12102 \n    23.9 \n  \n  \n     \n    3 \n    13601 \n    26.8 \n  \n  \n     \n    4 \n    13950 \n    27.5 \n  \n  \n    ruralcounty \n    0 \n    22517 \n    44.4 \n  \n  \n     \n    1 \n    28141 \n    55.6 \n  \n\n\n\n\n\n\n  如果需要输出符合统计规范的表格，则推荐使用table1或者gtsummary包，结果见 表 4.3 。\n\nlibrary(table1)\nvars <- c(\"ragender\", \"raeducl\", \n          \"wave\", \"ruralcounty\", \n          \"self_health\", \"cesd10\",\n          \"mmse\", \"hosfee\")\n\nlbs <- c(\"性别\", \"受教育程度\", \n         \"访次\", \"城乡\", \n         \"自评健康\", \"认知水平\", \n         \"自我行动能力评分\", \"住院费用\")\n\nfor (i in 1:length(lbs)) {\n    label(harm[[vars[i]]]) <- lbs[i]\n}\n\ntable1(~ ragender + raeducl + \n         wave + ruralcounty, \n         data = harm)\n#---#\nlibrary(gtsummary)\ntbl_summary(harm[, c(\"ragender\", \"raeducl\", \n                     \"wave\", \"ruralcounty\")],\n           missing_text = \"(Missing)\",\n           label = list(ragender ~ \"Gender\",\n                        raeducl ~ \"Education\",\n                        wave ~ \"Wave\",\n                        ruralcounty ~ \"Rural or Not\"),\n           ) %>% as_gt()\n\n\n表 4.3: 利用table1和gtsummary包作仅分类变量描述表\n\n\n\n\n(a) table1包 \n\n\n\nOverall(N=50658)\n\n\n\n\n性别\n\n\n\n1\n12800 (25.3%)\n\n\n2\n37858 (74.7%)\n\n\n受教育程度\n\n\n\n1\n43984 (86.8%)\n\n\n2\n5369 (10.6%)\n\n\n3\n1297 (2.6%)\n\n\nMissing\n8 (0.0%)\n\n\n访次\n\n\n\n1\n11005 (21.7%)\n\n\n2\n12102 (23.9%)\n\n\n3\n13601 (26.8%)\n\n\n4\n13950 (27.5%)\n\n\n城乡\n\n\n\n0\n22517 (44.4%)\n\n\n1\n28141 (55.6%)\n\n\n\n\n\n\n\n\n\n(b) gtsummary包 \n  \n  \n    \n      Characteristic\n      N = 50,6581\n    \n  \n  \n    Gender\n\n        1\n12,800 (25%)\n        2\n37,858 (75%)\n    Education\n\n        1\n43,984 (87%)\n        2\n5,369 (11%)\n        3\n1,297 (2.6%)\n        (Missing)\n8\n    Wave\n\n        1\n11,005 (22%)\n        2\n12,102 (24%)\n        3\n13,601 (27%)\n        4\n13,950 (28%)\n    Rural or Not\n\n        0\n22,517 (44%)\n        1\n28,141 (56%)\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\n\n\n\n\n4.2.2.2 仅数值变量\n  modelsummary、table1、gtsummary均推荐。如果只用于数据概览，不考虑输出成规范的表格，推荐使用modelsummary包中的datasummary()或datasummary_skim()函数，其优点在于可以快速的了解数值型变量的分布情况，这两个函数之间其实是等价的，结果见 表 4.4 。如果需要将输出结果用于报告或者论文写作，方法同分类变量，此处不再举例说明。\n\ndatasummary(self_health + cesd10 + mmse +\n            hosfee ~ Mean + SD + Min + Max + Histogram,\n            data = harm,\n            output = \"default\" )\n\n\n\n表 4.4:  利用modelsummary包作仅数值变量描述表 \n \n  \n      \n    Mean \n    SD \n    Min \n    Max \n    Histogram \n  \n \n\n  \n    self_health \n    2.97 \n    0.97 \n    1.00 \n    5.00 \n    ▁▂▇▃▁ \n  \n  \n    cesd10 \n    8.37 \n    6.38 \n    0.00 \n    30.00 \n    ▇▆▄▃▂▂▁▁ \n  \n  \n    mmse \n    15.43 \n    5.03 \n    0.00 \n    30.00 \n    ▁▂▄▆▇▅▂▁ \n  \n  \n    hosfee \n    1724.53 \n    11988.19 \n    0.00 \n    1400000.00 \n    ▇ \n  \n\n\n\n\n\n\n\n\n4.2.2.3 分类和数值变量混合描述\n  将分类变量和数值变量同时进行描述是在论文写作中最常见的一种需求，推荐使用table1和gtsummary包。\n  table1，结果见 表 4.5 。\n\ntable1(~ ragender + raeducl + \n         ruralcounty + \n         self_health + cesd10 + \n         mmse + hosfee, \n         data = harm)\n\n\n\n表 4.5:  利用table1包生成分类和数值变量描述表（一） \n\n\n\nOverall(N=50658)\n\n\n\n\n性别\n\n\n\n1\n12800 (25.3%)\n\n\n2\n37858 (74.7%)\n\n\n受教育程度\n\n\n\n1\n43984 (86.8%)\n\n\n2\n5369 (10.6%)\n\n\n3\n1297 (2.6%)\n\n\nMissing\n8 (0.0%)\n\n\n城乡\n\n\n\n0\n22517 (44.4%)\n\n\n1\n28141 (55.6%)\n\n\n自评健康\n\n\n\nMean (SD)\n2.97 (0.966)\n\n\nMedian [Min, Max]\n3.00 [1.00, 5.00]\n\n\nMissing\n2440 (4.8%)\n\n\n认知水平\n\n\n\nMean (SD)\n8.37 (6.38)\n\n\nMedian [Min, Max]\n7.00 [0, 30.0]\n\n\nMissing\n3890 (7.7%)\n\n\n自我行动能力评分\n\n\n\nMean (SD)\n15.4 (5.03)\n\n\nMedian [Min, Max]\n16.0 [0, 30.0]\n\n\nMissing\n15173 (30.0%)\n\n\n住院费用\n\n\n\nMean (SD)\n1720 (12000)\n\n\nMedian [Min, Max]\n0 [0, 1400000]\n\n\nMissing\n951 (1.9%)\n\n\n\n\n\n\n\n  gtsummary，结果见 表 4.6 。\n\ntbl_summary(harm[, c(\"ragender\", \"raeducl\", \n                     \"wave\", \"ruralcounty\", \n                     \"self_health\", \"cesd10\",\n                     \"mmse\", \"hosfee\")],\n           missing_text = \"(Missing)\",\n           label = list(ragender ~ \"Gender\",\n                        raeducl ~ \"Education\",\n                        wave ~ \"Wave\",\n                        ruralcounty ~ \"Rural or Not\",\n                        self_health ~ \"Self report Health\",\n                        cesd10 ~ \"CESD 10\",\n                        mmse ~ \"MMSE\",\n                        hosfee ~ \"Hospital expense\"),\n           statistic = list(all_continuous() ~ \n                             \"{mean} ({sd})\")) %>% as_gt()\n\n\n\n\n\n表 4.6:  利用gtsummary包作分类和数值变量描述表（二） \n  \n  \n    \n      Characteristic\n      N = 50,6581\n    \n  \n  \n    Gender\n\n        1\n12,800 (25%)\n        2\n37,858 (75%)\n    Education\n\n        1\n43,984 (87%)\n        2\n5,369 (11%)\n        3\n1,297 (2.6%)\n        (Missing)\n8\n    Wave\n\n        1\n11,005 (22%)\n        2\n12,102 (24%)\n        3\n13,601 (27%)\n        4\n13,950 (28%)\n    Rural or Not\n\n        0\n22,517 (44%)\n        1\n28,141 (56%)\n    Self report Health\n\n        1\n4,626 (9.6%)\n        2\n6,770 (14%)\n        3\n24,569 (51%)\n        4\n9,759 (20%)\n        5\n2,494 (5.2%)\n        (Missing)\n2,440\n    CESD 10\n8 (6)\n        (Missing)\n3,890\n    MMSE\n15.4 (5.0)\n        (Missing)\n15,173\n    Hospital expense\n1,725 (11,988)\n        (Missing)\n951\n  \n  \n  \n    \n      1 n (%); Mean (SD)\n    \n  \n\n\n\n\n\n\n\n4.2.2.4 按变量分组后分类和数值变量混合描述\n  除上述几种情况之外，还有一种使用更频繁场景，那就是分组描述并进行统计检验，推荐使用gtsummary和table1包。\n  gtsummary，默认是对数值变量进行Wilcoxon rank sum test，分类变量进行Pearson’s Chi-squared test。结果见 表 4.7 。\n\ntbl_mix <- \n    tbl_summary(harm[, c(\"ragender\", \"raeducl\", \n                        \"wave\", \"ruralcounty\", \n                        \"self_health\", \"cesd10\",\n                        \"mmse\", \"hosfee\")],\n                by = ruralcounty,\n                missing_text = \"(Missing)\",\n                label = list(ragender ~ \"Gender\",\n                            raeducl ~ \"Education\",\n                            wave ~ \"Wave\",\n                            ruralcounty ~ \"Rural or Not\",\n                            self_health ~ \"Self report Health\",\n                            cesd10 ~ \"CESD 10\",\n                            mmse ~ \"MMSE\",\n                            hosfee ~ \"Hospital expense\"),\n                statistic = list(all_continuous() ~ \n                                \"{mean} ({sd})\"),  \n            ) \n                     \ntbl_mix %>% add_p() %>% as_gt()\n\n\n\n\n\n表 4.7:  利用gtsummary包作分组分类和数值变量描述表（一） \n  \n  \n    \n      Characteristic\n      0, N = 22,5171\n      1, N = 28,1411\n      p-value2\n    \n  \n  \n    Gender\n\n\n<0.001\n        1\n6,996 (31%)\n5,804 (21%)\n\n        2\n15,521 (69%)\n22,337 (79%)\n\n    Education\n\n\n<0.001\n        1\n17,409 (77%)\n26,575 (94%)\n\n        2\n3,905 (17%)\n1,464 (5.2%)\n\n        3\n1,197 (5.3%)\n100 (0.4%)\n\n        (Missing)\n6\n2\n\n    Wave\n\n\n0.8\n        1\n4,933 (22%)\n6,072 (22%)\n\n        2\n5,386 (24%)\n6,716 (24%)\n\n        3\n6,026 (27%)\n7,575 (27%)\n\n        4\n6,172 (27%)\n7,778 (28%)\n\n    Self report Health\n\n\n<0.001\n        1\n2,263 (11%)\n2,363 (8.8%)\n\n        2\n3,467 (16%)\n3,303 (12%)\n\n        3\n11,280 (53%)\n13,289 (49%)\n\n        4\n3,413 (16%)\n6,346 (24%)\n\n        5\n813 (3.8%)\n1,681 (6.2%)\n\n        (Missing)\n1,281\n1,159\n\n    CESD 10\n7 (6)\n9 (7)\n<0.001\n        (Missing)\n1,970\n1,920\n\n    MMSE\n16.6 (4.8)\n14.3 (5.0)\n<0.001\n        (Missing)\n5,216\n9,957\n\n    Hospital expense\n2,238 (15,889)\n1,317 (7,548)\n<0.001\n        (Missing)\n514\n437\n\n  \n  \n  \n    \n      1 n (%); Mean (SD)\n    \n    \n      2 Pearson's Chi-squared test; Wilcoxon rank sum test\n    \n  \n\n\n\n\n\n  如果想将默认的Wilcoxon rank sum test改成Student’s t-test，可以如下操作，结果见 表 4.8 。\n\nttest_func <- function(data, variable, by, ...) {\n\n    if (is.numeric(data[[variable]])) {\n        t.test(data[[variable]] ~ \n               as.factor(data[[by]])) %>%\n        broom::tidy() %>%\n        select(statistic, p.value)\n    } else {\n        chisq.test(table(data[[variable]], \n                         as.factor(data[[by]]))) %>%\n        broom::tidy() %>%\n        select(statistic, p.value)\n    }\n\n}\n\ntbl_mix %>%\n  add_stat(fns = everything() ~ ttest_func) %>%\n  as_gt()\n\n\n\n\n\n表 4.8:  利用gtsummary包作分组分类和数值变量描述表（二） \n  \n  \n    \n      Characteristic\n      0, N = 22,5171\n      1, N = 28,1411\n      statistic\n      p.value\n    \n  \n  \n    Gender\n\n\n722\n<0.001\n        1\n6,996 (31%)\n5,804 (21%)\n\n\n        2\n15,521 (69%)\n22,337 (79%)\n\n\n    Education\n\n\n3,364\n<0.001\n        1\n17,409 (77%)\n26,575 (94%)\n\n\n        2\n3,905 (17%)\n1,464 (5.2%)\n\n\n        3\n1,197 (5.3%)\n100 (0.4%)\n\n\n        (Missing)\n6\n2\n\n\n    Wave\n\n\n0.997\n0.8\n        1\n4,933 (22%)\n6,072 (22%)\n\n\n        2\n5,386 (24%)\n6,716 (24%)\n\n\n        3\n6,026 (27%)\n7,575 (27%)\n\n\n        4\n6,172 (27%)\n7,778 (28%)\n\n\n    Self report Health\n\n\n-22.9\n<0.001\n        1\n2,263 (11%)\n2,363 (8.8%)\n\n\n        2\n3,467 (16%)\n3,303 (12%)\n\n\n        3\n11,280 (53%)\n13,289 (49%)\n\n\n        4\n3,413 (16%)\n6,346 (24%)\n\n\n        5\n813 (3.8%)\n1,681 (6.2%)\n\n\n        (Missing)\n1,281\n1,159\n\n\n    CESD 10\n7 (6)\n9 (7)\n-37.2\n<0.001\n        (Missing)\n1,970\n1,920\n\n\n    MMSE\n16.6 (4.8)\n14.3 (5.0)\n42.6\n<0.001\n        (Missing)\n5,216\n9,957\n\n\n    Hospital expense\n2,238 (15,889)\n1,317 (7,548)\n7.92\n<0.001\n        (Missing)\n514\n437\n\n\n  \n  \n  \n    \n      1 n (%); Mean (SD)\n    \n  \n\n\n\n\n\n  table1，结果见 表 4.9 。\n\npvalue <- function(x, ...) {\n\n    y <- unlist(x)\n    g <- factor(rep(1:length(x), \n                   times = sapply(x, length)))\n    if (is.numeric(y)) {\n        p <- t.test(y ~ g)$p.value\n    } else {\n        p <- chisq.test(table(y, g))$p.value\n    }\n    c(\"\", sub(\"<\", \"&lt;\", \n              format.pval(p, digits = 3, \n                          eps = 0.001)))\n}\n#---#\ntable1(~ ragender + raeducl + \n         self_health + cesd10 + \n         mmse + hosfee | ruralcounty, \n         overall = FALSE, \n         extra.col = list(\"P-value\" = pvalue),\n         data = harm)\n\n\n\n表 4.9:  利用table1包生成分组后分类和数值变量描述表 \n\n\n\n0(N=22517)\n1(N=28141)\nP-value\n\n\n\n\n性别\n\n\n\n\n\n1\n6996 (31.1%)\n5804 (20.6%)\n<0.001\n\n\n2\n15521 (68.9%)\n22337 (79.4%)\n\n\n\n受教育程度\n\n\n\n\n\n1\n17409 (77.3%)\n26575 (94.4%)\n<0.001\n\n\n2\n3905 (17.3%)\n1464 (5.2%)\n\n\n\n3\n1197 (5.3%)\n100 (0.4%)\n\n\n\nMissing\n6 (0.0%)\n2 (0.0%)\n\n\n\n自评健康\n\n\n\n\n\nMean (SD)\n2.86 (0.940)\n3.06 (0.976)\n<0.001\n\n\nMedian [Min, Max]\n3.00 [1.00, 5.00]\n3.00 [1.00, 5.00]\n\n\n\nMissing\n1281 (5.7%)\n1159 (4.1%)\n\n\n\n认知水平\n\n\n\n\n\nMean (SD)\n7.16 (5.86)\n9.31 (6.61)\n<0.001\n\n\nMedian [Min, Max]\n6.00 [0, 30.0]\n8.00 [0, 30.0]\n\n\n\nMissing\n1970 (8.7%)\n1920 (6.8%)\n\n\n\n自我行动能力评分\n\n\n\n\n\nMean (SD)\n16.6 (4.80)\n14.3 (5.00)\n<0.001\n\n\nMedian [Min, Max]\n17.0 [0, 30.0]\n15.0 [0, 30.0]\n\n\n\nMissing\n5216 (23.2%)\n9957 (35.4%)\n\n\n\n住院费用\n\n\n\n\n\nMean (SD)\n2240 (15900)\n1320 (7550)\n<0.001\n\n\nMedian [Min, Max]\n0 [0, 1400000]\n0 [0, 250000]\n\n\n\nMissing\n514 (2.3%)\n437 (1.6%)\n\n\n\n\n\n\n\n\n  如果进行数据探索，可以使用modelsummary中的datasummary_balance()的函数快速进行组间比较及描述，结果见 表 4.10 。\n\ndatasummary_balance(~ ruralcounty,\n                    dinm = FALSE,\n                    dinm_statistic = \"p.value\",\n                    data = harm[, \n                           c(\"ragender\", \"raeducl\", \n                             \"wave\", \"ruralcounty\", \n                             \"self_health\", \"cesd10\",\n                             \"mmse\", \"hosfee\")])\n\n\n\n表 4.10:  利用modelsummary包作分组后分类和数值变量描述表 \n \n\n\n0 (N=22517)\n1 (N=28141)\n\n  \n      \n       \n    Mean \n    Std. Dev. \n    Mean \n    Std. Dev. \n  \n \n\n  \n    self_health \n     \n    2.9 \n    0.9 \n    3.1 \n    1.0 \n  \n  \n    cesd10 \n     \n    7.2 \n    5.9 \n    9.3 \n    6.6 \n  \n  \n    mmse \n     \n    16.6 \n    4.8 \n    14.3 \n    5.0 \n  \n  \n    hosfee \n     \n    2238.3 \n    15889.1 \n    1316.5 \n    7548.3 \n  \n  \n     \n     \n    N \n    Pct. \n    N \n    Pct. \n  \n  \n    ragender \n    1 \n    6996 \n    31.1 \n    5804 \n    20.6 \n  \n  \n     \n    2 \n    15521 \n    68.9 \n    22337 \n    79.4 \n  \n  \n    raeducl \n    1 \n    17409 \n    77.3 \n    26575 \n    94.4 \n  \n  \n     \n    2 \n    3905 \n    17.3 \n    1464 \n    5.2 \n  \n  \n     \n    3 \n    1197 \n    5.3 \n    100 \n    0.4 \n  \n  \n     \n    NA \n    6 \n    0.0 \n    2 \n    0.0 \n  \n  \n    wave \n    1 \n    4933 \n    21.9 \n    6072 \n    21.6 \n  \n  \n     \n    2 \n    5386 \n    23.9 \n    6716 \n    23.9 \n  \n  \n     \n    3 \n    6026 \n    26.8 \n    7575 \n    26.9 \n  \n  \n     \n    4 \n    6172 \n    27.4 \n    7778 \n    27.6 \n  \n\n\n\n\n\n\n\n\n\n4.2.3 回归模型的输出\n  回归结果的表格输出虽然不属于描述性分析的内容，但是与本节内容关联性较大，因此在此一并介绍了。推荐modelsummary包，相比于另一个用于回归模型输出的stargazer包而言，其支持的模型函数更全面，并且输出的文件类型也更全面，比如html、latex、word等，示例结果如 表 4.11。modelsummary包不仅仅只是将回归结果输出，也拥有较丰富的自定义功能，比如可以指定输出的回归变量、回归参数等。同时也可以直接通过vcov参数将值传递给sandwich包来实现稳健标准误的计算。关于其支持的模型类型和稳健标准误计算方法，可详见modelsummary 。\n\nlibrary(plm)\nlibrary(modelsummary)\n\ndata(\"Grunfeld\", package=\"plm\")\n\ngrun.fe <- plm(inv ~ value + capital, \n               index=c(\"firm\",\"year\"),\n               data = Grunfeld, model = \"within\")\n               \ngrun.re <- plm(inv ~ value + capital, \n               index=c(\"firm\",\"year\"),\n               data = Grunfeld, model = \"random\")\n\nmodelsummary(list(\"Fixed Effect\" = grun.fe,\n                  \"Random Effect\" = grun.re), \n             vcov = \"robust\",\n             stars = TRUE,\n             output = \"kableExtra\")\n\n\n\n表 4.11:  利用modelsummary包输出回归结果 \n \n  \n      \n    Fixed Effect \n    Random Effect \n  \n \n\n  \n    value \n    0.110*** \n    0.110*** \n  \n  \n     \n    (0.012) \n    (0.010) \n  \n  \n    capital \n    0.310*** \n    0.308*** \n  \n  \n     \n    (0.017) \n    (0.017) \n  \n  \n    (Intercept) \n     \n    −57.834* \n  \n  \n     \n     \n    (28.899) \n  \n  \n    Num.Obs. \n    200 \n    200 \n  \n  \n    R2 \n    0.767 \n    0.770 \n  \n  \n    R2 Adj. \n    0.753 \n    0.767 \n  \n  \n    AIC \n    2147.6 \n    2159.0 \n  \n  \n    BIC \n    2157.5 \n    2172.2 \n  \n  \n    RMSE \n    51.16 \n    52.39 \n  \n  \n    Std.Errors \n    HC3 \n    HC3 \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n\n\n4.2.4 小结\n  如果你和我一样有选择焦虑，可选项多了反而不知所措了，那么就直接看 表 4.12 。\n\n\n表 4.12: 支持输出符合统计规范表格的packages总结\n\n\n需求\n首推荐\n次推荐\n最次推荐\n\n\n\n\n描述表格\ngtsummary\ntable1\nmodelsummary\n\n\n回归结果\nmodelsummary\nstargazer\ngtsummary"
  },
  {
    "objectID": "chapter_4.html#数据的可视化",
    "href": "chapter_4.html#数据的可视化",
    "title": "4  数据的描述",
    "section": "4.3 数据的可视化",
    "text": "4.3 数据的可视化\n  统计图形的种类非常丰富，并且随着可视化工具的极大发展，目前数据可视化的展示方法越来越多样化。这样带来的好处是可以充分发挥出研究者的想象力，不好的地方是眼花缭乱的可视化方式容易造成研究人员选择困难，并且难以找到各类可视化方式的是用场景。在本节中，将进行总结并给出一定的示例，以下示例均基于R的ggplot2包实现。\n  统计图形可以归纳为四大类，这四类图形基本可以涵盖大部分的数据可视化需求。\n\n分布型：Distribution of a single variable\n关系型：Relationship between two variables\n构成型：Composition of a single or multiple variables\n对比型：Comparison between different categories/individuals\n\n\n4.3.1 分布型图形\n\n4.3.1.1 直方图\n  直方图（Histogram）是直观了解连续型变量分布情况的首选和最有效手段之一。一般情况下用于单个变量分布的观察，如 图 4.1 (a) ，但是若需要进行同一个变量不同组间，或是不同变量之间的比较时，亦可在同一个图形中展示，如 图 4.1 (b) 。\n\nlibrary(ggplot2)\n\nset.seed(2022)\ndf <- data.frame(\"value\" = c(rnorm(1000, 6, 2), \n                             rnorm(1000, 8, 4)),\n                 \"group\" = c(rep(\"Rural\", 1000), \n                             rep(\"Urban\", 1000))\n                )\n\n# Univariate\nggplot(df) +\n  geom_histogram(aes(x = value, \n                     y = ..density..),\n                     bins = 50,\n                     fill = \"blue\",\n                     color = \"black\") +\n  geom_density(aes(x = value), \n               alpha = 0.5, \n               size = 1.2,\n               color = \"red\") +\n  labs(x = \"Indicator\",\n       y = \"Density\") +\n  theme_bw()\n\n# Multivariates\nggplot(df) +\n  geom_histogram(aes(x = value, \n                     y = ..density.., \n                     fill = group),\n                     bins = 50,\n                     alpha = 0.5,\n                     position = \"identity\",\n                     color = \"black\") +\n  geom_density(aes(x = value, color = group), \n               alpha = 0.5, \n               size = 1.2) +\n  labs(x = \"Indicator\",\n       y = \"Density\") +\n  theme_bw() +\n    theme(legend.position = c(0.1, 0.85))\n\n\n\n\n\n\n\n(a) Univariate Histogram\n\n\n\n\n\n\n\n(b) Multivariates Histogram\n\n\n\n\n图 4.1: Histogram\n\n\n\n\n\n4.3.1.2 箱图和小提琴图\n  箱图（Boxplot）是另一种观察数据分布情况的统计图形，箱子的上横线表示上四分位数（Q75），下横线表示下四分位数（Q25），中间的横线为中位数（Median），上下延长线的末端表示中位数加减1.5倍的四分位间距，如 图 4.2 (a) ，箱图在识别异常值时可以提供很大的帮助。\n  箱图（Boxplot）的缺点也很明显，就是无法观测到数据的聚集和离散情况，如在哪一区间数据分布较为集中。为了弥补这一不足，可以将小提琴图（Violin plot）或是Jitter plot与箱图结合，也可以单独使用，如 图 4.2 (b) ，图中较胖的区域说明数据分布较为集中。\n\np <- \n    ggplot(df, aes(x = group, y = value, \n                   fill = group)) + \n    geom_boxplot(width = 0.1) +\n    theme_bw() +\n      theme(legend.position=\"none\")\np\n#---#\np +\n  geom_violin(trim = FALSE)+\n  geom_jitter(shape = 16, \n              alpha = 0.2,\n              position = position_jitter(0.2)) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(x = \"Group\", \n       y = \"Value\") +\n  theme_bw() +\n    theme(legend.position=\"none\")\n\n\n\n\n\n\n\n(a) Boxplot\n\n\n\n\n\n\n\n\n\n(b) Violin & Box & Jitter plot\n\n\n\n\n图 4.2: 箱图和小提琴图\n\n\n\n\n\n\n4.3.2 关系型图形\n  探索两个因素之间的关联性或是因果关系是社会科学研究中的关键问题，而这一类问题的分析起点就是通过图形探索两个因素之间是否存在相互依存关系，此时关系型图形的作用就非常大。常见的关系型图形包括散点图、线图以及热力图。\n\n4.3.2.1 散点图\n  散点图（Scatter Plot）最长应用的场景就是描述两个连续型变量之间的关系，当然有时也可用于连续型变量与分类变量之间，示例结果如 图 4.3 。\n\ndata(mtcars)\n#---#\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point() +\n  labs(x = \"Horsepower\",\n       y = \"Miles/(US) gallon\") +\n  theme_bw()\n\n#---#\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(x = \"Horsepower\",\n       y = \"Miles/(US) gallon\") +\n  theme_bw()\n\n`geom_smooth()` using formula 'y ~ x'\n\n#---#\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(shape = 15) +\n  labs(x = \"Horsepower\",\n       y = \"Miles/(US) gallon\") +\n  theme_bw()\n\n#---#\n# change shape, color, fill, size\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(shape=23, \n             fill = \"blue\", \n             color = \"darkred\", size=3) +\n  labs(x = \"Horsepower\",\n       y = \"Miles/(US) gallon\") +\n  theme_bw()\n\n\n\n\n\n\n\n(a) 基础散点图\n\n\n\n\n\n\n\n(b) 基础散点图加拟合线\n\n\n\n\n\n\n\n\n\n(c) 散点图改变Shape\n\n\n\n\n\n\n\n(d) 散点图改变Shape、Color等\n\n\n\n\n图 4.3: 散点图\n\n\n\n\n\n4.3.2.2 线图\n  线图（Line Plot）也是最常见的用于观察两个变量之间关系的统计图形之一，例如股市的K线图，用于观察股价与时间的关系。示例结果如 图 4.4 。\n\ndata(Orange)\n\nggplot(Orange) +\n    geom_line(aes(x = age, \n                  y = circumference, \n                  linetype = Tree)) +\n    theme_bw() +\n      theme(legend.position = \"bottom\")\n\n\n\n\n图 4.4: 线图\n\n\n\n\n\n\n4.3.2.3 热力图\n  热力图（Heatmap）是通过颜色的深浅来区分某个连续型变量在两个分类变量之间的集中和离散情况，在生物信息学中应用较为广泛。目前，ggplot2包支持简单的热力图可视化，若需求更高，如需要同时了解不同分类在连续型变量上的聚类情况，可以使用pheatmap包。\n\nlett <- LETTERS[1:20]\nweek <- paste0(\"Week\", seq(1,20))\ndata <- expand.grid(\"lett\" = lett, \"week\" = week)\ndata$value <- runif(400, 60, 100)\n \n# Heatmap \nggplot(data, aes(lett, week, fill = value)) + \n  geom_tile() +\n  scale_fill_distiller(palette = \"RdPu\")\n\n\n\n\n图 4.5: 热力图\n\n\n\n\n\n\n\n4.3.3 构成型图形\n  构成型图形相对比较简单，主要是饼图（Pie plot）。在ggplot2中，饼图的实现方法有两种思路，一是原始数据已经给出了某个分类的构成比，此时可以直接作图，二是需要经过计算之后方可获得构成比，如本书中的示例。一般而言，第二种情况更为常见。示例结果如 图 4.6 。\n\n\n\n\n\n\n好物推荐\n\n\n\n\n\n\n推荐一个支持自动填充符合不同学术期刊配色要求的包ggsci。\n其优点在于可以让你省去不少寻找RGB等配色的时间，其中预设了包括Lancet、JAMA、Nature等期刊常用的配色板。\n尽管未能覆盖全部的应用场景，但是适用面已经相对较广了。\n\n\n\n\n\nlibrary(dplyr)\ndata(mtcars)\n\ntable(mtcars$cyl) %>% \n  prop.table(.) %>%\n  as.data.frame(.) %>%\n  ggplot(aes(x = \"\", y = Freq *  100, fill = Var1)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(Freq * 100, \"%\")),\n            position = position_stack(vjust = 0.5)) +\n  labs(x = \"\", y = \"\") +\n  coord_polar(theta = \"y\") +\n  ggsci::scale_fill_jama(name = \"Cyl\") +\n  theme_bw() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n图 4.6: 饼图\n\n\n\n\n\n\n4.3.4 对比型图形\n  对比型图形主要是条形图（Bar Plot），但是实际研究过程中线图和散点图等均可以实现对比的分析需求，这里所讲的对比型图形主要是指在进行数据对比时更为常用和直观的图形。条形图也称作柱状图，其变化形式较多样，根据柱子的不同摆放方式，可以分为堆积条图（Stack），并排条图（Dodge），百分条图（Fill）。\n  三种不同类型的偏好场景略有不同，堆积条图和百分条图常用于展示计数资料，而并排条图既常用于计数变量也常用于计量变量。在ggplot2中，可以通过geom_bar()或geom_col()函数实现。条形图的变化非常多，本书中仅对常见的情况进行演示，其他的希望读者自行探索。\n\nggplot(mtcars, aes(x = factor(cyl), y = ..count..)) +\n  geom_bar(stat = \"count\", fill = \"blue\") +\n  labs(x = \"Cyl\", y = \"Count\") +\n  theme_bw()\n\n#---#\nmtcars %>% group_by(cyl, gear) %>%\n  summarise(mpg_mean = mean(mpg)) %>%\n  ggplot(aes(x = factor(cyl), \n             y = mpg_mean, \n             fill = factor(gear))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = mpg_mean), \n            vjust = -0.8,\n            size = 3,\n            position = position_dodge(0.9)) +\n  labs(x = \"Cyl\", y = \"Mpg\") +\n  ggsci::scale_fill_aaas(name = \"Gear\") +\n  theme_bw()  +\n    theme(legend.position = c(0.85, 0.8))\n\n#---#\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(aes(fill = factor(gear)), position = \"stack\") +\n  labs(x = \"Cyl\", y = \"Count\") +\n  ggsci::scale_fill_npg(name = \"Gear\") +\n  theme_bw() +\n    theme(legend.position = \"bottom\")\n\n#---#\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(aes(fill = factor(gear)), position = \"fill\") +\n  labs(x = \"Cyl\", y = \"Percentage\") +\n  ggsci::scale_fill_lancet(name = \"Gear\") +\n  theme_bw() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n(a) 基础条图\n\n\n\n\n\n\n\n(b) 并排条图\n\n\n\n\n\n\n\n\n\n(c) 堆积条图\n\n\n\n\n\n\n\n(d) 百分条图\n\n\n\n\n图 4.7: 条形图\n\n\n\n\n\n4.3.5 分面\n  一般而言，二维平面图形中最多能展示三个属性，假设有4个属性（Age、BMI、Sex、Region）想要一同展示在同一个图形中，常规方法是无法完成的，如横轴为Age，纵轴为BMI，那么只能利用颜色或者形状来区别Sex或者Region。当需要在一个图形中展现四种属性时，就需要利用到分面（Facet）功能，在ggplot2包中，可以利用 facet_wrap()和facet_grid()函数实现，后者多用于多个分面变量，使结果呈现为行列交叉的形式，结果如 图 4.8 。\n\nggplot(mtcars) +\n  geom_point(aes(x = hp, y = mpg, \n                 shape = factor(vs))) +\n  facet_wrap(~ cyl) +\n  labs(x = \"Horsepower\",\n       y = \"Miles/(US) gallon\") +\n  scale_shape_discrete(name = \"Engine\",\n                       breaks = c(0, 1),\n                       labels = c(\"V-shaped\", \n                                  \"Straight\")) +\n  theme_bw() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n图 4.8: 分面图\n\n\n\n\n\n\n4.3.6 空间可视化\n  关于空间可视化的内容十分复杂，其本来也是空间数据分析或者空间统计学的主要内容，想要介绍清楚需要很多的篇幅和精力。在本书中，将仅仅介绍在数据描述过程中最常用的空间填色图（Choropleth）。\n\n4.3.6.1 地图素材类型\n  空间可视化的基础素材就是想要研究的地区的地图图层，一般有三种常有类型：shp素材，geojson素材，地图包内置地图素材。\n  shp和geojson格式素材均可转换为以下两种地图数据格式：\n\nsp:SpatialPolygonDataFrame\nsf:Simple feature list column\n\n  这两种格式的数据集所描述的信息差不多是一致的。第一种格式（sp）是R语言绘图比较传统的数据格式，它将地理信息数据分割为两大块：描述层和映射层。在数据存放时，描述层记录各个地理区域的名称、ID、编号、简写、iOS编码，以及其他标识信息和度量变量，描述层是一个dataframe，我们可以用data@data来提取描述层的数据框。而对应的几何映射层，是每一个行政区域的多边形边界点，这些边界点按照order排序，按照group分组。多边形边界点信息是一个多层嵌套的list结构，但是我们仍然可以通过fortity()函数将其转化为数据框。即sp空间数据对象是一个dataframe（描述层）和polygons（几何映射层）两个对象的组合对象。\n  而sf对象将这种控件数据格式件进行了更加整齐的布局，使用st_read()导入的空间数据对象完全是一个整齐的数据框，拥有整齐的行列，这些行列中包含着数据描述和几何多边形的边界点信息。其中最大的特点是，它将每一个行政区划所对应的几何边界点封装成了一个list对象的记录，这条记录就像其他普通的文本记录、数值记录一样，被排列在对应行政区划描述的单元格中。\n\n\n4.3.6.2 地图来源\n  世界地图基本在很多支持空间分析的软件包中内置，虽然方便，但是这些地图基本会存在更新不及时导致的过于老旧问题。获得正确的地图素材是进行空间研究的基本前提。\n\n\n\n\n\n\n关于中国地图需要注意的\n\n\n\n\n\n\n在这里重点要指出的是在适用中国地图时，不论是商业出版还是发表学术成果，必须适用具有官方审图号的地图素材，否在会带来潜在的不必要麻烦。\n目前建议使用民政部网站提供的地图（http://xzqh.mca.gov.cn/map），其审图号为GS（2022）1873号。\n该地图中，县级地图数据不包括香港和澳门特别行政区，市级地图数据不包括台湾省。\n\n\n\n\n\n\n4.3.6.3 地图数据的读入方式\n  在R中目前有三种读取地图的方法，但是第一种方法目前不太推荐。\n\nsp::readShapePoly()\nrgdal::readOGR()\nsf::st_read()\n\n\n\n4.3.6.4 R中空间可视化常见三种方式\n  第一种，geom_polygon()函数。此方法必须通过@data及fortity()函数将描述层与几何层分别读取后，添加需要映射的变量，然后合并后进行画图。\n\nggplot(data = map_data) +\n    geom_polygon(aes(x=long, y=lat, \n               group=group, \n               fill = fill_var))\n\n  第二种，geom_map()函数。fill_file为包含需要作图变量的数据框，map_file 为地图素材，即通过直接读取的地图文件，使用geom_map()函数不需要进行合并，因为合并的过程由map_id所指定的参数自动进行合并，merge_id为fill_file中的识别变量。示例如 图 4.9 。\n\nids <- factor(c(\"1.1\", \"2.1\", \"1.2\", \n                  \"2.2\", \"1.3\", \"2.3\"))\nvalues <- data.frame(\n      ids = ids,\n      value = c(3, 3.1, 3.1, \n              3.2, 3.15, 3.5)\n    )\n    \npositions <- data.frame(\n      id = rep(ids, each = 4),\n      x = c(2, 1, 1.1, 2.2, 1, 0, 0.3, 1.1, \n          2.2, 1.1, 1.2, 2.5, 1.1, 0.3,\n            0.5, 1.2, 2.5, 1.2, 1.3, 2.7, \n          1.2, 0.5, 0.6, 1.3),\n      y = c(-0.5, 0, 1, 0.5, 0, 0.5, 1.5, 1, \n          0.5, 1, 2.1, 1.7, 1, 1.5,\n            2.2, 2.1, 1.7, 2.1, 3.2, 2.8, \n          2.1, 2.2, 3.3, 3.2)\n    )\n\nggplot(values, aes(fill = value)) + \n    geom_map(aes(map_id = ids, fill = value),\n           map = positions, color = \"red\") +\n    expand_limits(positions)\n\n\n\n\n图 4.9: 利用geom_map进行空间可视化\n\n\n\n\n\n\n\n\n\n\n注意\n\n\n\n\n\n\nmap_file中必须包含三个变量x或long、y或lat、region或id。\nmap_id指定的merge_id必须是能与region或id变量进行合并。\n\n\n\n\n  第三种，geom_sf()函数，此方法仍然需要一次合并过程，示例如 图 4.10 。\n\nlibrary(ggplot2)\n\nchina_map <- \n  sf::st_read(\"dataset/MCA_China_province.geojson\",\n              quiet = TRUE)\n\nnine_line <- \n  sf::st_read(\"dataset/MCA_Nine_line.geojson\",\n              quiet = TRUE)\n\nchina_map$pop <- \n  runif(length(china_map$NAME), 100, 1000)\n\n\nggplot() +\n  geom_sf(aes(fill = pop), data = china_map) +\n  geom_sf(data = nine_line)\n\n\n\n\n图 4.10: 利用geom_sf进行空间可视化\n\n\n\n\n  但是，绘制中国地图有一个比较特殊的地方。由于南海九段线会占用较大的绘图空间，导致图片比例不太协调，因此常以小图框的形式展示。结合一些必要的美化后，中国地图填色图的示例如 图 4.11 。\n\nlibrary(ggspatial)\nlibrary(cowplot)\n\nfig_main <-\n  ggplot() +\n    geom_sf(aes(fill = pop), data = china_map) +\n    geom_sf(data = nine_line) +\n    labs(caption = \"Map No.: GS(2022)1873\") +\n    scale_fill_gradient2(name = \"Population\",\n                        low = \"orange\", \n                        mid = \"yellow\", \n                        high = \"red\",\n                        n.breaks = 8,\n      guide = guide_legend(\n                        direction = \"horizontal\",\n                        title.position = 'top',\n                        title.hjust = 0.5,\n                        label.hjust = 1,\n                        nrow = 1,\n                        byrow = T,\n                        label.position = \"bottom\")) +\n    annotation_scale(location = \"bl\") +\n    annotation_north_arrow(location = \"tl\",\n                          height = unit(0.8, \"cm\"),\n                          width = unit(0.8, \"cm\")) +\n    coord_sf(ylim = c(1869414.769862395, \n                      7187874.74616931), \n             crs = 3857) +\n    theme_map() +\n      theme(legend.position = c(0.5, 0.85))\n\n\n#-南海九段线-#\nfig_nine <- \n  ggplot() +\n    geom_sf(aes(fill = pop), data = china_map) +\n    geom_sf(data = nine_line) +\n    scale_fill_gradient2(name = \"Population\",\n                        low = \"orange\", \n                        mid = \"yellow\", \n                        high = \"red\",\n                        n.breaks = 8) +\n    coord_sf(ylim = c(278392.10080518876, \n                      2991845.069153875),\n             xlim = c(11631734.185889415, \n                      13868701.579770062), \n             crs = 3857) +\n    theme_map() +\n      theme(legend.position = \"none\",\n            plot.margin = unit(c(0, 0, 0, 0), \"mm\"),\n            panel.border = element_rect(fill = NA,\n                                        color = \"grey10\",\n                                        linetype = 1,\n                                        size = 0.5)\n            )\n\n# 使用cowplot包将主图和小图框合并\nfig_china <- ggdraw(fig_main) +\n             draw_plot(fig_nine, \n                       x = 0.84, \n                       y = 0.058, \n                       width = 0.15, \n                       height = 0.45)\n\n# pdf(\"China map.pdf\", width = 10, height = 14)\nfig_china\n# dev.off()\n\n\n\n\n图 4.11: 利用geom_sf绘制中国地图\n\n\n\n\n\n\n4.3.6.5 R中的tmap包\n  R中支持空间可视化的第三方包选项还是十分丰富的，比如上文演示的ggplot2包，还有latticeExtra包、tmap包、leaflet包等，这些包各有优劣，一般综合起来使用。这里再推荐tmap包，其具有几点优点：\n\n与ggplot2包相比，在地理可视化方面其更加专业，可以提供比例尺、指北针等选项。当然结合ggspatial包，ggplot2也可以实现这些功能，如 图 4.11 。\n与leaflet包相比，其不仅同样支持reveal.js动态交互地图，还能输出符合学术要求的图片格式。\n\n\ntm_shape(china_map) +\n  tm_polygons(col = \"pop\", \n              pal = c(\"white\", \"skyblue\")) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"), \n               width = 0.1) +\n  tm_compass(position = c(\"right\", \"bottom\"),\n             size = 1) +\n  tm_layout(legend.width = 0.1)\n\n\n\n\n图 4.12: Tmap输出空间填色图\n\n\n\n\n4.3.6.6 关于坐标系\n  做空间可视化，坐标系是不可忽略的一个问题。投影的问题源于地球是球形，而地图是平面，因此就存在一个如何将球体转换为平面的问题，这里面的学问挺大的，我也讲不清楚就不展开了。投影可以理解为将地理坐标系转换到投影坐标系的过程，即将地球曲面转换为地图平面的过程。地理坐标系（Geographic Coordinate Systems）是球面坐标，由经度、纬度和高度组成，参考平面是椭球面。投影坐标系（Projected Coordinate Systems）是平面坐标系，参考平面是水平面。地理坐标系和投影坐标系统称为坐标参考系（Coordinate Reference System，CRS) 。只有当地理坐标系和投影坐标系采用同一个坐标系时，才能正确地将真实的地理坐标投影到地图中。\n  关于CRS只需要记住两个常用的就可以了：WGS84坐标系和伪墨卡托投影坐标系（Pseudo-Mercator）。\n\nWGS84坐标系：EPSG编号为43263，也就是以地心基准面，使用地球的质心作为原点，也称为地心大地坐标系，由卫星数据得到，美国GPS系统就是采用此坐标系。\n伪墨卡托投影坐标系：EPSG编号为3857，是基于墨卡托投影，把WGS84坐标系投影到正方形上的一种坐标系。伪墨卡托坐标系是擅长显示数据，但不适合存储数据的，因此通常使用WGS84存储数据，使用伪墨卡托显示数据。伪墨卡托投影最早由Google提出，并在其地图产品中广泛使用，目前绝大多数的在线地图均采用伪墨卡托投影。但是，凡是都有例外，显示中国境内地理信息的在线地图使用的是GCJ02经纬度投影坐标系，包括Google Map，因为Google Map中国大陆地区的数据是由高德提供的。这样不影响日常使用，但是当通过利用高德等提供的坐标编码等功能获取某地的坐标时，如全国的医院，得到的经纬度不是基于WGS84而是GCJ-02。\n\n  GCJ-02是由中国国家测绘局（国测局）制订的地理信息系统的坐标系统，本质是对WGS84坐标系统进行了加偏处理，在WGS84坐标基础上随机加上偏倚量，并且这个偏移量为随机的，各地的偏移情况都会有所不同，广大网友戏称“火星坐标系统”。\n\n\n4.3.6.7 关于在R中设置坐标系\n  一般可以使用sf包中的st_crs()函数，sf是R中开展空间分析的重要工具，对于空间对象的处理可非常丰富的函数。在读取完地图素材后，第一步就应该是查看其坐标参考系。例如，上文使用的china_map素材的自带CRS就是WGS84。\n\nsf::st_crs(china_map)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n  如果需要修改CRS，那么可以使用st_transform()函数，如下：\n\nchina_map_3875 = sf::st_transform(china_map, 3857)\nsf::st_crs(china_map_3875)\n\nCoordinate Reference System:\n  User input: EPSG:3857 \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\n\n  而对于还未设置CRS的空间对象，可以使用st_crs()函数来设置，如下：\n\nwaterfalls <- \n  data.frame(\"name\" = c(\"Iguazu Falls\", \"Niagara Falls\",\n                       \"Victoria Falls\"), \n             \"lat\" = c(-25.686785, 43.092461, -17.931805), \n             \"lon\" = c(-54.444981, -79.047150, 25.825558))\n\nwaterfalls_sf <- sf::st_as_sf(waterfalls, \n                              coords = c(\"lon\", \"lat\"))\n\nsf::st_crs(waterfalls_sf) <- 3875\n\n  此外，在geom_polygon和geom_map方法中，也可使用coord_map()函数指定坐标投影系，第三种可以使用coord_sf()函数，其中有参数crs用于指定投影方式，需要使用proj4string格式的字符串。如墨卡托投影用crs = “+proj=merc”。某些地图投影其字符串中包含经纬度等参数，如crs = “+proj=laea +lat_0=35 +lon_0=-100”。其它地图投影见Projection methods。crs本身包含坐标数据的，就不能再增加xlim, ylim等参数。\n\n\n\n\n\n\n小结\n\n\n\n\n\n\n以上示例展示的只是较为常见的统计图形可视化方法，所举示例也是相对比较简单，只是针对数据分析过程中较为常见的场景，并未能覆盖大部分乃至全部需求。数据可视化是一个充满了创新和创意的领域，读者可以根据自身需求和兴趣大胆尝试和探索。\n如果你对基于ggplot2包的空间可视化感兴趣，Timo Grossenbacher的Blog也许对你有帮助。\n没有画不出好看图的工具，只有画不出好看图的人，创意和探索非常重要。"
  },
  {
    "objectID": "chapter_7.html#引子",
    "href": "chapter_7.html#引子",
    "title": "7  番外篇（二）：匹配",
    "section": "7.1 引子",
    "text": "7.1 引子\n\n“The goal of matching is to reduce imbalance in the empirical distribution of the pre-treatment confounders between the treated and control groups.”\n\n\n\n—– Stuart, Elizabeth A. (2010)"
  },
  {
    "objectID": "chapter_7.html#背景介绍",
    "href": "chapter_7.html#背景介绍",
    "title": "7  番外篇（二）：匹配",
    "section": "7.2 背景介绍",
    "text": "7.2 背景介绍\nCoarsened Exact Matching (CEM) 方法由University of Milan的Stefano M. Iacus，Harvard University的Gary King， 以及University of Trieste的Giuseppe Porro提出，其算法最早于2008年在线发表在Gary King的Harvard University主页上 “Matching for Causal Inference Without Balance Checking.”。\n其后分别在 Journal of Statistical Software (2009) 和 The Stata Journal (2009) 上发表了R和Stata版本的相关package， 其正式成果于2011年发表于 Journal of the American Statistical Association 上，以及2012的Political Analysis上。\nCEM亦可称之为“Cochran Exact Matching” ，衍生于Cochran于1986年提出的subclassification-based method （Cochran, W. G., 1968)，在2011年发表的论文中Gary King等人亦将CEM与PSM (Propensity Score Matching)进行了比较，提出了CEM的优势。"
  },
  {
    "objectID": "chapter_7.html#psm与cem",
    "href": "chapter_7.html#psm与cem",
    "title": "7  番外篇（二）：匹配",
    "section": "7.3 PSM与CEM",
    "text": "7.3 PSM与CEM\n在CEM方法提出之前，已有较多的匹配方法，其中最具有代表性的就是 Paul R. Rosenbaum and Rubin (1983) 提出的Propensity score matching (PSM)， 截至目前，使用PSM发表的论文已超过10000篇，是目前最常用的匹配方法。从下图中也能看出两种方法在发表论文中使用的差距。Gary King在一篇Working paper (“Why Propensity Scores Should Not Be Used for Matching”)中指出了PSM的不足，原文如下：\n\nWe show here that PSM, as it is most commonly used in practice (or with many of the refinements that have been proposed by statisticians and methodologists), increases imbalance, inefficiency, model dependence, research discretion, and statistical bias at some point in both real data and in data generated to meet the requirements of PSM theory. In fact, the more balanced the data, or the more balanced it becomes by pruning some observations through matching, the more likely PSM will degrade inferences — a problem we refer to as the PSM paradox. If one’s data are so imbalanced that making valid causal inferences from it without heavy modeling assumptions is impossible, then the paradox we identify is avoidable and PSM will reduce imbalance but then the data are not very useful for causal inference by any method.\n\n也许只有名字里带King的人论文题目敢这么写，当然，不出你所料，自然也会有这样一篇文章“Why propensity scores should be used for matching” (Ben Jann, 2017). ,至于CEM与PSM孰优孰劣，只能依据个人研究自行判断了。"
  },
  {
    "objectID": "chapter_7.html#算法",
    "href": "chapter_7.html#算法",
    "title": "7  番外篇（二）：匹配",
    "section": "7.4 算法",
    "text": "7.4 算法\nCEM没有PSM那么复杂的反事实假设，其算法一共可分为三步：\n\n将所有纳入匹配的协变量，粗化（coarsen）成离散分组，对于已是分类变量的协变量，可以自行设定（比如，性别、教育程度等），而对于连续型变量 CEM算法会根据自动频率分布直方图进行自动粗化，当然也可以自行手动设定cutpoint,s, 并且可以设定自动粗化的方法(Sturges’ rule, the default), “fd” (Freedman-Diaconis’ rule), “scott” (Scott’s rule) and “ss” (ShimazakiShinomoto’s rule)。\n对所有粗化之后的分类进行分层并排序\n删除未同时包含至少一个Treat组和Control组的层"
  },
  {
    "objectID": "chapter_7.html#不平衡度测量",
    "href": "chapter_7.html#不平衡度测量",
    "title": "7  番外篇（二）：匹配",
    "section": "7.5 不平衡度测量",
    "text": "7.5 不平衡度测量\nCEM中引入了一个参数L1来衡量Treat组和Control组之间在协变量上的不平衡度，其计算公式如下：\n\nL1(f, g) = ∑|fe(1…k)−ge(1…k)|\n\nL1取值在0~1之间，0代表完全平衡，1代表完全不平衡。若L1为0.6，即说明有40%的粗化后各层的频率分布 直方图在Treat组和Control组之间是重叠的，L1即是根据各层的相对频率差值求和而得，示例如下：\n假设有三个协变量（X1…X），粗化后个协变量的分类为（2, 3, 5），那么粗化后共有235=30个层， 我们随机为Treat组和Control组在360个层生成一个正整数，最后计算频率并画出直方图。\n\nlibrary(magrittr)\nlibrary(ggplot2)\n# Set seed\nset.seed(2019)\n\n# Data\nstrate <- sample(paste(\"str\", c(1:30), sep = \"_\"), size = 2000, replace = TRUE)\ngroup <- rep(c(\"Treat\", \"Control\"), 1000)\n\nimb <- data.frame(strate = strate, group = group)\nimb_sum <- prop.table(table(imb$group, imb$strate), 1) %>% as.data.frame()\n\n# Plot\nggplot(imb_sum) +\n     geom_bar(aes(x = Var2, y = Freq, fill = Var1), position = \"dodge\", stat = \"identity\") +\n     scale_fill_manual(name = \"\", values = c(3, 2)) +\n     labs(y = \"Prop\", x = \"Strates\") +\n     theme_bw()"
  },
  {
    "objectID": "chapter_7.html#匹配",
    "href": "chapter_7.html#匹配",
    "title": "7  番外篇（二）：匹配",
    "section": "7.6 匹配",
    "text": "7.6 匹配\n采用R语言中的cem包，示例的dataset为cem包中自带的LeLonde\n\n7.6.1 示例的dataset介绍\nOutcome variable: re78\nTreat variable: treated, 1 = treat group, 0 = control group\nControl variable: c(“age”, “education”, “black”, “married”, “nodegree”, “re74”, “re75”, “hispanic”, “u74”, “u75”,“q1”)\n\nlibrary(cem)\ndata(LeLonde)\ndf <- LeLonde[c(\"treated\", \"re78\", \"age\", \n                \"education\", \"black\", \"married\", \n                \"nodegree\", \"re74\", \"re75\",\n                \"hispanic\", \"u74\", \"u75\", \"q1\")] %>%\n        na.omit()\n       \nhead(df)\n\n\n\n7.6.2 匹配前准备：不平衡度测量\n利用 imbalance() 函数对匹配的数据的不平衡进行测量，如下结果所示，整体不平衡度为0.902， 意味数据存在较高的不平衡性。\n对于连续型变量，默认计算mean in difference，对于分类变量默认计算chi square。\n\nimbalance(group = df$treated, data = df[, -c(1, 2)])\n\n\n\n7.6.3 开始匹配\n利用 cem() 函数进行CEM匹配，参数treatment用来指定分组变量，drop用来排除结局变量。\n从结果可以看出，Control组从全部392个样本中匹配上95例，Treat组从全部258个样本中匹配上84例，匹配后样本的整体 L1为0.605，相比匹配前，有所下降。另外，从statistic列的结果也可看出，在各匹配变量中两组之间无统计学差异。\n\nmat <- cem(treatment = \"treated\", data = df, drop = \"re78\", eval.imbalance = TRUE)\nmat\n\n然而，此处我产生一个小疑惑，纳入匹配变量的数据类型是否会影响粗化分组过程，从而影响匹配结局？\n因此，我将black、married等分类变量设置成factor类型，比较前后不平衡度测量及匹配结果。\n区别，比较结果看出，与前述结果并无差异，仅在测量不平衡度时对于分类变量采用了chi square.\n另一个明显的区别在于，对于设置成分类变量后，在进行匹配时，不会再对分类变量进行粗化。\n\ndf_1 <- df\ndf_1$black <- as.factor(df_1$black)\ndf_1$married <- as.factor(df_1$married)\ndf_1$nodegree <- as.factor(df_1$nodegree)\n\nimbalance(group = df$treated, data = df_1[, - c(1, 2)])\n\n\ncem(treatment = \"treated\", data = df_1, drop = \"re78\", eval.imbalance = TRUE)\n\n\n\n7.6.4 匹配后处理\n匹配后生成的匹配对象（mat），其类为 cem.match，其属性实为 list。\n\n7.6.4.1 mat的数据结构如下:\n其中详细记录了匹配结果与参数，需要关注的有breaks，matched，w三个对象中的信息。\n\nbreaks为一个list，其中记录匹配变量自动粗化过程中设置的cutpoint（仅包含numeric类型）\n\nmatched实为一个逻辑向量，记录了该个体是否进入了匹配后的样本\n\nw为匹配权重（详细见5.5），用于后续的统计分析中\n\n\nstr(mat)\n\n\n\n7.6.4.2 提取匹配后的样本:\nCEM包中给出了不用单独提取出匹配后样本进行回归的函数 att(), 不过我个人比较倾向将匹配后的样本单独存储为一个对象， 但是 CEM包中并未给出像 MatchIt中的 match.out()函数，至少我还没有找到，所以只能自己动手，丰衣足食\n\n# 提取匹配结果\nmat$matched\n\n# 提取匹配后的样本\ndf_matched <- cbind(df, mat$w, mat$matched)[which(mat$matched),]\n\n\n\n\n7.6.5 自行设定粗化的cutpoint\n对于连续型变量或者类别较多的分类变量，可以通过 cem()函数 中cutpoints和grouping两个参数来设定粗化的分割点，以下以cutpoints为例;\n从上述匹配后的结果中可以看出age变量的自动cutpoint为 17, 20.8, 24.6, 28.4, 32.2, 36, 39.8, 43.6, 47.4, 51.2, 55\n 如果是采用算法自动设定cutpoint， 可以通过 cem()函数 中L1.breaks = “fd”等（见3. 算法）来选择不同的方法。\n\n如果自行设定，需要通过cutpoints = list(education = c(0, 6.5, 8.5, 12.5, 17))来设定，即通过list形式为需要的匹配变量 赋值一个数值向量。\ngrouping参数赋值同理，list(c(“strongly agree”, “agree”), c(“neutral”,“no opinion”), c(“strongly))，不过我个人比较习惯提前使用factor() 设定好分类。\n\n\n\n7.6.6 权重weights的应用\n由于CEM为不对称匹配，当一个Treat样本匹配多个Control样本时，需要通过权重来更准确的估计平均处理效应(ATT)，Gary King的原话如下：\n\nThey enable us to use a calculation trick that makes it easy to estimate the ATT in a weighted least squares regression program without the involved procedure.\n\n关于权重需要注意的三点：\n\n当匹配为不对称时，对匹配后的样本进行的所有的统计分析，都应对权重进行加权，在PSM中进行一对多匹配时同理\nCEM中权重的理解十分简单，未匹配上的个案权重全为0， 匹配上的Treat组个案权重都为1， 匹配上的Control组个案的权重是对粗化后各层内 Treat和Control组的样本比与全部样本中Treat和Control组的样本比相乘而来((m_C/m_T)*Ws)\n匹配后样本的权重之和就等于匹配后样本量的大小，如本例中sum of weigths = sample of matched = 179\n\n关于权重的具体计算方法，详见An Explanation for CEM Weights （需要科学上网）\n\n\n7.6.7 k2k进行1:1匹配\n虽然CEM的优势在于可以进行非对称匹配，从而保留更多的样本，但是当样本量比较充足时，为了保证更准确的估计ATT，可以 进行1:1匹配，cem() 包 也给出了对应的函数 k2k() ，示列如下：\n\n# 用单独的k2k函数时，之前生成cem对象mat时，必须加上keep.all = TRUE参数\n# mat2 <- k2k(obj = mat, data = df, method = \"euclidean\", mpower = 1)\n\n# 或\n\nmat2 <- cem(treatment = \"treated\", data = df_1, drop = \"re78\",\n             eval.imbalance = TRUE, k2k = TRUE, method = \"euclidean\", mpower = 1)\n\n进行1:1匹配时，实际采用最近距离法在各层内选取，判断距离的方法可选（‘euclidean’, ‘maximum’, ‘manhattan’, ‘canberra’, ‘binary’ and ‘minkowski’)，默认为NULL，即随机选取。\n 需要注意的一点是，使用k2k进行1:1匹配后，后续统计分析时就无需进行权重加权了。"
  },
  {
    "objectID": "chapter_7.html#参考文献",
    "href": "chapter_7.html#参考文献",
    "title": "7  番外篇（二）：匹配",
    "section": "7.7 参考文献",
    "text": "7.7 参考文献\n\nStuart, Elizabeth A. (2010): “Matching Methods for Causal Inference: A Review and a Look Forward”. In: Statistical Science, no. 1, vol. 25, pp. 1–21.\nIacus SM, King G, Porro G (2008). “Matching for Causal Inference Without Balance Checking.” Submitted, URL http://gking.harvard.edu/files/abs/cem-abs.shtml.\nStefano M Iacus, Gary King, and Giuseppe Porro. 2009. “CEM: Software for Coarsened Exact Matching.” Journal of Statistical Software, 30.\nMatthew Blackwell, Stefano Iacus, Gary King, and Giuseppe Porro. 2009. “CEM: Coarsened Exact Matching in Stata.” The Stata Journal, 9, Pp. 524–546.\nStefano M Iacus, Gary King, and Giuseppe Porro. 2011. “Multivariate Matching Methods That are Monotonic Imbalance Bounding.” Journal of the American Statistical Association, 106, 493, Pp. 345-361.\nStefano M. Iacus, Gary King, and Giuseppe Porro. 2012. “Causal Inference Without Balance Checking: Coarsened Exact Matching.” Political Analysis, 20, 1, Pp. 1–24. Website Copy at http://j.mp/2nRpUHQ\nRosenbaum, Paul R. and Donald B. Rubin (1983): “The Central Role of the Propensity Score in Observational Studies for Causal Effects”. In: Biometrika, vol. 70, pp. 41–55.\nCochran, W. G. (1968), “The Effectiveness of Adjustment by Subclassification in Removing Bias in Observational Studies,” Biometrics, 24, 295–313 [350].\nWhy propensity scores should be used for matching,” German Stata Users’ Group Meetings 2017 01, Stata Users Group."
  },
  {
    "objectID": "chapter_8.html#引子",
    "href": "chapter_8.html#引子",
    "title": "8  番外篇（三）：主成分分析",
    "section": "8.1 引子",
    "text": "8.1 引子\n  主成分分析（Principal component analysis，PCA）是由Pearson在1901年提出的，后来被Hotelling在1933进行了发展。PCA目的是把多个相关的原始变量降维成少数几个不相关的主成分，其基本原理是原始变量的线性组合。"
  },
  {
    "objectID": "chapter_8.html#进行主成分分析的步骤",
    "href": "chapter_8.html#进行主成分分析的步骤",
    "title": "8  番外篇（三）：主成分分析",
    "section": "8.2 进行主成分分析的步骤",
    "text": "8.2 进行主成分分析的步骤\n\n8.2.1 数据预处理\n  原始数据矩阵或者相关系数矩阵都可以进行分析，需要注意的几点：\n\n原始数据矩阵实际是计算协方差矩阵进行的主成分分析。\n原始数据矩阵只适用于度量单位相同，或者差别不大时进行分析，如果差别大可以进行Z得分转换。\n不论是原始数据矩阵还是相关系数矩阵，都不能有缺失值。\n\n\n\n8.2.2 确定主成分个数\n  在计算主成分得分之前，需要先判断主成分个数，通常有四种准则：\n\n根据先验经验或者理论知识。\nKaiser-Harris准则，只保留特征值大于1的主成分。\nCattell碎石检验，通过绘制特征值与主成分数的图形，只保留在图形变化最大处之上的主成分。\n平行分析，依据与初始矩阵相同大小的随机数据矩阵来判断要提取的特征值，若基于真实数据的某个特征值大于一组随机数据矩阵相 应的平均特征值，那么该主成分可以保留。\n\n  通常需要结合四种准则，综合判断主成分个数。\n\n\n8.2.3 计算主成分\n  在R语言中既可以使用base包中的princomp()函数，也可以使用psych包中的principal()函数。\n\n\n8.2.4 旋转主成分\n  旋转是一系列将成分载荷阵变得更容易解释的数学方法，它们尽可能地对成分去噪。旋转方法有两种：\n\n正交旋转：使选择的成分保持不相关\n斜交旋转：和让它们变得相关\n\n  旋转方法也会依据去噪定义的不同而不同。最流行的正交旋转是方差极大旋转，它试图对载荷阵的列进行去噪，使得每个成分只是由一组有限的变量来解释（即载荷阵每列只有少数几个很大的载荷，其他都是很小的载荷）。\n\n\n8.2.5 解释结果并计算主成分得分\n  对结果进行解释，并计算各个主成分的得分，需要注意的是：使用相关系数矩阵进行主成分分析时，无法直接获得主成分得分，只能得到原始变量的主成分系数。"
  },
  {
    "objectID": "chapter_8.html#主成分分析的示列",
    "href": "chapter_8.html#主成分分析的示列",
    "title": "8  番外篇（三）：主成分分析",
    "section": "8.3 主成分分析的示列",
    "text": "8.3 主成分分析的示列\n  建议在R语言中，使用psych包进行主成分分析。\n\n8.3.1 载入包和数据\n  示列数据集使用的是psych包中自带的Harman23.cor数据集。Harman23.cor是一个list，需要提取出分析使用的cov数据集.\n\nlibrary(psych)\n\ndata(Harman23.cor)\nstr(Harman23.cor) \n\nList of 3\n $ cov   : num [1:8, 1:8] 1 0.846 0.805 0.859 0.473 0.398 0.301 0.382 0.846 1 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:8] \"height\" \"arm.span\" \"forearm\" \"lower.leg\" ...\n  .. ..$ : chr [1:8] \"height\" \"arm.span\" \"forearm\" \"lower.leg\" ...\n $ center: num [1:8] 0 0 0 0 0 0 0 0\n $ n.obs : num 305\n\ndf <- Harman23.cor$cov\nstr(df)\n\n num [1:8, 1:8] 1 0.846 0.805 0.859 0.473 0.398 0.301 0.382 0.846 1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:8] \"height\" \"arm.span\" \"forearm\" \"lower.leg\" ...\n  ..$ : chr [1:8] \"height\" \"arm.span\" \"forearm\" \"lower.leg\" ...\n\n\n\n\n8.3.2 判断主成分个数\n  psych包中给出了一个特别有用的函数fa.parallel(),可以很方便的同时给出Kaiser-Harris准则、Cattell碎石检验、平行分析的结果。由上步骤可以看出，df实际为相关系数矩阵，n.obs是指定原始观测数，n.iter是指定平行分析的随机迭代次数，fa是用来指定进行主成分分析，取值有三类（‘fa’，‘pc’，‘both’）。由 图 8.1 看出，特征值大于1，拐点之上，大于随机的特征值，满足以上三个条件，只有前两个主成分，因此主成分个数为2.\n\nfa.parallel(df, \n            n.obs = 302, \n            n.iter = 100, \n            fa = 'pc', \n            show.legend = TRUE)\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  2 \n\n\n\n\n\n图 8.1: Parallel Analysis Scree Plots\n\n\n\n\n\n\n8.3.3 主成分分析\n  结果中：\n\nPC1和PC2既为2个主成分在各个原始变量上的载荷（loading，也就是相关系数），可以看出系数都较高。\nh2为成分公因子方差，用来说明主成分对每个变量的方差解释度，此值越高说明解释力度越大。\nu2栏指成分唯一性——方差，用来说明无法被主成分解释的比例，h2与u2之和为1。\nProportion Var为每个主成分对整个数据集的解释程度。\nCumulative Var是Proportion Var的累计和，其值至少应该需大于0.80以上。\n\n  可以看出，PC1和PC2在部分变量上都有较高的解释度，不满足各个主成分之间应该正交的条件，需进行旋转。\n\nprincipal(df, \n          nfactors = 2, \n          rotate = 'none', \n          n.obs = 302, \n          scores = TRUE)\n\nPrincipal Components Analysis\nCall: principal(r = df, nfactors = 2, rotate = \"none\", n.obs = 302, \n    scores = TRUE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n                PC1   PC2   h2    u2 com\nheight         0.86 -0.37 0.88 0.123 1.4\narm.span       0.84 -0.44 0.90 0.097 1.5\nforearm        0.81 -0.46 0.87 0.128 1.6\nlower.leg      0.84 -0.40 0.86 0.139 1.4\nweight         0.76  0.52 0.85 0.150 1.8\nbitro.diameter 0.67  0.53 0.74 0.261 1.9\nchest.girth    0.62  0.58 0.72 0.283 2.0\nchest.width    0.67  0.42 0.62 0.375 1.7\n\n                       PC1  PC2\nSS loadings           4.67 1.77\nProportion Var        0.58 0.22\nCumulative Var        0.58 0.81\nProportion Explained  0.73 0.27\nCumulative Proportion 0.73 1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.05 \n with the empirical chi square  46.77  with prob <  1.1e-05 \n\nFit based upon off diagonal values = 0.99\n\n\n  从结果可以看出，PC1和PC2变成了RC1和RC2，即表示经过旋转了，可以很清楚的看出：旋转后，RC1在前4个变量（height – lower.leg）上具有更高的解释度。RC2在后4个变量（weight – chest.width）上具有更高的解释度，说明两个主成分正交（不相关），同时可以发现，旋转后并不改变Cumulative Var\n\ndfp <- principal(df, nfactors = 2, \n                 rotate = 'varimax', \n                 n.obs = 302, \n                 scores = TRUE)\ndfp\n\nPrincipal Components Analysis\nCall: principal(r = df, nfactors = 2, rotate = \"varimax\", n.obs = 302, \n    scores = TRUE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n                RC1  RC2   h2    u2 com\nheight         0.90 0.25 0.88 0.123 1.2\narm.span       0.93 0.19 0.90 0.097 1.1\nforearm        0.92 0.16 0.87 0.128 1.1\nlower.leg      0.90 0.22 0.86 0.139 1.1\nweight         0.26 0.88 0.85 0.150 1.2\nbitro.diameter 0.19 0.84 0.74 0.261 1.1\nchest.girth    0.11 0.84 0.72 0.283 1.0\nchest.width    0.26 0.75 0.62 0.375 1.2\n\n                       RC1  RC2\nSS loadings           3.52 2.92\nProportion Var        0.44 0.37\nCumulative Var        0.44 0.81\nProportion Explained  0.55 0.45\nCumulative Proportion 0.55 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.05 \n with the empirical chi square  46.77  with prob <  1.1e-05 \n\nFit based upon off diagonal values = 0.99\n\n\n\n\n8.3.4 计算主成分得分\n  在principal函数中的scores即是制定是否计算主成分得分的，如果为TRUE，则在dfp中已保存有得分，利用dfp$scores即可以取出得分。但是本示列中使用的是相关系数矩阵，无法计算得分，只能计算得分权重，为：\n\ndfp$weights\n\n                       RC1         RC2\nheight          0.27524417 -0.04748169\narm.span        0.29673051 -0.08005490\nforearm         0.29823990 -0.09158460\nlower.leg       0.28014088 -0.06027214\nweight         -0.06053059  0.33228637\nbitro.diameter -0.07752349  0.32477593\nchest.girth    -0.10366026  0.33763942\nchest.width    -0.03730720  0.27392667"
  },
  {
    "objectID": "chapter_references.html",
    "href": "chapter_references.html",
    "title": "参考文献",
    "section": "",
    "text": "Ashenfelter, Orley, and David Card. 1985. “Using the\nLongitudinal Structure of Earnings to\nEstimate the Effect of Training\nPrograms.” The Review of Economics and Statistics\n67 (4): 648. https://doi.org/10.2307/1924810.\n\n\nAylmer Fisher, Ronald. 1926. “The Arrangement of Field\nExperiments.” Journal of the Ministry of Agriculture of Great\nBritain 33: 503–13. https://doi.org/10.23637/ROTHAMSTED.8V61Q.\n\n\nCalonico, Sebastian, Matias D. Cattaneo, Max H. Farrell, and Rocio\nTitiunik. 2022. Rdrobust: Robust Data-Driven Statistical Inference\nin Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nCox, D R. 2009. “Randomization in the Design of\nExperiments.” International Statistical\nReview 77 (3): 415–29.\n\n\nD. Cattaneo, Matias, Nicolas Idroboy, and Roc Titiunik. 2017. A\nPractical Introduction to Regression Discontinuity Designs: Volume\ni. collingwoodresearch.com.\n\n\nDasu, T., and T. Johnson. 2003. Exploratory Data Mining and Data\nCleaning. John Wiley & Sons, Inc.\n\n\nFleiss, Joseph L, Bruce A Levin, and Myunghee Cho Paik. 2003.\nChapter 5: How to Randomize in\nStatistical Methods for Rates and Proportions.\nHoboken, N.J.: Wiley-Interscience.\n\n\nJacob, Robin, Pei Zhu, and Marie-Andrée. 2012. A Practical Guide to\nRegression Discontinuity. MDRC.org.\n\n\nKabacoff, Robert. 2013. R in Action, Data Analysis and Graphics with\nr. Manning.\n\n\nKang, Minsoo, Brian G. Ragan, and Jae-Hyeon Park. 2008. “Issues in\nOutcomes Research: An Overview of\nRandomization Techniques for Clinical\nTrials.” Journal of Athletic Training 43 (2):\n215–21. https://doi.org/10.4085/1062-6050-43.2.215.\n\n\nKendall, J M. 2003. “Designing a Research Project: Randomised\nControlled Trials and Their Principles.” Emergency Medicine\nJournal 20 (2): 164–68. https://doi.org/10.1136/emj.20.2.164.\n\n\nKernan, Walter N, Catherine M Viscoli, Robert W Makuch, Lawrence M\nBrass, and Ralph I Horwitz. 1999. “Stratified\nRandomization for Clinical Trials.”\nJournal of Clinical Epidemiology 52 (1): 19–26. https://doi.org/10.1016/S0895-4356(98)00138-3.\n\n\nRoberts, C., and D. Torgerson. 1998. “Understanding Controlled\nTrials: Randomisation Methods in Controlled Trials.”\nBMJ 317 (7168): 1301–10. https://doi.org/10.1136/bmj.317.7168.1301.\n\n\nRon, Cody. 2008. Cody’s Data Cleaning Techniques Using SAS.\nCary, NC: SAS Institute Inc.\n\n\nRubin, Donald B. 1980. “Andomization Analysis of Experimental Data\nin the Fisher Randomization Test.” Journal American\nStatistical Association 75 (371). https://www.jstor.org/stable/.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations\nin r.” Journal of Statistical Software 45 (3): 1–67. https://doi.org/10.18637/jss.v045.i03.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n刘晓燕, 陈峰, 魏永越, 柏建岭, and 于浩. 2008.\n“响应—自适应随机化分组方法.”\n10.3969/j.issn.1009-2501.2008.06.016. 临床药理学 13 (6):\n684–89.\n\n\n刘玉秀, 姚晨, 杨友春, and 陈峰. 2001.\n“随机化临床试验及随机化的SAS实现.”\n10.3969/j.issn.1009-2501.2001.03.001. 中国临床药理学与治疗学 6\n(3): 193–95. https://doi.org/10.3969/j.issn.1009-2501.2001.03.001.\n\n\n吴春霖, 王镭, and 李卫兵. 2013.\n“临床试验随机化分组及其Stata的实现.”\n10.7507/1672-2531.20130041. 中国循证医学杂志 13 (2): 242–44. https://doi.org/10.7507/1672-2531.20130041.\n\n\n李立明. 2007. 流行病学. 北京:\n人民卫生出版社.\n\n\n胡良平, 关雪, 毛玮, and 高辉. 2011.\n“各种常见随机化的sas实现.”\n中华脑血管病杂志(电子版) 5 (01): 68–76.\n\n\n谢谦, 薛仙玲, and 付明卫. 2019.\n“断点回归设计方法应用的研究综述.” 经济与管理评论\n35 (2): 11.\n\n\n高涛, 肖楠, and 陈钢. 2013. R语言实战. 人民邮电出版社."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "卫生政策与管理实证研究手册",
    "section": "",
    "text": "前言\n  这是一本写给卫生政策与管理（Health Policy and Management）专业的硕博士研究生的研究入门工具书，主要是分享一些作者多年来积累的一些经验教训，以便为后来的青年学者快速提高科研素养提供一些参考，少走一些弯路。\n  卫生政策与管理在国内可以说还是一个比较年轻的学科，卫生管理、卫生政策以及卫生经济学的现代化思潮大约始于20世纪80年代。对于学科在中国发展起到极大推动作用的，当属1991年6月由中国卫生部与世界银行经济发展学院共同发起成立的“中国卫生经济培训与研究网络” （简称“网络”）。网络邀请了包括世界银行和哈佛大学在内的世界级顶尖教授，为来自全国各地的学者讲授前沿的卫生管理和卫生经济理论与方法，网络的系列培训活动一直持续到21世纪初，培养了中国第一代与国际接轨的卫生管理和卫生经济学专家。而这些系统学习了卫生政策与管理和卫生经济学理论方法的老一辈学者，又为我国培养了下一代的卫生政策与管理学专业人才，这些人已经成长为我国医疗卫生管理系统的中坚力量，其中不少已经是各省市政府或卫生行政部门负责人，也不乏国家级人才计划获得者。\n  由于历史原因，参加网络培训的学者多来自医学院校的流行病学或者卫生统计学专业，其教育背景多为预防医学。受其影响，直到现在绝大部分高校的卫生管理专业仍然设置在公共卫生学院中，这就导致在卫生管理和卫生政策研究方法方面一直是受卫生统计学或者流行病学的影响比较大。但是近些年随着因果推断革命（Causal Inference Revolution）对各个学科在研究方法上的强烈冲击，卫生政策与管理领域的研究在方法学上也开始受到计量经济学非常大的影响。与此同时，随着经济社会水平的不断提高，科学技术的飞速发展，科学研究的问题越来越复杂，学科交叉融合已成为大势所驱。因此，因果推断方法和编程（Coding）技能已成为当代卫生政策与管理学科研究生和青年学者的必备技能之一了。时刻更新自己的知识体系是科研工作者日常生活中的一部分，保持对新知识的学习热情也是一个合格的科研工作者的基本素养。\n本书共分为五个章节：\n\n第一章：科学知识的获取与管理，主要介绍文献的检索与管理、知识谱系的记录与管理。\n第二章：数据库操作（Data Manipulate），主要介绍在实证研究中所涉及的有关数据库的增删改查、结构变换等方法。\n第三章：数据库清洗（Data Clean），主要介绍数据库清洗的步骤和方法。\n第四章：数据的描述，主要介绍如何在统计软件中进行统计描述及推断，并制作符合统计和学术规范的图表。\n第五章：实证研究中的因果推断，主要介绍在卫生政策研究中前沿的因果推断方法、注意事项及实现过程。"
  },
  {
    "objectID": "chapter_5.html#引子",
    "href": "chapter_5.html#引子",
    "title": "5  因果推断方法",
    "section": "5.1 引子",
    "text": "5.1 引子\n  最近几年随着微信公众号和知乎等迅速成为各类信息传播的主要平台，国内科研工作者慢慢的很少”逛”论坛、发帖子、写博客了，有的转战公众号开设自己的账号，有的开始团队运营，这些平台给科学研究提供了不少便利性，扩大了知识获取面，降低了知识检索的时间成本。但是其弊端也毕竟明显，一方面是导致很多人对前沿科学咨讯的获取基本来源于公众号，很少阅读原始文献，另一方面是公众号或者知乎平台分享的信息和知识点没有经过同行评议，其实不乏有错误之处，若不加思索的参考或者照搬很容易导致研究方法误用，特别是对于尚处于学术生涯初期的研究生。\n  关于介绍因果推断方法的书籍非常多了，包括Angrist & JoshuaDavid著的《Mostly Harmless Econometrics: An Empiricist Companion》，Wooldridge的《计量经济学导论》等等。本书并不打算再次搬运以上这些著作的内容，而是主要记录一个学习者在学习和使用这些方法过程中的心路历程，分享学习和使用这些方法的一些经验和体会。\n本书的主要参考资料如下：\n\n\nAngrist, & JoshuaDavid. (2010). Mostly Harmless Econometrics: An Empiricist Companion. Princeton University Press.\n伍德里奇, & 费剑平校. (2010). 计量经济学导论: 第4版. 中国人民大学出版社.\nScott Cunningham. (2020). Causal Inference: The Mixtape. Yale University Press.\nhttps://lost-stats.github.io/Model_Estimation/Research_Design/event_study.html"
  },
  {
    "objectID": "chapter_5.html#双重差分",
    "href": "chapter_5.html#双重差分",
    "title": "5  因果推断方法",
    "section": "5.2 双重差分",
    "text": "5.2 双重差分\n\n5.2.1 简介\n  双重差分法（Difference-in-Difference，DID）通常又可称作倍差法或差异中的差异法，是目前社会科学领域用于开展政策效果评估和识别两个因素因果关联的的最常用方法之一。DID最早由Orley Ashenfelter和David Card提出 ，两位学者于1985年在学术期刊《Review of Economics and Statistics》上发表了运用DID的方法估计美国综合就业和培训法案对收入的影响 Ashenfelter and Card (1985) 。DID本质是一种准实验设计（quasi-experimental design），利用干预组和对照组的纵向数据来获得适当的反事实（counterfactual）来估计因果效应，方法是比较参加项目的人群（干预组）和未参加的人群（对照组）之间的结果随时间的变化，其通常用于估计特定的干预或治疗（如某项法律的通过、某种政策的实施或大规模项目实行）的效果。\n  DID的基本原理很简单，但是如果据此就认为采用DID识别框架的方法都很简单就错了，随着Joshua D. Angrist、Guido W. Imbens、Gary King、Susan Athey等经济学和统计学家对因果识别方法体系的贡献，时至今日，DID已经衍生出了多种多样的类型来使用不同的研究设计和数据结构，可以说已经是一大方法家族了吧。\n  DID的统计学基础是哈佛大学著名统计学家 (Rubin 1980) 提出的潜在结果模型（potential outcomes model）或称为反事实框架 (counterfactual framework)，其核心思想在于利用“反事实”假设构造一个可比的对照组来消除样本选择偏误（selection bias）。根据潜在结果模型可知，要想获得某种干预的因果效应就必须比较同一个研究对象在接受干预和不接受干预时结果差异，此时这种差异才是接受干预相对于不接受干预的效果。然而，对于同一研究对象而言，通常无法既观察其接受干预的结果，又观察其不接受干预的结果。对于接受干预的研究对象而言，不接受干预时的状态是一种 “反事实” 状态，对于不接受干预的研究对象而言，接受干预时的状态也是一种 “反事实” 状态，正是由于在同一研究对象中同一时点接受干预与非干预的状态存在互斥性，因此无法通过直接观察“反事实”状态下的效应值。\n  在医学研究中，为了解决这个问题通常采用随机对照试验（RCT）的方法，通过随机化的方法将一组具有高度同质性的研究对象随机分配去接受干预（trentment group）或不接受干预（control group），由于研究对象具有高度同质性且通过随机化消除了样本选择偏误，那么就可以在限定条件下认为，未接受干预的那部分研究对象（control group）就是接受了干预的研究对象（trentment group）在未接受干预时（untreated）的状态。但是，在政策研究中，通常无法人为对政策的执行施加影响，无法做到随机化，干预组和对照组的研究对象很难保证具有高度同质性，尽管如此，计量经济学家们探索出了利用干预组与对照组之间可能存在不随时间变化的系统性差异的特点，构造出了消除样本选择偏误而得到相对纯净政策效应的计量方法，其中就以DID为代表。\n\n\n5.2.2 基本型\n\n\n5.2.3 变化"
  },
  {
    "objectID": "chapter_5.html#diff-in-diff的适用场景",
    "href": "chapter_5.html#diff-in-diff的适用场景",
    "title": "5  因果推断方法",
    "section": "5.3 1. Diff-in-Diff的适用场景",
    "text": "5.3 1. Diff-in-Diff的适用场景\n\n5.3.1 1.1 为什么要用双重差分\n最简单的一句话就是因果识别是各个学科关注的核心问题之一，但是社会科学领域很难开展实验研究，所以需要从分析手段上想办法找到估计干预效应更准的方法。这就是为什么流行病与卫生统计学科的人不怎么热衷双重差分的原因，因为他们更关注RCT，在研究设计上就解决了随机分组的问题，直接student t或者anova就能估计出净的干预效应，通常称为average treatment effect on treated (ATT)。\n\n\n5.3.2 1.2 什么时候可以用双重差分\n简单来说就是需要进行因果识别研究的时候，具体而言通常就是政策评估时常用，因为政策评估通常为社会自然实验，无法进行随机分组和随机抽样，因此无法采用随机对照试验的方法，政策评估有四种常用方法：\n\n工具变量法：寻找外生的工具变量，通过两阶段ols估计，分离出扰动项中与内生变量相关的变异，从而获得更干净的内生解释变量的效应值。具体而言就是，第一阶段用内生解释变量作为被解释变量（因变量），将工具变量作为解释变量（自变量），进行ols估计，获得内生解释变量的预测值；第二阶段则是将内生解释变量的预测值作为解释变量（自变量），将研究原本的指标作为被解释变量再进行ols估计，则内生解释变量的估计参数即为净效应值。缺点：工具变量难找\n断点回归：本质就是趋近局部的随机对照试验，具体介绍可见这篇帖子。\n双重差分：通过进行两次差分获得ATT\n倾向得分匹配：适用于横断面研究\n\n双重差分分析对数据的要求，也就是什么情况下可以考虑双重差分：\n\n两个时间点的个体横断面数据（微观数据）：\n两个横断面个体不是同一批人，也就是常说的两个混合截面，很多较为早期开展的survey数据是这种形式。只要能找到一个变量，能够在第一个横断面中识别出与第二个横断面中受到干预的个体相对应的个体即可，也就需要找到第一个横断面中哪些个体属于干预组。通常可以通过地区变量来分辨，对于试点政策多采取这种方法，因为追踪调查比较费时费力。缺点：无法避免两次横断面调查带来的抽样误差，因为两个横断面的个体并不是同一批人，虽然抽样误差会在组间diff的时候被减去，但这种情况需要抽样误差在对照和干预个体中是均匀分布的。\n两期及更多期的追踪数据，本质就是面板数据：\n和上面情况一样，都是survey数据，但是为追踪调查，也就是每个时间点调查的个体都是同一批人。这种数据最优，可以进行的差分方法变化也最多，但是较难获得，好在最近些年开展的大规模人群调查都在采取追踪调查的方式。\n两期及更多期的机构或地区数据，就是常见的机构或地区面板数据（宏观数据）：\n比第二种情况更容易获得数据，但是缺点在于很难收集到样本量足够大、分析单元足够小的数据，比如较为容易获得的省级面板数据实际缺点较大。\n单一的横断面数据：严格来讲是无法进行双重差分设计的，但是也有采用横断面进行差分研究的实例，比如研究中国大饥荒的通过不同出生队列构建的差分模型。\n\n\n\n5.3.3 1.3 双重差分的基本原理\n为什么通过两次差分就能估计出净效应的原理不详述，这里只讨论一个问题：为什么RCT的研究在估计干预效应时只diff一次，而这里要diff两次？\n这就涉及到了双重差分方法的一个最重要的前提假设：共同趋势（Common Trends）假设，即干预组和对照组在政策实施之前必须具有相同的发展趋势。\n那么为什么要有这个假设呢，可以看下面的示意图：\n左边的图就是最常见的双重差分原理的示意图，共同趋势假设的目的就是为了保证基线期（before）的diff，如果没有treatment，干预组和对照组之间的差异仍然是这个diff，所以在第二次差分时扣掉这个diff就能得到净干预效应。\n所以这个共同趋势假设是为了保证干预和对照组在干预前后基础的变化都是一致的，它包含两层意思：一是必须趋势不变，也就是都是上升或下降或不变；二是必须平行。因此这个共同趋势假设只是一个基础，是在这个假设成立的基础上，才能推断在干预后，干预组和对照组也会（是也会，不是一定，所以这就是双重差分并不完美的地方）按照这个趋势变化，才能利用第二次差分。而有的公众号中写的是只要共同趋势满足，就可以大胆使用双重差分，其实并不是。\n上面只说了共同趋势假设是为了保证反事实情况下的变化一致，但是并没有说出有这个假设的原因。应该也有人会问，为什么所有的双重差分的示意图在干预前的两条线都是平行的，但是有间隙。原因就是前面所讲的，这种自然试验无法保证干预组和对照组是随机分配的，换句话说就是这两组人在没有施加干预时，就是两个人群，所以会有差异，也就是示意的间隙。\n因为这个“间隙”（就是组间差异）比较大，所以就需要在干预后把它扣除掉，如果不扣除直接对干预后的两组进行差值比较，那么这个差值中时包含了这个“间隙”的，如果这个“间隙”比真实干预效应还大，那么得到的两组差值就失去意义了，所以才会有第二次diff（第一次diff两组分别是前后减，第二次diff是在第一次的结果上干预和对照减）。\n那么，如果这个“间隙”足够小呢，小到随机误差那么大，是不是就可以不用减的。那这就是为什么RCT只diff一次的原因，因为在试验设计阶段，通过严格的随机化已经保证的干预组和对照组个体的一致性，也就是已经将这个“间隙”降到足够小了，就像右边的图示意的一样。\n\n\n\nletter"
  },
  {
    "objectID": "chapter_5.html#diff-in-dff的模型设定",
    "href": "chapter_5.html#diff-in-dff的模型设定",
    "title": "5  因果推断方法",
    "section": "5.4 2. Diff-in-Dff的模型设定",
    "text": "5.4 2. Diff-in-Dff的模型设定\n双重差分的基准模型如下：\n\\[ y_{it} = \\beta_1 \\cdot Treat*Time + \\beta_2 Treat + \\beta_3 Time + \\beta_0+ \\epsilon_{it} \\]\n各参数代表意义就不解释了，其采用ols估计就行了，该模型适用于前面所提到的第一种和第二种中的两期的数据类型。\n但是，如果仔细看的话，该模型就是采用的双向固定效应模型，其中treat是个体效应，time是时间效应。因此，对于多期的面板数据，你又会看见下面的模型：\n\\[\ny_{it} = \\beta \\cdot Treat*Time + \\lambda t + \\mu_i + \\epsilon_{it}\n\\]\n其中没有了treat和time，取而代之是lambda和mu代表的个体效应和时间效应，因为在多期面板中，如果仍然用treat和time会过于粗糙，time只有0和1，无法识别出完整的时间效应，而treat同样存在这个问题。\n当然模型还有其他变化，比如当treat为连续变量时，政策冲击不是在同一时间单一完成的，但是归根结底都是双向固定效应模型的变换形式。"
  },
  {
    "objectID": "chapter_5.html#diff-in-dff的变化",
    "href": "chapter_5.html#diff-in-dff的变化",
    "title": "5  因果推断方法",
    "section": "5.5 3. Diff-in-Dff的变化",
    "text": "5.5 3. Diff-in-Dff的变化\n正如前面所将，双重差分法并不要求干预组和对照组是完全一致的，可以存在一定的差异，但是要求这种差异不随着时间产生变化，这也是在共同趋势假设成立的基础上，能够进一步推断干预后这个差异仍然是基线的差异的基础，因为前面几年是平行变化的，并不代表在政策施加之后也是平行变化的。但是这个不随时间变化的要求是无法检验的，所以这就成了双重差分法的一个无法避免的weakness。\n那么，有人会问如果共同趋势不满足怎么办，当然会有其他的方法：匹配+双重差分，最常见的是psm+did，当然也可以是cem+did\n对于两期的混合截面或者面板数据而言，是无法进行共同趋势检验的，所以有的论文中在这种情况下就不提共同趋势的事，对于多期面板虽然可以检验共同趋势，但是也会存在这个假设不满足的情况。\n所以，为了研究的严谨性和可继续下去，有的学者就提出提出对样本进行匹配，因为共同趋势不满足的根本原因就是干预组和对照组之间差异太大，所以就通过匹配的办法让两组个体更加可比一些，将“间隙”缩小一些。\n那么有的人又会问，既然前面说的横断面数据可以用匹配的方法进行效应估计，那既然都匹配了，也就是说趋近随机分组了，那么为什么还要diff两次。解释就是想要真正做到随机分组，匹配的变量需要很多，匹配的效果与RCT还是有差别，两组间的“间隙”仍然存在，所以还是需要用双重差分估计更干净的效应。\n这里需要注意的一点是，应用匹配+双重差分的方法，不同的数据情况采用不同的匹配策略。\n\n因为对于面板数据，得到了干预前匹配后的个体，自然就确定了干预后的个体。\n而对于两个时点的混合截面数据，由于干预前后的个体本来就不是精确对应的，所以如果要用匹配+did的话，就需要进行三次匹配：干预前：干预 vs 对照，获得干预前匹配好的样本，然后在这个基础上，在第二期截面数据里，分别在干预组和对照组中进行 干预前 vs 干预后的匹配，这样才能严格满足要求，但是缺点是会损失较大的样本量。\n当然，对于两个时点的混合截面数据，也可以只进行两次匹配，第一个时点：干预 vs 对照，第二个时间点：干预 vs 对照，这样做的一个前提是，要有足够的证据支持两个时点的样本都是总体的随机样本，也就是抽样误差要足够小。\n对于psm或cem阶段，如果采取的是1:1匹配，那么在后续did回归时不需要考虑匹配权重的问题，如果采取的不是1:1匹配，则需要在回归时进行加权回归（对干预前个体匹配获得的权重，应该也要赋给干预后的）。\n\n\n5.5.1 变化型一：Matching-DID\n\n\n5.5.2 变化型二：非同一时点干预\n\n\n5.5.3 变化型三：事件分析法\n\n\n5.5.4 变化型四：SCM-DID"
  },
  {
    "objectID": "chapter_5.html#工具变量",
    "href": "chapter_5.html#工具变量",
    "title": "5  因果推断方法",
    "section": "5.6 工具变量",
    "text": "5.6 工具变量"
  },
  {
    "objectID": "chapter_5.html#断点回归",
    "href": "chapter_5.html#断点回归",
    "title": "5  因果推断方法",
    "section": "5.7 断点回归",
    "text": "5.7 断点回归\n\n5.7.1 简介\n  断点回归（Regression Discontinuity）适用于以下情形：人群是否接受干预（Treatment）是依据某一数值变量（rating variable）是否高于或低于某一确定的阈值（threshold）或者分割点（cut-point），例如在研究是否上大学会影响收入时，数值变量（rating variable，也叫 assignment variable，score，running variable，forcing variable， or index）就是高考分数，阈值或者分割点就是本科录取分数线。以下内容主要参考：(Calonico et al. 2022; D. Cattaneo, Idroboy, and Titiunik 2017; Jacob, Zhu, and Marie-Andrée 2012; 谢谦, 薛仙玲, and 付明卫 2019)。\n\n\n5.7.2 发展历史\n  Regression Discontinuity最早由社会学家Thistlethwaite and Campbell 在1960年提出的，用于评估社会项目，但是他们的研究虽然引起一些影响但是没有得到广泛的注意，后来，在1972-2009年期间，被一系列经济学家(Goldberger，van der Klaauw，Imbens and Kalyanaraman等)在方法学方面进行了完善，最终在2008年达到顶峰，标志是在2008年Journal of Econometrics出了一期RD分析的Special Issue，光看期刊名字就知道是经济学顶刊了(Journal of Econometrics是公认的计量经济学顶尖期刊，是教育部认可的12本经济学国际顶级期刊之一 )。\n\n\n5.7.3 特点与分类\nRD有两个特点：\n\n在Rating variable的一个明确点上，outcome出现了跳跃或者不连续(discontinuity at a cut-point)\n可以认为在一个限定的rating variable区间上，个体是服从局部随机的(local randomization)\n\nRD有两种类型：\n\n精确断点(sharp design): 即所有个体(All subject)在明确的cut-point之后全部接受干预(treatment)\n模糊断点(fuzzy design): 即在cut-point前后，存在 no-shows (treatment group members who do not receive the treatment)或者crossovers (control group members who do receive the treatment) ，换句话说就无法找到一个明确的cut-point完全区分干预和对照组。更严格的分法是，fuzzy也可以分成两类：type I是no-shows，只存在处理组有未接受处理的个体，type II是同时存在no-shows和crossovers。实际就是RCT中常说的沾染问题。\n\n\n\n5.7.4 适用RD分析的先决条件(Conditions for Internal Validity)\n  由于RD仍然属于非实验方法，尽管也被成为类实验(quasi-experimental)，但本质还是非实验方法(nonexperimental)， 所以它必须满足一系列前期条件，才能提供无偏估计和更可能的接近RCT的严格情形。\n\n一、 Rating variable (中文的翻译很多，但比较通用的翻译为驱动变量或者配置变量)不能被干预(treatment)所影响，Rating variable 的值必须是在干预(treatment)之前就已经确定了，或者是不可再更改的变量。也就是只能是驱动变量的值决定是否接受干预，不能是干预决定驱动变量的值。比如：高考成绩(Rating variable)是在判断是否能够上大学(treatment)之前就已经确定了，并且分数不会再发生变化，如果存在严重的根据分数线修改高考成绩的情况，则高考分数就不能作为Rating variable。 检验方法是：画Rating variable的频率分布直方图或者密度图，再进行McCrary 检验\n二、 断点(cut-point)是完全外生的，并且干预与否完全取决于驱动变量和断点。即断点不受其他因素影响而改变干预的判断，即把本不该接受干预的对象划入了干预，或者把该接受干预的对象划入了对照。 比如，如果高考录取线(cut-point)的确定是完全已经严格的录取指标划定的， 而不存在为了使得某部分人群能够获得上大学的机会，而修改录取线。\n三、 在驱动变量的cut-point前后，除了干预与否(treatment)是不连续或者跳跃的(比如断点前为0，断点后为1)，其他任何因素都不能出现不连续或者跳跃(Nothing other than treatment status is discontinuous in the analysis interval)，因为要保证outcome的跳跃只能是干预的不连续或者跳跃导致的。比如，如果高考分数线除了决定是否能上大学外，还决定是否享有创业扶持机会，那么如果研究上大学对某种结局的影响，就无法分离出到底是大学教育导致，还是获得创业扶持机会导致的。\n四、 驱动变量(rating variable)与结局变量(outcome)之间的函数关系应该是连续的，严格的说，应该是在进行RD分析的这段区间内(interval)是连续的。换句话说就是，如果不存在干预的情况，驱动变量与结局变量之间不会出现不连续或者跳跃，才能在结局变量出现不连续的情况下，反推出是干预引起的，因为只有干预是不连续的。注意：此条件只需要在选择参数估计方法是要满足(applies only to parametric estimators)\n\n\n\n5.7.5 断点回归的图形分析(Graphical Presentations in the RD)\n图形分析是进行RD的第一步，也是非常重要的组成部分\n通常，进行RD至少需要画4种图：\n\n驱动变量与干预之间的关系图，在这一步可以确定是应该采取Sharp还是Fuzzy断点回归设计\n非结局变量与驱动变量的关系图，可以用来判断是否满足第3条内部有效性(Internal Validity)条件\n驱动变量的密度分布图，是为了判断驱动变量是否连续，以及在cut-point附近是否存在被操控，可以用来判读石佛满足第1条内部有效性(Internal Validity)条件\n结局变量与驱动变量的关系图，可以帮助预估干预效应的大小，以及判断结局变量与驱动变量之间的函数关系。而且必须是结局变量在纵轴，驱动变量在横轴。注意：通常这一幅图是用来初步判断整个研究是否能够获得预期的干预效应的，如果在这幅图中无法观测到明显的jump，基本后续的分析也是徒劳。\n\n\n\n5.7.6 进行RD分析的步骤及示例\n示列软件: R version 4.0.5 (2021-03-31)\n示列分析包：在R语言中一共有三个package可以进行RD分析：\n\nrdd: 由Dimmery在2016年发布，最后一次更新是2016-3-14，貌似目前没有获得积极的更新\nrddtools：由Stigler & Quast最早在2013年发布，最后一次更新是2015-07-27，貌似目前也没有获得积极的更新\nrdrobust：由Sebastian Calonico, Matias D. Cattaneo, Max H. Farrell等在2016年发布的，最后一次更新是2018-09-26，是目前功能最完善的RD分析包，同时也开发了Stata版本。该包的作者同时也是RD方法学上的大佬，多种bandwidth选择方法的发明者，著名的CCT就是。\n\n\nrdrobust简介：目前该包的开发项目可在https://sites.google.com/site/rdpackages/home主页上查看，该包共有三个函数： + rdplot：用来画图 + rdrobust：用来进行局部非参数估计 + rdbwselect：用来进行bandwidth选择 另外还有一个协同的包rddensity，需要从CRAN上单独安装，用来进行对驱动变量是否被操控进行检验，因为rdrobust包未包含McCrary检验\n\n示列数据集：采用rdrobust包中自带的rdrobust_RDsenate数据集：\n\n数据集简介：该数据集包含了1914–2010年间美国参议院选举的数据，可以利用RD的方法分析民主党赢得参议院席位对于在下次选举中获得的相同席位的影响。该数据共包含两个变量：\n> + vote：记录了州级层面下的参议院议席中民主党所占的比例，值从0到100，是RD分析中结局变量 > + margin：记录了民主党在上次选举中获得相同参议院席位的胜利边际，值从-100到100，当大于0时，说明民主党胜利，小于0则输了，是RD分析中的驱动变量\n\n\n5.7.6.1 Step 1: 确定驱动变量(Rating variable)和断点(Cut-point)\n根据研究设计，判断是否存在采用RD进行因果识别或者效应估计的可能，即是否可以找到合适的驱动变量和明确的断点来识别施加干预的状态。\n通常情况下时间（具体到天）、年龄（如研究退休，代表性的文章: “退休影响会健康吗”）、地理距离（研究雾霾，代表性文章: “冬季供暖导致雾霾？来自华北 城市面板的证据”）比较容易作为驱动变量。\n\n\n5.7.6.2 Step 2: 内部有效性(Internal Validity)条件的检验\n1) 条件一：通过画驱动变量的频率分布直方图或者密度分布图，以及McCrary检验\n可以利用graphics包或者ggplot2包画histogram图，也可以利用rddenstiy包中的rddenstiy函数画密度图，也可利用rdd包进行McCrary检验。\nrddenstiy函数的其他参数可以用默认值，输出结果中，最下面一行robust的p值大于0.05，就说明驱动变量未受到操纵。\n\n方法一\n\n\nlibrary(rdrobust)\nlibrary(rddensity)\nlibrary(magrittr)\ndata(rdrobust_RDsenate)\n\nhist(x = rdrobust_RDsenate$margin, \n     breaks = 30, \n     col = 'red',\n     xlab = 'margin', \n     ylab = 'Frequance', \n     main = 'Histogram of Rating Variable')\nabline(v = 0)\n\n\n\n\n图 5.1: Histogram of Rating Variable (1)\n\n\n\n\n\n方法二\n\n\nlibrary(ggplot2)\nggplot(rdrobust_RDsenate) +\n    geom_histogram(aes(x = margin, \n                       fill = margin < 0), \n                       color = 'black', \n                       binwidth = 4, boundary = 4) +\n    geom_vline(xintercept = 0, size = 1) +\n    labs(title = 'Histogram of the Rating varibale',\n         y = 'Number of Observation', \n         x = 'Margin') + \n    scale_fill_manual(name = '', \n                      values = c('red', 'blue'),\n                      labels = c('Control', 'Treat')) +\n    theme_bw() +\n        theme(legend.position = c(.9, .9),\n              legend.background = element_blank())\n\n\n\n\n图 5.2: Histogram of Rating Variable (2)\n\n\n\n\n\n方法三\n\n\nrdplotdensity(rddensity(rdrobust_RDsenate$margin),\n                 X = rdrobust_RDsenate$margin)$Estplot[1]\n\n$data\nlist()\nattr(,\"class\")\n[1] \"waiver\"\n\n\n\n\n\n图 5.3: Histogram of Rating Variable (3)\n\n\n\n\n2) 条件二三四根据研究设计及背景资料进行判断。\n\n\n5.7.6.3 Step 3: 画驱动变量与干预的关系图，判断sharp或者fuzzy类型\n  画驱动变量与干预的散点图，判断是否为sharp或者fuzzy类型。可以看出本例中，干预在cut-point前后，有明确的界限，为sharp类型。\n\nrdrobust_RDsenate$treatment <- \n  ifelse(rdrobust_RDsenate$margin < 0, 0, 1)\n\nrdrobust_RDsenate$color <- \n  ifelse(rdrobust_RDsenate$margin < 0, \n                                  'blue', \n                                  'red')\n\nplot(x = rdrobust_RDsenate$margin, \n     y = rdrobust_RDsenate$treatment, \n     col = rdrobust_RDsenate$color,\n       type = 'p', \n     pch = 16, \n     xlab = 'Margin', \n     ylab = 'Treatment',\n     main = 'Relationship between \n             rating variable and treatment')\nabline(v = 0)\n\n\n\n\n图 5.4: Relationship between rating variable and treatment\n\n\n\n\n\n\n5.7.6.4 Step 4: 画驱动变量与结果变量的关系图，选择合适的bin数量\n一、画散点图\n  横轴为驱动变量，纵轴为结果变量，画散点图，初步判断两者之间关系，但是散点图噪音(noise)太大，不利于发现跳跃点，需要对驱动变量分不同的段(intervals or bin)将使得关系取线更平顺。\n\n# 第一步：散点图\n\nplot(x = rdrobust_RDsenate$margin, \n     y = rdrobust_RDsenate$vote, \n     type = 'p',\n       col = rdrobust_RDsenate$color, \n     pch = 16, \n     cex = 0.8, \n     xlab = 'Margin', \n     ylab = 'Vote')\nabline(v = 0)\n\n\n\n\n图 5.5: 散点图\n\n\n\n\n二、将驱动变量分段的步骤\n  将驱动变量分段后再做关系图，大致有四步：\n\n将驱动变量分成若干宽度(width)相等的区间(bin)，但是需要注意的是不能存在骑跨cut-point的区间， 所以最好从cut-point开始分别向左右两端进行划分，并不一定要求左右两边的bin数量相等。\n\n计算每个bin中的结局变量的平均值、驱动变量的中位数，以及个体的数量(num of observation)\n将驱动变量的中位数作为横轴，将结局变量的平均值作为纵轴，同时用每个bin中个体的数量进行加权\n为了更好的显示两变量之间的关系，最好加上局部加权平滑回归取线(如：lowess: locally weighted regression)\n\n三、确定bins数量的方法\n  如何设定bin的数量或者binwidth的方法比较复杂，bin的数量不宜过多或过少，过少起不到将噪音的作用，过多会导致jump不明显，在Stata和R中有打包的好的方法去判断bin的数量，本例中采用rdrobust包中的rdplot函数进行判断。\nrdplot函数的常用参数有：\n\nc为cut-point，默认为0\np为全局多项式的幂次方，默认为4\nnbins用来设定断点左右的bin的数量，默认为NULL\nkernel用来设定加权方法，默认为uniform，同时还有triangular和epanechnikov方法\nbinselect为判断方法，默认为’esmv’，\n\n判断方法常用一共4种：Evenly-spaced (es)，Quantile-spaced (qs)，以及结合mimicking-variance (MV)的esmv和qsmv，推荐使用qs方法。p和nbin如果不确定也可缺失，让binselect方法自行设定，从结果可以看出总的样本量，以及cut-point左右的样本量，以及设定的bin的数量。\n\nbins <- rdplot(rdrobust_RDsenate$vote, \n               rdrobust_RDsenate$margin, \n               c = 0, p = 4,\n               nbins = c(20, 20), \n               binselect = 'esmv', \n               kernel = 'uniform')\n\nsummary(bins)\n\nCall: rdplot\n\nNumber of Obs.                 1297\nKernel                      Uniform\n\nNumber of Obs.                  595             702\nEff. Number of Obs.             595             702\nOrder poly. fit (p)               4               4\nBW poly. fit (h)            100.000         100.000\nNumber of bins scale              1               1\n\nBins Selected                    20              20\nAverage Bin Length            5.000           5.000\nMedian Bin Length             5.000           5.000\n\nIMSE-optimal bins                 8               9\nMimicking Variance bins          15              35\n\nRelative to IMSE-optimal:\nImplied scale                 2.500           2.222\nWIMSE variance weight         0.060           0.084\nWIMSE bias weight             0.940           0.916\n\n\n\n\n\n图 5.6: 确定bins数量的方法\n\n\n\n\n\n\n5.7.6.5 Step 5: 模型估计效应大小\n1) 效应大小的估计方法一共有两种：\n\n全局参数估计(Parametric/global strategy): 即是利用全部个体的数据，对驱动变量和结局变量拟合线性、二次项、三次项、四次项回归模型，此方法利用的是RD设计的”断点处不连续(discontinuity at a cut-point)“的特性。模型设定如下：\n\n\\[\n\\begin{aligned}\nY_i = & \\alpha + \\beta_0 \\cdot Treatment_i + \\beta_1 \\cdot (rating_i - cutpoint)^k + \\\\\n      & \\beta_3 \\cdot Treatment_i \\cdot (rating_i - cutpoint)^k + \\\\\n      & \\beta_4 \\cdot Confouders_i + \\epsilon_i\n\\end{aligned}\n\\]\n\n式中：\n\nY为结局变量，\\(\\alpha\\) 为常数项，表示在控制驱动变量后干预组结局变量的平均值，k为多项式的幂次，通常为1到4次方，其他参数含义见字面意思，\n\\(\\beta_0\\)就是核心关注的系数，表示干预措施在断点处的边际效应，也就是干预的处理效应。\n\n而如何选择合适的次项数，需要多次尝试，通过比较AIC，选择AIC最小的模型。\n驱动变量减去cutpoint的目的是为了中心化，为了使\\(\\alpha\\)反映的是断点处的平均值，因为差值在断点处为0\nConfouders为混杂因素，也可以加入模型中，但是中RD分析时，混杂因素的控制不是必须的。\n\n局部非参数估计(Nonparametric/local strategy): 即是在cut-point左右选择一段合适的带宽(bandwidth)，在这一段带宽局部范围内， 拟合驱动变量和结局变量的线性或者多项式回归模型，此方法利用的是RD设计的”局部随机(local randomization)“的特性。此方法适用于样本量较大的情况， 因为样本量太少，设定带宽之后样本量将会不足，增加估计误差。前面所提到的R包中均只提供局部非参数估计函数。局部非参数估计的难点在于如何确定最优带宽，常用的方法有交叉验证法(cross validation procedure， CV)、 IK 法和 CCT 法，后两种方法较为推荐\n\n  关于上述两种方法的比较或者权衡，主要是关于精度和误差，全局参数估计因为利用了全部样本，通常对效应的估计精度更高，然而其也存在一个问题，就是驱动变量和结局变量的关系函数在越大的区间下越难以准确识别；而非参数估计可以通过局部拟合加权线性或者多项式回归模型而减少这种偏误。\n  以上方法主要针对sharp类型，fuzzy类型的估计方法见最后一节。\n2) 全局参数估计\n  分为一次线性、一次线性加交互项、以及二次、三次线性及交互项，共6个模型，比较AIC或者回归残差，较小者模型较优。\n\nrdrobust_RDsenate$margin_del <- rdrobust_RDsenate$margin - 0\n\n# linear\nfit_1 <- lm(vote ~ margin_del + treatment, \n            data = rdrobust_RDsenate) \n\n# linear interaction\nfit_2 <- lm(vote ~ margin_del * treatment, \n            data = rdrobust_RDsenate) \n\n# quadratic\nfit_3 <- lm(vote ~ margin_del + \n                   I(margin_del ^ 2) + treatment, \n            data = rdrobust_RDsenate) \n\n# quadratic interaction\nfit_4 <- lm(vote ~ (margin_del + I(margin_del ^ 2)) * \n                   treatment, \n            data = rdrobust_RDsenate) \n\n# cubic\nfit_5 <- lm(vote ~ margin_del + I(margin_del ^ 2) + \n                   I(margin_del ^ 3) + treatment, \n            data = rdrobust_RDsenate) \n\n# cubic interaction\nfit_6 <- lm(vote ~ (margin_del + I(margin_del ^ 2) + \n                    I(margin_del ^ 3)) * treatment, \n            data = rdrobust_RDsenate) \n\n\nlibrary(modelsummary)\n\nmodelsummary(list(\"Linear\" = fit_1, \n                  \"Linear interaction\" = fit_2, \n                  \"Quadratic\" = fit_3), \n              output = \"kableExtra\",\n              stars = TRUE)\n\n\n\n表 5.1:  全局参数估计 \n \n  \n      \n    Linear \n    Linear interaction \n    Quadratic \n  \n \n\n  \n    (Intercept) \n    47.331*** \n    44.904*** \n    45.486*** \n  \n  \n     \n    (0.542) \n    (0.699) \n    (0.616) \n  \n  \n    margin_del \n    0.348*** \n    0.216*** \n    0.285*** \n  \n  \n     \n    (0.013) \n    (0.028) \n    (0.017) \n  \n  \n    treatment \n    4.785*** \n    6.044*** \n    6.663*** \n  \n  \n     \n    (0.923) \n    (0.942) \n    (0.963) \n  \n  \n    margin_del × treatment \n     \n    0.170*** \n     \n  \n  \n     \n     \n    (0.032) \n     \n  \n  \n    I(margin_del^2) \n     \n     \n    0.001*** \n  \n  \n     \n     \n     \n    (0.0002) \n  \n  \n    Num.Obs. \n    1297 \n    1297 \n    1297 \n  \n  \n    R2 \n    0.578 \n    0.587 \n    0.590 \n  \n  \n    R2 Adj. \n    0.577 \n    0.586 \n    0.589 \n  \n  \n    AIC \n    10083.7 \n    10056.7 \n    10049.9 \n  \n  \n    BIC \n    10104.4 \n    10082.5 \n    10075.7 \n  \n  \n    Log.Lik. \n    −5037.848 \n    −5023.354 \n    −5019.946 \n  \n  \n    F \n    886.433 \n    613.586 \n    619.091 \n  \n  \n    RMSE \n    11.77 \n    11.64 \n    11.61 \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n3) 局部非参数估计\n  利用rdrobust包的rdrobust函数进行拟合。由于该包是由Sebastian Calonico, Matias D. Cattaneo and Rocío Titiunik开发，所以该包中所提供的带宽选择方法可认为就是CCT法，因为CCT本来就是Calonico, Cattaneo, and Titiunik三人姓氏的缩写。\n有两种确定带宽的方法：\n\n一是先利用rdbwselect函数计算出最优带宽，然后用rdrobust函数中的h参数手动指定左右的带宽。\n\n二是直接利用rdrobust函数中的bdselect参数指定选择最优带宽的方法。\n\nrdrobust包中一共提供了10种选择最优带宽的方法，如下:\n\n其中，共包含MSE = Mean Square Error和CER = Coverage Error Rate两大类，MSE更适用于进行点估计的带宽选择， 而CER更适合区间估计的带宽选择。\n以rd结尾是表示选择的带宽在cut-point左右相等，而以two结尾是表示选择的带宽在cut-point左右不相等。\nh即为选择的左右最优带宽，而b给出的是用来进行敏感性分析时应该考虑的带宽。\n\n\nrdbwselect(y = rdrobust_RDsenate$vote, \n           x = rdrobust_RDsenate$margin, \n           all = TRUE) %>% summary()\n\nCall: rdbwselect\n\nNumber of Obs.                 1297\nBW type                         All\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  595          702\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nUnique Obs.                     595          665\n\n=======================================================\n                  BW est. (h)    BW bias (b)\n            Left of c Right of c  Left of c Right of c\n=======================================================\n     mserd    17.754     17.754     28.028     28.028\n    msetwo    16.170     18.126     27.104     29.344\n    msesum    18.365     18.365     31.319     31.319\n  msecomb1    17.754     17.754     28.028     28.028\n  msecomb2    17.754     18.126     28.028     29.344\n     cerrd    12.407     12.407     28.028     28.028\n    certwo    11.299     12.667     27.104     29.344\n    cersum    12.834     12.834     31.319     31.319\n  cercomb1    12.407     12.407     28.028     28.028\n  cercomb2    12.407     12.667     28.028     29.344\n=======================================================\n\n\n  虽然在此文中rdrobust: An R Package for Robust Nonparametric Inference in Regression-Discontinuity Designs提到也可以通过bdselect = ’CV’或者bdselect = ’IK’来采用CV和IK法，但是在写这篇笔记时，已无法使用这两种方法。\n输出的结果的最后一张表的第一行Conventional即为干预的处理效应。\n\n\n\n\n# 如果有混杂因素需要控制,可以用covs = c('var1', 'var2')\nloc_fit_1 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin, \n                      c = 0, p = 1,\n                      kernel = 'triangular', \n                      bwselect = 'msetwo') \n\n# c用来指定cut-point，p用来指定局部加权回归的多项式幂次\nloc_fit_2 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin, \n                      c = 0, p = 2,\n                      kernel = 'triangular', \n                      bwselect = 'msetwo') \n\nloc_fit_3 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin, \n                      c = 0, p = 1,\n                      kernel = 'triangular', \n                      bwselect = 'cerrd')\n\nloc_fit_4 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin, \n                      c = 0, p = 2,\n                      kernel = 'triangular', \n                      bwselect = 'certwo')\n\n\nmodelsummary(list(loc_fit_1, loc_fit_2,\n                  loc_fit_3, loc_fit_4),\n             output = \"kableExtra\",\n             stars = TRUE,\n             estimate = \"std.error\")\n\n\n\n表 5.2:  局部非参数估计 \n \n  \n      \n    Model 1 \n    Model 2 \n    Model 3 \n    Model 4 \n  \n \n\n  \n    Conventional \n    1.497*** \n    1.852*** \n    1.680*** \n    2.209*** \n  \n  \n     \n    (1.497) \n    (1.852) \n    (1.680) \n    (2.209) \n  \n  \n    Bias-Corrected \n    1.497*** \n    1.852*** \n    1.680*** \n    2.209*** \n  \n  \n     \n    (1.497) \n    (1.852) \n    (1.680) \n    (2.209) \n  \n  \n    Robust \n    1.759*** \n    2.056*** \n    1.841*** \n    2.282*** \n  \n  \n     \n    (1.759) \n    (2.056) \n    (1.841) \n    (2.282) \n  \n  \n    Kernel \n    Triangular \n    Triangular \n    Triangular \n    Triangular \n  \n  \n    Bandwidth \n    msetwo \n    msetwo \n    cerrd \n    certwo \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n\n\n5.7.6.6 Step 6: 敏感性分析\n  敏感性分析主要用来检验模型估计结果的稳健性，RD分析主要有四种敏感性分析方式;\n\n对前面所述的Internal Validity条件三的检验，即除了干预(Treatment)变量外，其他的任何非结局变量(包括混杂因素在内)都不能在cut-point处出现不连续或称为跳跃，如果存在，则无法推断结局变量的跳跃是由干预(Treatment)引起的，检验方式就是将所有的混杂因素作为结局，利用rdrobust函数进行检验，输出结果的coef应该很小，且p值应该统计学不显著 (Continuity-Based Analysis for Covariates)。\n对cut-point的敏感性分析，即是更换cut-point，检验是否左右还存在处理效应，如果更换断点后，仍然存在处理效应，则无法说明本研究的干预措施是有效的，因为在研究设定的断点处识别到的处理效应，有可能是由其他因素引起的。\n对cut-point附近个体的敏感性分析，在Internal Validity条件一中提到，驱动变量不可被操控，但是这个无法直接检验，因此，换个思路，如果驱动变量被超控，那自然是在cut-point左右离得最近的值被操纵的可能性最大，所以如果将这部分个体剔除，若仍然能观测到处理效应，则说明这种效应是真实由干预所导致。这种方法被称为甜甜圈(donut hole)法，同样也适用于个体过多的堆积与cut-point附近的RD分析。\n对带宽bandwidth的敏感性分析，即是cut-point不改变，而是更换选择的最优带宽，进行多次局部非参数检验，如果更换带宽之后，仍然能在断点处识别的处理效应，说明研究的干预措施是有效的，因为在研究设定的断点处识别到的处理效应不是由于某一特点的带宽下才观测到的，说明处理效应稳健。\n\n1) 对cut-point的敏感性分析\n  选择一个虚拟的断点用来替换真正的断点，但是真实的干预情况不改变。关于虚拟断点如何选择以及选择多少个并没有明确的标准，通常在真实断点的附近，左右对称选取即可\n\n# 除c以外的其他参数应该与前面分析保持一致\nsen_cut_1 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin,\n                      c = 1, p = 1,\n                      kernel = 'triangular', \n                      bwselect = 'msetwo') \n\n# 除c以外的其他参数应该与前面分析保持一致\nsen_cut_2 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin,\n                      c = -1, p = 1,\n                      kernel = 'triangular', \n                      bwselect = 'msetwo') \n\n\nmodelsummary(list(sen_cut_1, sen_cut_2), \n             output = \"kableExtra\",\n             stars = TRUE,\n             estimate = \"std.error\")\n\n\n\n表 5.3:  对cut-point的敏感性分析 \n \n  \n      \n    Model 1 \n    Model 2 \n  \n \n\n  \n    Conventional \n    1.406*** \n    1.614 \n  \n  \n     \n    (1.406) \n    (1.614) \n  \n  \n    Bias-Corrected \n    1.406*** \n    1.614 \n  \n  \n     \n    (1.406) \n    (1.614) \n  \n  \n    Robust \n    1.640** \n    1.865 \n  \n  \n     \n    (1.640) \n    (1.865) \n  \n  \n    Kernel \n    Triangular \n    Triangular \n  \n  \n    Bandwidth \n    msetwo \n    msetwo \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n2) 对cut-point附近个体的敏感性分析\n  对称剔除真实断点左右一定较小范围内的个体。关于范围的大小同样没有明确的标准，通常选取多个范围比较即可。\n\nrdrobust_RD_hole_1 <- subset(rdrobust_RDsenate, \n                             abs(margin) > 0.3)\nrdrobust_RD_hole_2 <- subset(rdrobust_RDsenate, \n                             abs(margin) > 0.4)\nrdrobust_RD_hole_3 <- subset(rdrobust_RDsenate, \n                             abs(margin) > 0.5)\n\n# 除c以外的其他参数应该与前面分析保持一致\nsen_hole_1 <- rdrobust(rdrobust_RD_hole_1$vote, \n                       rdrobust_RD_hole_1$margin, \n                       c = 0, p = 1,\n                       kernel = 'triangular', \n                       bwselect = 'msetwo') \n\n# 除c以外的其他参数应该与前面分析保持一致\nsen_hole_2 <- rdrobust(rdrobust_RD_hole_2$vote, \n                       rdrobust_RD_hole_2$margin, \n                       c = 0, p = 1,\n                       kernel = 'triangular', \n                       bwselect = 'msetwo') \n\n# 除c以外的其他参数应该与前面分析保持一致\nsen_hole_3 <- rdrobust(rdrobust_RD_hole_3$vote, \n                       rdrobust_RD_hole_3$margin, \n                       c = 0, p = 1,\n                       kernel = 'triangular', \n                       bwselect = 'msetwo') \n\n\nmodelsummary(list(sen_hole_1, sen_hole_2, sen_hole_3), \n             output = \"kableExtra\",\n             stars = TRUE,\n             estimate = \"std.error\")\n\n\n\n表 5.4:  对cut-point附近个体的敏感性分析 \n \n  \n      \n    Model 1 \n    Model 2 \n    Model 3 \n  \n \n\n  \n    Conventional \n    1.558*** \n    1.558*** \n    1.591*** \n  \n  \n     \n    (1.558) \n    (1.558) \n    (1.591) \n  \n  \n    Bias-Corrected \n    1.558*** \n    1.558*** \n    1.591*** \n  \n  \n     \n    (1.558) \n    (1.558) \n    (1.591) \n  \n  \n    Robust \n    1.839*** \n    1.860*** \n    1.918*** \n  \n  \n     \n    (1.839) \n    (1.860) \n    (1.918) \n  \n  \n    Kernel \n    Triangular \n    Triangular \n    Triangular \n  \n  \n    Bandwidth \n    msetwo \n    msetwo \n    msetwo \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n3) 对带宽bandwidth的敏感性分析\n  即是更换不同的带宽即可，带宽的变化实际是cut-point区间范围尾部的个体在发生变化，可以通过选取不同的带宽选择方式，真实存在的处理效应不会随着带宽的改变而发生变化。\n\n\n\n5.7.7 关于Fuzzy类型的RD模型估计方法\n  Fuzzy类型的RD可以根据驱动变量与干预的实际情况采取不同的处理方式:\n\n如果只是no-shows的情况，可以结合研究目的，决定是否采取意向性分析法(intent-to-treat)\n采用两阶段回归two-stage least squares (2SLS) ，两阶段模型设定如下:\n\n第一阶段：\n\\[ Treatment_i = \\alpha_1 + \\gamma_0 \\cdot treat-by-cutpoint_i + \\epsilon_i \\]\n第二阶段：\n\\[ Y_i = \\alpha_i + \\beta_0 \\cdot \\dot Treatment + \\mu_i \\]\n\n其中：\n\n\\(Treatment_i\\)为真实的干预情况\n\\(treat-by-cutpoint_i\\)是指驱动变量根据cut-point判断的是否接受了干预，为虚拟变量\n第二阶段模型中的\\(\\dot Treatment\\)将第一阶段模型的预测指yhat\n\n\n\n\n通过两阶段回归，第二阶段中的Standard errors是经过了调整的。在rdrobust包的rdrobust函数中，有一个fuzzy的逻辑参数，用来指定RD类型是否为fuzzy，可以很方便的进行fuzzy类型的模型估计。\n\n\n5.7.8 结束\n  由于真实世界的复杂性，对因果推断的分析方法的要求也越来越高，没有一种方法可以适用所有的情形，因此RD的方法也衍生发展出很多类型，包括多个断点(Multi-Cutoff RD Design)、多个驱动变量(Multi-Score RD Design)、离散驱动变量(Multi-Score RD Design)等等。\n\n\n\n\nAshenfelter, Orley, and David Card. 1985. “Using the Longitudinal Structure of Earnings to Estimate the Effect of Training Programs.” The Review of Economics and Statistics 67 (4): 648. https://doi.org/10.2307/1924810.\n\n\nCalonico, Sebastian, Matias D. Cattaneo, Max H. Farrell, and Rocio Titiunik. 2022. Rdrobust: Robust Data-Driven Statistical Inference in Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nD. Cattaneo, Matias, Nicolas Idroboy, and Roc Titiunik. 2017. A Practical Introduction to Regression Discontinuity Designs: Volume i. collingwoodresearch.com.\n\n\nJacob, Robin, Pei Zhu, and Marie-Andrée. 2012. A Practical Guide to Regression Discontinuity. MDRC.org.\n\n\nRubin, Donald B. 1980. “Andomization Analysis of Experimental Data in the Fisher Randomization Test.” Journal American Statistical Association 75 (371). https://www.jstor.org/stable/.\n\n\n谢谦, 薛仙玲, and 付明卫. 2019. “断点回归设计方法应用的研究综述.” 经济与管理评论 35 (2): 11."
  }
]