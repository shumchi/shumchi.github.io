[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "卫生政策实证研究手册",
    "section": "",
    "text": "这是一本写给卫生管理与卫生政策专业的硕博士研究生的研究入门工具书，主要是分享一些作者多年来积累的一些经验，以便为后来的青年学者快速提高科研素养提供一些参考，少走一些弯路。本书共分为五个章节：\n\n第一章：科学知识的获取与管理，主要介绍文献的检索与管理、知识谱系的记录与管理。\n第二章：数据库操作（Data Manipulate），主要介绍在实证研究中所涉及的有关数据库的增删改查、结构变换等方法。\n第三章：数据库清洗（Data Clean），主要介绍数据库清洗的步骤和方法。\n第四章：数据的描述，主要介绍如何在统计软件中进行统计描述及推断，并制作符合统计和学术规范的图表。\n第五章：实证研究中的因果推断，主要介绍在卫生政策研究中前沿的因果推断方法、注意事项及实现过程。"
  },
  {
    "objectID": "chapter_1.html",
    "href": "chapter_1.html",
    "title": "1  文献知识的获取与管理",
    "section": "",
    "text": "“读文献”应该是在科研生涯早期被导师要求进行的最多的科研活动了，足见文献的重要性1，但是很多同学起初对“读文献”是有些抵触的，常认为”读文献“没意思，光说不练，而多热衷于拿数据跑模型看统计显著性，我们当年还对“下现场”调研保留着颇高的兴趣，不知道近些年是否还是这样。那这样的现象是否正常呢？其实正常也不正常。说正常主要在于读文献需要带着问题去阅读，否则容易觉得枯燥，阅读文献的热情不高自然是正常现象。说不正常在于，若未经过深入的文献研究，直接开始处理数据分析结果，容易出现研究方法误用或是缺少创新性等问题。因此，依然要重视“读文献”，培养良好的文献阅读习惯是科学研究的基本要求，它也是科研工作中的重要组成部分。\n  那么，“读文献”的第一步就是检索文献了。如何在数以万计的文献中找到自己需要的研究，是一件具有挑战性的事情。刚刚进入研究生阶段的同学，面对这个问题的时候很多人都很茫然，从哪里检索文献？如何检索文献？哪些文献是应该要阅读的？等等，尽管这些在本科阶段都多少有课程讲解过，但是缺少实践的训练，很多人都忘得差不多了。\n  第二步自然就是阅读文献。如何快速地从大量的文献中筛选出高价值的文章，并且从这些文章中获得有用的信息，这些既需要大量的练习也有一些技巧，而刚进入科研领域的研究生在这一方面通常有些欠缺。\n  第三步就是管理文献了。少量的文献还可以较为从容进行存储和查找，但是如果当文献数量庞大，研究项目繁多，需要与其他研究组进行合作时，文献的管理工作就不可忽视了。建立良好的文献管理规范，对于研究组而言，是科研质量稳定且可持续性的重要保证2；对于个人而言，如果决定将科研作为自己的事业。那么阅读文献将有可能是伴随一生的工作3，在这个过程中积累下来的文献量将是很庞大，良好的文献管理习惯是发挥这些文献价值的重要途径之一。\n  文献检索是文献综述（Literature Review）的重要基础。文献综述的目的简单来讲就是为研究提供背景，对相关研究主题进行概述，了解相关知识点或者认知的前沿在哪里，以及还存在的差距（Research Gap）。文献综述是开展一项研究必不可少的环节，通过文献综述可以：\n\n增加对研究主题及其学术背景的熟悉程度。\n梳理并总结出研究的理论框架和方法。\n掌握目前所开展的研究在文献中的位置。\n归纳并提炼出当前开展的研究对相关领域的贡献。\n\n  文献综述的写作过程大体可以分为以下几个环节，如 图 1.1\n\n\n\n图 1.1: 文献综述的流程图\n\n\n  但是，本书中将不会介绍如何撰写文献综述，而是介绍文献综述过程中会涉及的几个重要但容易被忽视的的环节，具体如下：\n\n\n\n\n\n\n本章主要内容\n\n\n\n\n\n\n文献检索\n文献阅读\n文献管理"
  },
  {
    "objectID": "chapter_1.html#文献检索",
    "href": "chapter_1.html#文献检索",
    "title": "1  文献知识的获取与管理",
    "section": "1.2 文献检索",
    "text": "1.2 文献检索\n\n1.2.1 文献检索平台\n\n1.2.1.1 中文平台\n  中文文献资料检索平台主要有以下三个：\n\n中国知网：综合性数据库，最全面，文献更新最及时，也是应用最广泛的。\n万方：综合性数据库，每月更新一次。\n维普：综合性数据库，每半月更新一次。\n\n\n\n1.2.1.2 英文平台\n  英文文献的检索平台就相对丰富，各个出版集团基本都有自己的检索平台。首先简单介绍一下五大英文学术期刊出版集团：\n\n爱思唯尔（Elsevier）：1880年成立，总部在荷兰，现属于英国RELX Group集团，Lancet及其子刊、Cell及其子刊均属于该出版社，是目前出版量排名第一的出版集团，开发有自己的文献管理软件Mendeley。\n斯普林格（Springer）：德国出版集团，在2015年收购了Nature Publishing Group等几家出版社，现在旗下的出版社有：Nature、Springer、BioMed Central，出版量排名第二。\n约翰威立（Wiley）：美国出版集团，创始于1807年，主要集中在科学、工业、医学等学术领域的方向，神刊CA-A Cancer Journal for Clinicians就属于该出版社旗下。\n世哲（SAGE Publishing）：美国出版集团，创始于1965年，是世界第五大学术出版商。\n电气和电子工程师协会（IEEE）：IEEE主要致力于在电气、电子、计算机工程和与科学有关的领域开发和研究。\n\n  另外还有几个顶级期刊不属于任何出版集团：\n\nScience：由美国科学促进会（American Association for the Advancement of Science）出版。\nThe New England Journal of Medicine（NEJM）：由在马萨诸塞州医学会的NEJM出版社拥有和出版。\nThe Journal of the American Medical Association（JAMA）：美国医学会会刊，由美国医学会（American Medical Association）出版。\n\n\n\n\n\n\n\n关于开放获取期刊（Open Access）\n\n\n\n\n\n\n开放获取（Open Access）是近些年出现的一种学术期刊类别，是相对于之前closed access而言的。其实本质上是学术出版领域的一种新的商业模式，与pay-for-access model相对。\n以往的学术期刊都是由图书馆和学术机构订阅，每年都需要支付大额的订阅费用，而读者也只有拥有一定的权限才能阅读。而OA期刊是由论文作者支付出版费用，读者可以免费获取，在一定程度上降低了学术论文的获取门槛，有利于研究成果的传播。\n关于OA期刊目前还存在一定的争议，有部分学者认为发表OA期刊是在花钱买论文，而出版集团为了获取利益，存在降低发表论文质量的可能。但是，OA期刊的发展目前来看是一种趋势，许多顶级期刊也开始创办OA期刊，比如Nature Communications、Lancet Public Health、JAMA Network Open等等都是OA期刊。\n\n\n\n\n  了解了目前全球主要的学术期刊出版集团后，再介绍一下检索平台/数据库：\n\nWeb of Science（WoS）：WoS是由科睿唯安（Clarivate Analytics）公司开发的信息服务平台，数据来源于期刊、图书、专利、会议录、网络资源（包括免费开放资源）等。WoS所具有的三大引文索引系统（SCIE, SSCI, A&HCI）共收录了全球12400多种权威的、高影响力的国际学术期刊。WoS为非开放获取，提供的是收费服务，通常需要从学校图书馆进行访问。\nScienceDirect：是Elsevier旗下期刊的全文检索平台，为开放查询平台，可以查询Elsevie旗下出版期刊的论文和书籍等。\nScopus：是Elsevier于2004年11月推出的摘要和引文检索平台，虽然不直接提供全文下载服务，但是是目前全球规模最大的摘要和引文（A&I）数据库，涵盖了15000种科学、技术及医学方面的期刊，该数据库完整收录了Elsevier, Springer, Science等来自全球5000家出版社的20500多种经同行评议的出版物。\nWiley Online Library：是Wiley出版期刊的全文检索平台，开放查询。\nPubMed：美国国立医学图书馆国家生物技术信息中心提供的免费MEDLINE检索服务，主要提供免费的生物医学文摘型数据库检索服务。\nSCI-HUB：SCI-HUB当然是不可少的，以上平台提供的都是免费检索服务（WoS和Scopus除外），但是对于非OA期刊的文献下载是需要购买授权的，而SCI-HUB可以提供免费的文献下载服务。当然了，正是因为这一点，SCI-HUB一直在受到几大出版集团的打压。关于SCI-HUB是什么，看一下它的slogan就清楚了：the first website in the world to provide mass & public access to research papers。\n\n\n\n\n\n\n\n关于SCIE和SSCI：\n\n\n\n\n\n\nScience Citation Index Expanded (SCIE，科学引文索引，一般也统称为SCI) 历来被全球学术界公认为最权威的科技文献检索工具, 提供科学技术领域最重要的研究信息。共收录了8600多种自然科学领域的世界权威期刊，覆盖了176个学科领域。\nSocial Sciences Citation Index（SSCI，社会科学引文索引）是一个涵盖了社会科学领域的多学科综合数据库，共收录3000多种社会科学领域的世界权威期刊，覆盖了56个学科领域。\n\n\n\n\n  其实，中文也有与SCIE和SSCI相对应的期刊收录库，分别是CSCD和CSSCI，具体如下：\n\n\n\n\n\n\n关于CSCD和CSSCI：\n\n\n\n\n\n\n中国科学引文数据库(Chinese Science Citation Database，简称CSCD)。创建于1989年，收录我国数学、物理、化学、天文学、地学、生物学、农林科学、医药卫生、工程技术和环境科学等领域出版的中英文科技核心期刊和优秀期刊千余种。\n中文社会科学引文索引（Chinese Social Sciences Citation Index，缩写为CSSCI，一般也称作“C刊”或者“南大核心”）。由南京大学中国社会科学研究评价中心开发研制的数据库，用来检索中文社会科学领域的论文收录和文献被引用情况，是我国人文社会科学评价领域的标志性工程。目前收录包括法学、管理学、经济学、历史学、政治学等在内的25大类的500多种学术期刊。\n\n\n\n\n\n\n\n1.2.2 文献检索方法\n  检索文献是比较简单的一件事情，无非就是通过关键词查询文章，但是完成一项系统的文献检索工作并不容易，何为系统，那就是快速、准确且全面，想要做到这三点，就需要按照一定的程序进行文献的检索，也就是要制定好检索策略。\n  检索策略通常可以分为三个部分：\n\n检索目的：就是确定检索主题和检索范围。\n\n检索主题：通常根据研究问题来确定相关联的文献，比如研究医疗保险支付方式对医生诱导需求行为的影响，那么关联的文献将主要包括“医疗保险支付方式的效果”和“医生诱导需求”两个方面。\n检索范围：通常包括数据库、时间、文献类型（Article或Review等）、文献语言（中文或英文）。通常要对多个数据库检索，如WoS、Pubmed、Science Direct、Wiley等均需检索到，原因其实在前文已经给出，因为不同的数据库收录的文献范围不一样，为了避免遗漏，最后对多个数据库检索。\n\n检索词的选择：检索词可以是标题/摘要关键字、作者、医学MeSH主题词、索引号等。\n\n合理运用AND、OR、NOT、*、？等检索符。\n对于词组的检索，合理使用“”，如Health Policy和“Health Policy”检索得到的结果将不一样，前者得到的是Health OR Policy的结果，后者得到的是Health AND Policy的结果。\n\n检索结果入选和排除标准：也就是文献的筛选标准\n\n数据库收录情况，如SCIE和SSCI收录，通常而言，收录标准较高的期刊，其发表的文章质量更高。\n重视高被引论文。\n尽量选择5年内的研究，如果是对学术史进行梳理，时间范围可以放宽。\n\n检索结果的记录与存档：\n\n做好检索词以及检索时间的记录，以便日后核对和溯源检索结果。\n合理运用数据库的批量导出功能，将不同的检索词得到的结果分类导出。"
  },
  {
    "objectID": "chapter_1.html#文献阅读",
    "href": "chapter_1.html#文献阅读",
    "title": "1  文献知识的获取与管理",
    "section": "1.3 文献阅读",
    "text": "1.3 文献阅读\n  文献分为泛读和精读两种策略。因为文献的质量不一样，时间也有限，并非所有的文献都需要从头到尾的仔细阅读。\n\n泛读：顾名思义，就是泛泛而读，初步浏览Abstract、Introduction和Result之后，对研究问题、研究方法和研究发现有了解了，若发现其对自己的研究启示意义不大，即可无需再详细阅读。\n精读：即是对泛读过程中发现的好的文献，进行逐字阅读，除了完全掌握文献中的研究方法和研究结论外，也要对作者的图表展示、行文技巧进行学习，弄清楚作者是如何构思的，以便在自己写作过程中能够有所借鉴。\n文献阅读顺序：一篇科学论文的内容一般是按照Abstract、Introduction、Method、Result、Discussion和Conclusion的顺序印刷的（部分期刊除外），但是阅读时的顺序并非是这样，一般情况下先读Abstract，对全文有一个大致了解，这也是摘要的作用；然后再读Introduction，了解文章的研究问题、研究动机、研究的贡献；接着再需要读的应该是Result，从图表中掌握研究的主要发现，带着问题去看Discussion，了解作者是如何对研究结果进行解释，最后才是Method和Conclusion。"
  },
  {
    "objectID": "chapter_1.html#文献管理",
    "href": "chapter_1.html#文献管理",
    "title": "1  文献知识的获取与管理",
    "section": "1.4 文献管理",
    "text": "1.4 文献管理\n\n1.4.1 文献管理体系\n  如前文述，文献管理规范对于保证研究质量十分重要，本节就介绍如何形成个人的文献管理规范，搭建文献管理体系。所谓规范或是体系，其实就是一套可以重复并推广的标准化流程，文献管理体系主要包括两部分：\n\n一是文献及原始论文管理。\n二是阅读文献后的笔记管理。\n\n  这里介绍两种不同的管理文献的思路：\n\n项目式：也就是按照研究项目进行文献和笔记的管理，文献及笔记和其他项目文件存储在一起，这样的好处利于项目文件的管理，但是缺点也很明显，就是容易导致文献重复，且查找效率低。\n集中式：也就是将所有的文献和笔记集中管理（如 图 1.2），建立索引，我个人比较推荐此种方式，因为其具有较高的文献查找效率。\n\n\n\n\n图 1.2: 在Zotero中进行集中式文献管理\n\n\n\n\n\n\n\n\n关于文献的下载格式与命名规则\n\n\n\n\n\n\n建议统一下载为PDF格式，不推荐使用知网的CAJ格式，原因有二，一是CAJ格式需要CAJViewer阅读器才能打开，实在无必要多此一举，CAJViewer并没有比常见的PDF阅读器更优秀；二是CAJ是是中文本土格式，不利于对外进行文件交流。\n下载的文件最好建立起统一的命名规则，便于快速的查找并定位文献，如Author-Year-Short Title。 （Li and Zhou - 2005 - Political turnover and economic performance: the incentive role of personnel control in China）\n\n\n\n\n\n\n1.4.2 文献管理工具\n  主流的文献管理工具基本如 表 1.1 :\n\n\n表 1.1: 常用文献管理软件比较\n\n\n\n\n\n\n\n\n\n软件名\n开发商\n中文支持\n推荐指数\n是否收费\n\n\n\n\nEndNote\n科睿唯安（Clarivate Analytics）\n一般\n***\n收费\n\n\nNoteExpress\n北京爱琴海乐之\n优秀\n*\n免费\n\n\nZotero\n开源社区\n一般\n***\n免费\n\n\nMendely\n爱思唯尔（Elsevier）\n一般\n**\n免费\n\n\n\n\n  而对于文献笔记，可用的工具就非常多了，如印象笔记、Notebility、Obsidian等。Zotero 6.0版本以后也有非常好的文献笔记体验。推荐在选择文献笔记工具时应至少支持两项功能，一是可以云同步，二是支持Markdown。\n\n\n1.4.3 文献管理软件在论文写作中的应用\n  任何科学研究都是建立在前人的研究基础上的，因此论文、报告等学术成果的写作都需要引用参考文献。参考文献的引用本来不复杂，但是由于不同的出版方对参考文献的引用格式各不一样，就导致引用参考文献这件事情变得特别繁琐。\n  对于文献数量较少时，花费一定的时间手动去调整还可以接受，若是文献数量多加之文稿篇幅长，那么如果继续沿用人工手动方式就会占用非常多的时间，比如下面两种场景，但凡想一想就会觉得头大：\n\n科研活动中很常见的一种情况就是论文的投稿，而投稿过程中因为拒稿、撤稿等原因需要更换不同的期刊的情况十分常见，而不同的期刊都有各自的文献引用格式要求，如果每次改投期刊时都去手动进行修改，将会是一件十分费事且低效率的行为。\n对于动辄上百页的学位论文、结题报告等，其参考文献等引用数量都非常可观，而通常都会经历多轮次的修改。若是采用GB-7714的序号引用格式4，当文献引用的顺序在修改过程中被调整，那么就需要对最后对参考文献进行重新排序，这也会是一件让人头疼不已的事情。\n\n  幸运的是，以上工作可以由 表 1.1 中的文献管理软件完成。但是最好不要把这些软件仅当作文献引用的工具使用，其可以在整个文献知识的获取和管理中发挥非常有用的作用。"
  },
  {
    "objectID": "chapter_2.html",
    "href": "chapter_2.html",
    "title": "2  数据库操作",
    "section": "",
    "text": "在数据的发展历程中有过两次革命。第一次数据革命是近代科学诞生之时，实现了数据与科学研究的融合，数据在科学研究中的基础地位得到确立。对研究过程和结果赋予精确化的诉求，是近代科学的基本特征之一。在以数据为依据的研究范式中，数据的可靠性和准确性代表了研究的精确性，人们甚至将以数据为依据的实证研究作为判断 “科学”与“伪科学”的标准。第二次数据革命发生于因信息技术的发展而导致数据产生的速度和规模急剧发展之时，它不仅改变着科学研究范式，实现社会科学研究的定量化，也将促使经济、社会、军事等所有社会领域产生巨大的变革。1因此，掌握数据的分析能力是现代社会科学研究必须要掌握的技能。\n  现实社会中产生的数据是纷繁复杂的，有时甚至是杂乱无章的，并非我们所设想的那样拿到数据就可以跑模型。它需要经过一系列的处理过程，这个处理过程一般称作数据操作（Data Manipulate或者Data Wrangle）。在定量实证研究中，接近70%到80%的时间都需要花费在数据的清洗和预处理阶段 (Dasu and Johnson 2003)，然而很多青年学者都容易忽视这部分工作的重要性，而现实情况是，数据分析是没有捷径可以走的，机器学习也无法完成这部分工作，数据的质量决定了研究的质量，否则就是”Garbage in, Grabage out“。\n  一个基于数据开展实证研究（不论定性或是定量）的基本流程至少都应该包含以下步骤：\n\n\n\n\n\nflowchart LR\n  A[Collection] --> B(Digitalization)\n  B --> C{Clean}\n  C --> D[Analysis]\n  D --> E[Presentation]\n\n\n\n\n\n图 2.1: 数据分析流程图\n\n\n\n\n  不过，随着互联网技术的发展，现阶段基本可以采用一些调查工具将采集和数字化两个阶段同时进行，比如最常用的问卷星、腾讯问卷。国外的话有SurveyMonkey和LimeSurvey，也可以基于RedCap搭建自有的调查系统。\n\n\n\n\n\n\n本章就主要介绍数据操作（Data Manipulate）的主要内容和方法，包括：\n\n\n\n\n\n\n数据的采集原则\n数据的批量读取\n数据库转换\n数据整合"
  },
  {
    "objectID": "chapter_2.html#数据的采集原则",
    "href": "chapter_2.html#数据的采集原则",
    "title": "2  数据库操作",
    "section": "2.2 数据的采集原则",
    "text": "2.2 数据的采集原则\n\n2.2.1 数据的分类\n  在介绍数据采集原则之前，先说一下数据的分类，当然这里不讨论统计学上对数据的分类（如定性、定量、计数、计量等等），而是按照数据产生的途径将数据进行分类。我个人理解可以归纳为三类；\n\n调查数据（Survey data）：也就是通过针对个人或者机构的调查获得的数据，调查研究是开展社会科学研究的最主要方式，如田野调查、问卷调查、访谈等等。\n行为数据（Behavioral data）：即记录人或者机器行为的数据，如日常的网购数据、出行数据、够药数据、医保账户使用数据等等，最早应用于商业领域的用户行为分析。\n统计数据（Administration data）：即政府或者机构对于涉及经济、社会和日常职能业务相关的指标进行按时间加总之后的数据，如金融情况、财政情况、教育资源、卫生资源等等，严格来说统计数据是基于调查数据和行为数据进行聚合、统计、校正之后形成的。\n\n  我将三种数据的优缺点总结如 表 2.1\n\n\n表 2.1: 三种类型数据优缺点比较\n\n\n\n\n\n\n\n数据类型\n优点\n缺点\n\n\n\n\n调查数据\n1.容易组织，指的是小样本调查； 2.调查内容和质量相对可控； 3.分析简单。\n1.调查成本高，特别是大样本人群调查； 2.回忆偏倚、调查者偏倚、访员偏倚等不可避免。\n\n\n行为数据\n1.样本大，通常为海量数据； 2.时间和空间尺度细腻。\n1.获取较难，通常被机构和平台掌握； 2.对分析技能要求高； 3.通常混有非真实数据，如网购中的刷单，医保中的骗保等。\n\n\n统计数据\n1.较易获取。\n1.真实性难以验证； 2.时间和空间尺度粗糙； 3.指标不够丰富。\n\n\n\n\n\n\n2.2.2 数据库设计\n  数据库（Database）设计主要针对的是将调查问卷电子化，虽然这个工作看似简单，但是其对于后续的数据清洗和数据分析十分关键。如果数据库设计得不够合理，会给后续工作带来很多麻烦。很多研究人员往往都忽视了数据库设计的重要性。\n\n\n\n\n\n\n数据库的设计应该至少要做到以下三点：\n\n\n\n\n数据库的设计应该要考虑到数据分析的需求，如变量的设计、变量类型的约束、逻辑核查规则、纵向或横向结构等。\n数据库的设计应该与调查问卷设计同步进行，也就是当调查问卷定稿后，调查开始之前，就应将电子化的数据库搭建完成，并在预调查阶段对数据库进行修订。\n数据库的设计应该要重视版本管理，且必须由专人负责管理，这一点对于长期历时多年的追踪调查十分重要，必须要做到问卷版本与数据库版本对应。\n\n\n\n  下面针对以上三点分别举几例：\n\n关于数据库中变量设计最常见的就是多选题的变量设置，如对于以下一个多选题：\n\n\n您经常选择就诊的医疗机构有哪些？（多选）：1.村卫生室；2.诊所；3.卫生院；4.县区医院；5.省市医院。假设受访者回答；1，3，4。\n\n那么，数据库中变量设置有两种方法：\n\n设置1个变量（visit_institution），变量类型Char，长度为5，那么以上回答在录入时就是：134.\n设置5个变量（分别为：visit_village, visit_clinic, visit_thc, visit_county,visit_tert），变量类型分别为Num，长度为1，取值限定为0或1，那么以上回答在录入时就是，1，0，1，1，0。\n\n显然方法二对于描述性分析时更为友好。\n\n关于数据库的设计应该与调查问卷的设计同步进行，是为了保证提前对数据库和问卷进行检验和修订，及时对问卷中设置不合理的问题进行调整。若不同步进行就可能会带来一个问题，也就是在调查完成之后再进行数据库的建立，就很有可能在数据录入的过程中发现问卷中的问题难以在数据库中采取合理的方式进行录入。这一点对于采取电子化调查一样适用，因为问卷的形成通常是通过Word先完成的，电子化调查只是将数据调查过程和录入过程进行了合并。\n在说明数据库的版本管理为何重要之前，我们可以假设这样一个场景：\n\n\n有一项调查研究，需要历时5年，每一年进行一次追踪调查，每次调查完成之后进行数据的电子化，但是在第2年和第4年对问卷进行了补充和修订（这样的操作很常见）。\n\n那么，对于这样一个研究有几个问题是无法避免的：1）每次问卷调查人员会有变化；2）数据录入人员会有变动；3）数据库的建立人员会有变动。面对这样的问题，应该如何才能保证数据库管理的工作正常进行呢？我想一时间你很难想到很好的解决办法。\n在这里我给出三点建议：\n\n在问卷的footnote或者header里必须增加这些版本识别信息：起草人、生成日期、核准人、版本号。\n在数据库的footnote或者header里必须增加这些版本识别信息：起草人、生成日期、核准人、版本号、问卷版本号。\n数据的录入工作可以由不同的人完成，但是数据库的建立必须由专人负责，录入人员在录入时不得擅自建立数据库用于录入，且要保证录入问卷的版本号和数据库中记录的问卷版本号对应。\n\n\n\n\n\n2.2.3 变量名命名规范\n  变量名的命名是指为变量赋予一个简短的字符，用于在统计软件中作为变量的识别符号，一般需要遵循以下几项原则：\n\n尽量通过简短的单词或者符号来反映出变量的含义，推荐用英文单词。比如简单的：编号(id)、姓名(name)、性别(sex)，复杂一点的：家庭年收入(family_income)。这里不提倡用拼音的原因是为了保证研究的国际化，在研究过程中进行国际交流是十分常见的，如果采用拼音会在与国外的研究者进行交流时产生阻碍。同时也不提倡采用var1、var2这种没有规律的命名方式，因为难以对应出变量本身代表的含义。\n变量名不可用数字或者下划线开头，也不可包含. * ？- ！～等特殊字符。\n变量名的长度最好不要超过32个字符，如果过长会在统计分析的coding过程中增加不必要的键盘敲击量，增加额外的负担，影响效率。\n变量名可以采用驼峰命名法(familyIncome)、双峰命名法(FamilyIncome)、下划线法(family_income)等规则，不同的项目中会选择不同的命名规则，重要的是保持一致，尽量不要在同一个数据库或者项目中混合采用不同的命名方式。\n\n\n\n2.2.4 数据字典\n  数据字典（Data dictionary）也称作Codebook或者Specification，是指详细记录数据库中的变量名、变量内容、变量取值、变量标签、变量属性等信息，其作用主要是为了方便统计分析过程中检查和查看变量。\n  数据字典可以手动整理，也可以通过统计软件中的函数生成，如Stata中的codebook命令，R中也有codebookr和datadictionary两个package。\n  下面给出一个我之前整理的codebook示例，如 图 2.2 和 图 2.3\n\n\n\n\n\n\n图 2.2: Codebook封面信息\n\n\n\n\n\n\n\n\n\n图 2.3: Codebook内容\n\n\n\n\n\n\n\n2.2.5 数据库建立工具\n\n入门工具：EpiData，这个基本是各个学校开设数据库管理课程都会讲到的一个工具，但是EpiData比较古老了，如果没记错应该是从2008年之后就再未更新过。\n普通线上工具：问卷星、腾讯问卷，能满足基本的数据库建立需求，但是对于较大型的研究一般不推荐。\n专业线上工具：推荐LimeSurvey，有Cloud版和社区版，社区版可以通过自行搭建在云服务器或者本地服务器上。\n综合数据与分析平台：推荐RedCap(Research Electronic Data Capture)，RedCap是由范德堡大学Paul Harris教授团队自2004年开发的一个成熟开源、安全可靠、网络化的在线临床研究和试验数据库管理程序，基本国内外知名高校均搭建有自己服务器，西安交通大学也有，如下：\n\n\n\n\n图 2.4: 西安交通大学开放研究数据平台"
  },
  {
    "objectID": "chapter_2.html#数据的读取",
    "href": "chapter_2.html#数据的读取",
    "title": "2  数据库操作",
    "section": "2.3 数据的读取",
    "text": "2.3 数据的读取\n  数据读取是数据清洗和数据分析的第一步。为什么要单独把这一部分拿出来详细说明，是因为数据库管理工具通常与统计分析软件是分割开的。当然这里有一个例外，那就是MS EXCEL，很多人会把EXCEL既当成数据管理工具（虽然不推荐）又当作统计分析软件。数据读取虽然是很简单的一个操作，但是通常情况下会由于中文字符编码（Encoding）等问题导致在不同的统计软件之间转换时浪费一些时间和精力，所以有必要拿出来讲一讲。\n  尽管现在的数据管理软件，不管是古老的EpiData或是时下流行的RedCap，都可以支持直接导出适用于不同统计分析软件的特定格式，比如dta（支持Stata）、sas7bat（支持SAS）、sav（支持SPSS），但是我个人还是强烈推荐以CSV格式作为中介，因为几乎所有的统计软件都支持CSV数据格式。\n\n2.3.1 统计分析软件\n  顺带简单说一下目前在社会科学研究领域主要用的统计软件，如 表 2.2 。我个人推荐R和Python，因此后文关于coding部分均是基于R的。\n\n\n表 2.2: 社会科学研究中常用统计软件比较\n\n\n软件名\n说明\n推荐指数\n是否收费\n\n\n\n\nSPSS\n入门基础款\n*\n收费\n\n\nStata\n最流行款，学习门槛低，硬件要求低\n***\n收费\n\n\nSAS\n较流行，公共卫生领域研究人员使用多\n**\n收费\n\n\nR\n开源，学习门槛较高，第三方工具丰富\n***\n免费\n\n\nPython\n开源，学习门槛高，第三方工具较丰富\n**\n免费\n\n\n\n\n\n\n2.3.2 单个文件读取\n  在R中读取不同的数据格式还是相对比较方便的，主要有两种方式：\n\n基于base包的，可以相对较容易的读取CSV，如read.csv()。\n基于第三方包的，主要涉及readr、readxl、haven，均包含在tidyverse系列中：\n\nreadr：主要用于读取CSV，如read_csv()\nreadxl：主要用于读取EXCEL的xls和xlsx格式，如read_excel()\nhaven：主要用于读取SAS、SPSS和Stata格式数据文件，具体如下，摘自官方文档：\n\n\nSAS: read_sas() reads .sas7bdat + .sas7bcat files and read_xpt() reads SAS transport files (version 5 and version 8).\n\n\nSPSS: read_sav() reads .sav files and read_por() reads the older .por files. write_sav() writes .sav files.\n\n\nStata: read_dta() reads .dta files (up to version 15). write_dta() writes .dta files (versions 8-15).\n\n\n\n\n  由于以上package和函数都比较简单，参看官方文档之后就能熟练使用，因此不详细赘述。通过以上tidyverse系列的三个package，在R中基本可以应对绝大多数的数据读取问题。\n  另外，对于数据量较大的情况，如百万或者千万级别的数据记录条数时，以上package中的函数会耗时较多，遇到这种情况，推荐使用data.table中的fread()函数处理。尽管data.table是不同于tidyverse的另一套对于数据库的操作体系，但是data.table同时具备data.frame的属性，兼容tidyverse语法。\n  但是，fread()函数有一个不足之处，就是字符编码只支持UTF-8和Latin，无法支持中文字符常见的gb18030编码，略显遗憾。\n\n\n2.3.3 批量读取\n  批量读取数据是在统计分析过程中很常见的一个需求，具体分为两种情况。\n\n2.3.3.1 读取同一个文件夹中的多个文件\n  如，在一个名为data的的文件夹中有365个CSV文件，分别命名为china_cities_20210101…china_cities_20211231，分别记录了全国所有地级市每一天的空气质量数据，现在需要对空气质量进行分析，那么第一步就需要将这365个文件读取并整合进一个数据库中，面对这个问题，你首先会想到如何处理呢？\n  这里提供两种思路：\n\n根据文件命名，可以发现其中有一定的规律，也就是每个文件名自由最后的数字在变动，并且是按日期累加，那么就可以直接利用循环进行读取，因为思路较简单，代码略。\n以上这种情况并不常见，有时很难从文件名中发现规律，那么就只能先想办法获取所有文件名称，然后进行循环读取。\n\n\n由于文件全部为同一类型，因此可以使用list.files()函数进行获得（代码如下），其中path参数指定的是文件夹路径，pattern参数指定的是文件类型，其中*是通配符。\n剩下的工作就是循环读取了，可以使用if循环，但是不推荐，这里推荐使用purrr包中的map()函数，其实purrr是R自带的apply()函数族的高阶版本，由于是CSV文件，因此map()函数中FUNC参数使用readr包中read_csv()函数。\n最后一步就是将合并为一个data.frame，由于map()函数返回的是一个列表list，因此可以通过do.call()函数对list中的对象进行递归。\n也可以使用 map_dfr()直接省略第三步。\n\n\n#---------Mehotd 1---------------#\ncsv_list <- paste0(dir, list.files(path = dir, \n                                   pattern = \"*.csv\"))\ndf_list <- purrr::map(csv_list, readr::read_csv, \n                      locale = locale(encoding = \"UTF-8\"))  \ndf <- do.call(rbind, df_list)\n\n#---------Mehotd 2---------------#\ncsv_list <- paste0(dir, list.files(path = dir, \n                                   pattern = \"*.csv\"))\ndf <- purrr::map_dfr(csv_list, readr::read_csv, \n                     locale = locale(encoding = \"UTF-8\"))\n\n\n\n2.3.3.2 读取同一个EXCEL文件中的多个Sheet\n  这种情况在现实中更为常见，处理思路基本同上，第一步依然是获取所有Sheet的名称。这里需要使用readxl包中的excel_sheets()函数，具体代码如下：\n\nsheet_name <- readxl::excel_sheets(\"datasets.xlsx\")\n\ndf <- purrr::map_dfr(sheet_name, \n                     readxl::read_excel, \n                     path = \"datasets.xlsx\")\n\n\n\n\n\n\n\n有两点需要强调一下：\n\n\n\n\n\n\n所有的文件路径中最好不要有中文.\n只有当不同的文件或者Sheet中的数据结构完全一致时，才建议直接使用map_dfr()函数将文件合并，如果不一致，建议使用map()函数先保存为列表list，然后根据不同的分析需求进一步处理。"
  },
  {
    "objectID": "chapter_2.html#数据库转换",
    "href": "chapter_2.html#数据库转换",
    "title": "2  数据库操作",
    "section": "2.4 数据库转换",
    "text": "2.4 数据库转换\n  这一节的内容其实和 Section 3.2.2 关联比较紧密，通常会用在Tidy Data的整理过程中。数据库转换其实就是对数据库的行（Row）和列（Column）进行的一系列操作，可以对数据库的结构进行改变。\n\n2.4.1 长宽类型转换\n  也称作数据库的重构（Reshape），本质是对长数据（Long）和宽数据（Wide）形式之间进行转换。先简单说明一下长数据（Long）和宽数据（Wide）：\n\n长数据（Long）：不是指行比列多，虽然通常情况下这种数据结构确实是行比列多，但本质是指同一个个体2有多条观测值，比如面板数据（Panel data）就是长数据。英文表述可能更清楚：A “long” dataset contains more than one row per subject, and uses a unique ID to identify each subject.\n宽数据（Wide）：与长数据相对的形式，比较好理解了，就是每个个体只有一条观测值。\n\n  从网上借了一张图，很清晰的说明了长款数据之间的差异，如 图 2.5：\n\n\n\n图 2.5: 长款数据示意图，数据来源：https://www.statology.org/long-vs-wide-data/\n\n\n  在数据分析过程中，进行长宽数据之间的转换是经常会有的需求，这时你可能就会问了，比如呢！比如 表 2.3，一个微观调查的数据库，当在回归中需要控制时间固定效应时，你肯定希望数据是 表 2.3 (a) 形式，而当需要计算两次wave调查的income的差值时，你肯定希望数据是 表 2.3 (b) 形式。\n\n\n表 2.3: 长宽数据转换需求示例\n\n\n\n\n(a) 长数据示例\n\n\nID\nwave\nincome\n\n\n\n\n001\nWave 1\n1500\n\n\n001\nWave 2\n2000\n\n\n002\nWave 1\n2200\n\n\n002\nWave 2\n2800\n\n\n\n\n\n\n(b) 宽数据示例\n\n\nID\nwave1\nwave2\n\n\n\n\n001\n1500\n2000\n\n\n002\n2200\n2800\n\n\n\n\n\n\n  常见的统计软件也都提供了数据库长宽转换的方法，只是易用性方面有所不同。比如，Stata中有reshape命令，而在R中，目前用得最为广泛的是tidyr包中的pivot_longer()和pivot_wider()函数，而之前这两个函数的功能是由另外两个函数：gather() 和 spread()提供的，由于后两个函数不容易被记住（这是Hadley Wickham的原话），所以Wickham就重写了这两个函数。\n  pivot_longer()和pivot_wider()函数的功能在官方文档中解释得非常清楚了，直接搬运如下：\n\npivot_longer() makes datasets longer by increasing the number of rows and decreasing the number of columns.\npivot_longer() is commonly needed to tidy wild-caught datasets as they often optimise for ease of data entry or ease of comparison rather than ease of analysis.\n\n  以上两个函数的使用方式都特别简单和直观，参考官方文档即可，在此不再赘述，仅提供一个例子作为演示，这里还是使用 表 2.3 的例子\n\n构造数据库\n\n\nsurvey <- data.frame(\"ID\" = c(\"001\", \"001\", \n                              \"002\", \"002\", \n                              \"003\", \"003\"),\n                     \"wave\" = rep(c(\"Wave 1\", \n                                    \"Wave 2\"), 3),\n                     \"income\" = c(1500, 2000, \n                                  2200, 2800, \n                                  3000, 2400)                    \n                     )\n\nknitr::kable(survey)\n\n\n\n表 2.4: Dataframe of demo\n\n\nID\nwave\nincome\n\n\n\n\n001\nWave 1\n1500\n\n\n001\nWave 2\n2000\n\n\n002\nWave 1\n2200\n\n\n002\nWave 2\n2800\n\n\n003\nWave 1\n3000\n\n\n003\nWave 2\n2400\n\n\n\n\n\n\n\n长数据转换为宽数据\n\n\nlibrary(tidyr)\n\nsurvey_wide <- pivot_wider(survey, \n                           names_from = \"wave\", \n                           values_from = \"income\")\n\nknitr::kable(survey_wide)\n\n\n\n表 2.5: Dataframe of demo (Long to Wide)\n\n\nID\nWave 1\nWave 2\n\n\n\n\n001\n1500\n2000\n\n\n002\n2200\n2800\n\n\n003\n3000\n2400\n\n\n\n\n\n\n\n宽数据转换为长数据\n\n\nsurvey_long <- pivot_longer(survey_wide, \n                            cols = c(\"Wave 1\", \"Wave 2\"),\n                            names_to = \"wave\", \n                            values_to = \"income\")\n\n\nknitr::kable(survey_long)\n\n\n\n表 2.6: Dataframe of demo (Wide to Long)\n\n\nID\nwave\nincome\n\n\n\n\n001\nWave 1\n1500\n\n\n001\nWave 2\n2000\n\n\n002\nWave 1\n2200\n\n\n002\nWave 2\n2800\n\n\n003\nWave 1\n3000\n\n\n003\nWave 2\n2400\n\n\n\n\n\n\n\n\n2.4.2 数据库的连接\n  数据库的连接分为两大类：\n\n横向连接，一般称为merge或者join，指的是对列（Column）的扩展。\n纵向连接，一般称为append或者concat或者union，指的是对行（Row）的扩展。\n\n\n2.4.2.1 横向联接\n  在统计软件中一般使用merge()函数的较多，比如Stata和R中都具备此命令或者函数。不过现在使用join系列函数的比较多，可能更多的是为了和SQL语言统一，比如R中dplyr包提供了join_**()系列函数。横向连接一共分为六种情况，这里就用从网上借的一张 图 2.6 来说明:\n\n内联（inner join）\n左联（left join）\n右联（right join）\n全联（full join）\n外联（anti join）\n半连（semi join）\n\n  从图中可以很清晰地看出不同联接方式的差异。dplyr包中的join_**()系列函数也十分友好，查看官方文档之后就可以很快熟练使用，在此不再举例说明。\n\n\n\n图 2.6: dplyr包中的join系列函数，数据来源：https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\n\n\n\n\n\n\n\n\n使用merge或者join时应该注意的问题\n\n\n\n\n在联接之前，应该先对需要进行合并操作的数据库结构有熟悉的了解，特别是联接键（Identify variable）的情况，是否有重复？两个数据库之间是何种关系？这些情况都需要提前掌握。\n在进行联接操作之前，应该已经对联接后的结果有一个基本的预期和判断，这样才能保证连接操作的正确性。\n\n\n\n\n\n\n\n\n\nOne more thing: 提供两种一次联接多个数据库的方法\n\n\n\n\n方法一：利用管道函数（%>%）实现\n\n\ndf <- df1 %>%\n        left_join(df2, by='a') %>%\n        left_join(df3, by='a')\n\n\n方法二：利用purrr包中的reduce()函数\n\n\ndf <- purrr::reduce(list(df1, df2, df3), \n                    left_join, \n                    by = \"a\")\n\n\n\n\n\n2.4.2.2 纵向联接\n  纵向联接也有多种方法如R自带base包提供的rbind()函数，功能相对比较基本，以及dplyr包中的系列函数，如下：\n\nunion()：获得两个数据库行（Row）的并集，但会剔除重复的行。\nunion_all()：获得两个数据库行（Row）的并集，但会保留重复的行。\nintersec()：获得两个数据库行（Row）的交集，也就是只保留重复的行。\nsetdiff()：获得两个数据库行（Row）的差集，也就是只保留第一个数据库中与第二个数据不一样的行。\n\n  具体示例如下：\n\n构造数据库\n\n\ndf1 = data.frame(\"id\" = c(1:6), \n                 \"product\" = c(rep(\"Oven\", 3), \n                               rep(\"Television\", 3)))\ndf2 = data.frame(\"id\" = c(4:7),\n                 \"product\" = c(rep(\"Television\", 2), \n                               rep(\"Air conditioner\", 2)))\n\n  示例数据集的基本情况如 表 2.7 。\n\nknitr::kable(df1)\nknitr::kable(df2)\n\n\n表 2.7: 示例数据集\n\n\n\n\n(a) 1\n\n\nid\nproduct\n\n\n\n\n1\nOven\n\n\n2\nOven\n\n\n3\nOven\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n6\nTelevision\n\n\n\n\n\n\n(b) 2\n\n\nid\nproduct\n\n\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n6\nAir conditioner\n\n\n7\nAir conditioner\n\n\n\n\n\n\n\n\n纵向联接\n\n\n#---#\ndf_union <- dplyr::union(df1, df2)\nknitr::kable(df_union)\n#---#\ndf_union_all <- dplyr::union_all(df1, df2)\nknitr::kable(df_union_all)\n#---#\ndf_ints <- dplyr::intersect(df1, df2)\nknitr::kable(df_ints)\n#---#\ndf_setdiff <- dplyr::setdiff(df1, df2)\nknitr::kable(df_setdiff)\n\n\n表 2.8: 纵向联接操作\n\n\n\n\n(a) union\n\n\nid\nproduct\n\n\n\n\n1\nOven\n\n\n2\nOven\n\n\n3\nOven\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n6\nTelevision\n\n\n6\nAir conditioner\n\n\n7\nAir conditioner\n\n\n\n\n\n\n(b) union_all\n\n\nid\nproduct\n\n\n\n\n1\nOven\n\n\n2\nOven\n\n\n3\nOven\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n6\nTelevision\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n6\nAir conditioner\n\n\n7\nAir conditioner\n\n\n\n\n\n\n\n\n(c) intersect\n\n\nid\nproduct\n\n\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n\n\n\n\n(d) setdiff\n\n\nid\nproduct\n\n\n\n\n1\nOven\n\n\n2\nOven\n\n\n3\nOven\n\n\n6\nTelevision\n\n\n\n\n\n\n\n\n\n\n2.4.3 筛选观测值\n  筛选观测值，或者称作筛选行（Row），是非常常见的数据库操作，但是也比较简单，主要是和条件语句（Condition）结合使用。在Stata里就是通过keep、drop、in等命令进行行筛选操作。在R里主要有两种方法：\n\nbase包自带的函数：如subset()函数，或者切片等方法。\ndplyr包里的filter()函数。\n\n\n\n2.4.4 筛选变量\n  筛选变量，也称作筛选列（Column）或者筛选变量（Variable），同样是非常常见的数据库操作，一样也很简单。在Stata里也可以通过keep、drop等命令进行列筛选操作，是的你没看错，这两个命令可以同时进行行和列筛选。在R里主要有两种方法：\n\nbase包自带的函数：如subset()函数，或者切片等方法，是的你还是没看错，此方法可以同时进行行和列筛选。\ndplyr包里的select()函数，特别可以注意一下与starts_with()、ends_with()、contains()等函数联用3，可以极大提高变量选择的效率。另外，select()函数现在有三个拓展函数select_all()、select_at()、select_if()，针对不同的使用场景，如下：\n\nselect_if()：如果需要一次性筛选所有的数值型变量，那么可以df %>% select_if(is.numeric)\nselect_at()：如果需要对符合特定条件的变量进行某种操作，那么可以select_at(vars(-contains(\"ar\")), toupper)"
  },
  {
    "objectID": "chapter_2.html#数据聚合",
    "href": "chapter_2.html#数据聚合",
    "title": "2  数据库操作",
    "section": "2.5 数据聚合",
    "text": "2.5 数据聚合\n  上一节中主要讲述的是在不改变数据观测层级的情况下，对于数据库的一系列操作。然而在实际数据分析过程中，经常会遇见需要改变数据观察层级的情况。比如对一个以个人为最小观测单元的数据库，需要计算家庭或者县域为单位的平均收入，这种操作通常被称为数据聚合（Data Aggregating）。\n\n2.5.1 聚合运算\n  最常见的聚合运算就是：求均数、标准差、中位数、最大值、最小值、四分位数等。因为相对简单，在此就不再举例。这类运算的特点就是得到的新数据库观测值明显少于原始数据库。\n  需要注意的是，由于在15.0版本以前的Stata中每次只可读取一个数据库，当进行聚合运算时，运算结果通常以结果的形式输出在命令对话框中，尽管15.0版本以后增加了frame命令，可以实现将聚合运算结果同时存储在另一个数据库，但是操作十分不便利。然而，在很多场景下，需要再次调用聚合运算结果，这个时候在Stata中操作就相对复杂。\n\n\n2.5.2 分组计算\n  聚合运算很多时候都会与分组计算相结合，正如前文所举例一样，很多统计软件也提供了类似by的操作方法，这里只对R中的实现方法进行介绍。与之前一样，分组计算在R中有base语法和tidyverse语法两种实现方式：\n\n在base包中提供了aggregate()函数进行分组运算，示例如下：\n\n如 表 2.9 展示了datasets包中自带的一个数据集，可以看出一共有31个变量。\n\nlibrary(datasets)\ndata(mtcars)\nknitr::kable(head(mtcars))\n\n\n\n表 2.9: mtcars\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\n  假设现在需要计算不同的cyl气缸数（Number of cylinders）对应的hp马力（Gross horsepower）的均值，则可以：\n\nmtcars_hp_mean <- aggregate(hp ~ cyl, \n                            data = mtcars, \n                            FUN = mean)\n                            \nknitr::kable(head(mtcars_hp_mean))\n\n\n\n\ncyl\nhp\n\n\n\n\n4\n82.63636\n\n\n6\n122.28571\n\n\n8\n209.21429\n\n\n\n\n\n\n在dplyr包中提供了group_by()函数，结合管道函数（pipe function, %>%）和summarise()函数可以实现变量的分组运算，示例如下：\n\n\nlibrary(dplyr)\n\nmtcars_hp_mean <-\n  mtcars %>%\n    group_by(cyl) %>%\n    summarise(hp_mean = mean(hp))\n\nknitr::kable(head(mtcars_hp_mean))\n\n\n\n\ncyl\nhp_mean\n\n\n\n\n4\n82.63636\n\n\n6\n122.28571\n\n\n8\n209.21429\n\n\n\n\n\n  以上示例只是最基础的分组运算，在实际数据处理过程中，可以实现很丰富的需求。\n\n\n\n\nDasu, T., and T. Johnson. 2003. Exploratory Data Mining and Data Cleaning. John Wiley & Sons, Inc."
  },
  {
    "objectID": "chapter_3.html",
    "href": "chapter_3.html",
    "title": "3  数据的清洗",
    "section": "",
    "text": "正如前文所述，数据清洗（Data Cleaning）和数据准备（Data Preparation）工作通常会占掉整个数据分析过程70%到80%的时间，这一点基本被很多学者所认可，因此，应该清楚地认识到数据清洗工作的重要性。然而，很多学者，特别是还处在科研早期阶段的硕博士研究生，十分热衷于跑模型（Run model）。拿到数据既不详细摸索数据结构，也不观察变量分布情况，简单地描述一下数据后，就开始跑各种回归，目的只在发现统计显著的回归系数。这是一个不太好的研究习惯。"
  },
  {
    "objectID": "chapter_3.html#数据清洗相关概念",
    "href": "chapter_3.html#数据清洗相关概念",
    "title": "3  数据的清洗",
    "section": "3.2 数据清洗相关概念",
    "text": "3.2 数据清洗相关概念\n\n3.2.1 数据清洗（Data Cleaning）\n  何谓数据清洗？目前其实没有统一的定义，但是在不同的学科领域中，数据清洗的目标和内容基本是明确一致的，那就是识别、修改或删除不正确和不完整的数据，识别和删除重复信息和无关数据，以及按一定的标准统一数据格式、缺失值和拼写错误，规范数据结构，从而更好地为下一步分析工作服务。结合Microsoft、Amazon以及 (Ron 2008) 的观点，本书将常见的数据清洗步骤归纳如下：\n\n识别并处理异常值：异常值会对数据分布产生明显影响，干扰正常结果，并显著影响模型性能，通常进行删除。\n识别并处理缺失数据：缺失值是无法避免的问题，但需要掌握数据中的缺失特征并予以处理。\n识别并处理重复数据：删除重复信息。\n规范变量类型及取值：根据变量的属性特征，正确将其存储为数值、字符、日期、时间等类型，并统一取值类型。\n识别并处理不相关数据：确定特定分析的关键字段并从分析中删除不相关数据。\n识别并处理结构错误：识别数据库在录入和建立过程中出现的错误，如输入错误、错行等。\n\n\n\n3.2.2 整洁数据（Tidy Data）\n  整洁数据（Tidy Data）是由Posit公司（之前名称为Rstudio）的首席科学家Hadley Wickham提出的，初衷是为了简化数据清洗的流程，提高数据清洗效率。Wickham (2014) 认为Tidy Data更易于操作、建模和可视化，并且应该具备三种特征：\n\nEach variable is a column, and that column contains one “type” of data.\nEach observation is a row.\nEach type of observational unit is a table.\n\n  打眼一看，你可能觉得上面三句话说的是废话，心里想着大家不是都是这么建数据库的吗？其实不然，比如可以看下面的例子。\n  如 表 3.1 是日常工作和研究中非常常见的建数据库方式，但是这样的方式对于统计分析来说并不友好。比如当你想要知道每一科考试得分的最大值最小值时，你通常需要对数学、语文、英语重复三次同样的操作。\n\n\n表 3.1: 常见数据结构示例一\n\n\n姓名\n数学\n语文\n英语\n\n\n\n\n张三\n80\n90\n95\n\n\n李四\n88\n95\n97\n\n\n\n\n  再比如，当有多次考试的成绩需要记录时，很多人会选择 表 3.2 这样的方式，但是这样的结构在数据分析时会更不友好。\n\n\n表 3.2: 常见数据结构示例二\n\n\n姓名\n第一次\n\n\n第二次\n\n\n\n\n\n\n\n数学\n语文\n英语\n数学\n语文\n英语\n\n\n张三\n80\n90\n95\n85\n92\n97\n\n\n李四\n88\n95\n97\n87\n90\n98\n\n\n\n\n  符合Tidy Data要求的建立数据库方式应该如 表 3.3 所示，在这种结构下，结合Group by等操作，会给数据分析带来很大的便利。\n\n\n表 3.3: 常见数据结构示例三\n\n\n姓名\n考试\n学科\n得分\n\n\n\n\n张三\n第一次模拟考\n数学\n80\n\n\n李四\n第一次模拟考\n数学\n88\n\n\n张三\n第一次模拟考\n语文\n90\n\n\n李四\n第一次模拟考\n语文\n95\n\n\n张三\n第一次模拟考\n英语\n95\n\n\n李四\n第一次模拟考\n英语\n97\n\n\n张三\n第二次模拟考\n数学\n85\n\n\n李四\n第二次模拟考\n数学\n87\n\n\n张三\n第二次模拟考\n语文\n92\n\n\n李四\n第二次模拟考\n语文\n90\n\n\n张三\n第二次模拟考\n英语\n97\n\n\n李四\n第二次模拟考\n英语\n98\n\n\n\n\n  关于Tidy Data的内容还有很多，具体可以详细参考 Wickham (2014) 的文章，这里就不进行搬运工作了。在设计数据库的时候，只需要记住不要把数据展示形式当成数据库的结构形式就可以。"
  },
  {
    "objectID": "chapter_3.html#数据清洗内容",
    "href": "chapter_3.html#数据清洗内容",
    "title": "3  数据的清洗",
    "section": "3.3 数据清洗内容",
    "text": "3.3 数据清洗内容\n  由于异常值处理、重复值处理、规范变量类型及取值、识别并处理结构错误的内容相对比较简单，在本节中将不会详细举例说明。以下就异常值及缺失值的处理流程予以详细说明。一般情况下，数据清洗可以按照 图 3.1 所示流程进行。\n\n\n\n\n\nflowchart LR\n  A[概览] --> B(异常值识别处理)\n  B --> C{缺失值识别与处理}\n  C --> D[重复值识别与处理]\n  D --> E[标准化变量和取值]\n\n\n\n\n\n图 3.1: 数据清洗流程图\n\n\n\n\n\n3.3.1 数据清洗的第一步\n  数据清洗的第一步就是需要对数据库有一个整体的把握，如有多少个变量、多少条记录、变量类型如何、取值是什么等。在规范的研究项目（如注册的药物/器械临床试验）中，这些信息会记录在Codebook中随同数据一并交给统计师，通过翻阅Codebook就可以清楚地了解以上这些信息。这就是为何本书在 Section 2.2.4 中强调整理和制作Codebook的重要性。然而，很多时候，对于大部分的问卷调查数据而言，很少见到有研究者严格按照类似临床试验的要求建立和管理数据库的，以及对于从医院、互联网平台等处获取的数据而言，都不会事先提供这些信息，因此拿到数据库的第一步就是去了解这些信息。\n  一般我将这一步称为数据库的概览（Overview），在R中通常使用str()函数或者glimpse()函数可以大致对数据结构有一个初步了解，更为推荐的工具和方法可参见 Section 4.2.2 ，本节仅给出简单的示例，具体如下：\n\ndata(mtcars)\n\ndplyr::glimpse(mtcars[1:5, 1:6])\n\nRows: 5\nColumns: 6\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7\n$ cyl  <dbl> 6, 6, 4, 6, 8\n$ disp <dbl> 160, 160, 108, 258, 360\n$ hp   <dbl> 110, 110, 93, 110, 175\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440\n\n\n\n\n3.3.2 规范变量类型及取值\n  严格来说，这一步工作属于数据库管理的内容范畴，不应该在数据清洗阶段。但是，考虑到现在的研究，很多时候拿到的就是非常自然和原始的数据，这些工作也需要在数据清洗的时候进行。规范变量类型和取值的具体含义是指，数值型变量在数据库中的类型应该为Int或者Float，如年龄或者收入等指标；字符变量在数据库应该为String，如家庭住址或者出院诊断等指标；日期就应该Date，时间就应该是Time；另有一类较为特殊的是分类变量，如性别或学历等，可以用0、1、2等数字编码后，以Int类型存储，亦可直接以String类型存储为“男”和“女”等，两种方法各有利弊，根据研究习惯选择即可。\n\n\n3.3.3 异常值（Outliers）处理\n  异常值处理根据变量的类型不同采取的方法有所区别。对于数值型变量1 采用直方图（Histogram）、箱式图（Boxplot）或者小提琴图（Violin plot）可直观的了解异常值的分布情况；对于分类变量采用频数频率表或交叉表（Crosstable）可清晰地了解异常值情况。关于作图和作表方法可详见本书后续章节 。\n  关于数值型变量异常值的判定标准，大致可以分为以下三种方法：\n\n分位数法：通过计算1%和99%处的分位数，并将其作为上下界，界限之外的即为异常值。\n四分位数间距法：通过计算25%和75%的分位数，也就是四分位数Q1和Q3，以及其间距IQR，然后计算异常值的上下限分别为：Q1 - 1.5 x IQR和Q3 + 1.5 x IQR。\n标准差法：将数据均值加减3~5个标准差，定义为上下界。\n\n  关于数值型变量异常值的处理，如果是问卷数据，通常需要首先核对原始数据，排除因录入错误导致的数据异常，若排除此种原因，可采取截尾方法，剔除异常值。"
  },
  {
    "objectID": "chapter_3.html#缺失值missing-value处理",
    "href": "chapter_3.html#缺失值missing-value处理",
    "title": "3  数据的清洗",
    "section": "3.4 缺失值（Missing Value）处理",
    "text": "3.4 缺失值（Missing Value）处理\n  由于这部分内容较多，因此单独用一节来介绍，本部分主要参考 (高涛, 肖楠, and 陈钢 2013; Kabacoff 2013; van Buuren and Groothuis-Oudshoorn 2011)。Missing Value的处理也是非常重要的一个环节，因此规范的缺失数据的处理直接影响着研究结果的稳健性和真实性，也是研究能够Reproductive的重要保障。通常，处理缺失值的步骤如下：\n\n识别缺失数据。\n检查导致数据缺失的原因。\n删除包含缺失值的实例或是合理的数值代替（插补）缺失值。\n\n\n3.4.1 识别缺失数据\n\n3.4.1.1 缺失数据分类\n  在识别缺失数据之前，先来了解一下缺失数据的分类，它直接关系到处理缺失值方式的选择。\n\n\n完全随机缺失。若某变量的缺失数据与其他任何观测或未观测变量都不相关，则数据为完全随机缺失（MCAR）。若12个动物的做梦时长值缺失不是由于系统原因，那么可认为数据是MCAR。注意，如果每个有缺失值的变量都是MCAR，那么可以将数据完整的实例看做是对更大数据集的一个简单随机抽样。\n\n\n随机缺失 若某变量上的缺失数据与其他观测变量相关，与它自己的未观测值不相关，则数据为随机缺失（MAR）。\n\n\n非随机缺失 若缺失数据不属于MCAR或MAR，则数据为非随机缺失（NMAR）。\n\n\n  例如：\n\n（1）在最近一个问卷调查中，发现一些项常常一同缺失。很明显这些项聚集在一起，因为调查对象没有意识到问卷的第三页的背面也包含了这些项目。此时，可以认为这些数据是MCAR。\n\n（2）在一个关于全球领导风格的调查中，学历变量经常性地缺失。调查显示欧洲的调查对象更可能在此项目上留白，这说明某些特定国家的调查对象没有理解变量的分类。此时，这种数据最可能是MAR。\n\n  大部分处理缺失数据的方法都假定数据是MCAR或MAR。此时，你可以忽略缺失数据的生成机制。当数据是NMAR时，想对它进行恰当地分析比较困难，既要对感兴趣的关系进行建模，还要对缺失值的生成机制进行建模。\n\n\n3.4.1.2 识别缺失值\n  此次只用R来演示软件操作过程，Stata及SAS等其他软件可自行查阅相关文档。以下研究主要使用R的VIM包，以及该包中自带的sleep数据集进行演示。\nsleep数据集简介\n\n睡眠变量包含睡眠中做梦时长（Dream）、不做梦的时长（NonD）以及它们的和（Sleep）。体质变量包含体重（BodyWgt，单位为千克）、脑重（BrainWgt，单位为克）、寿命（Span，单位为年）和妊娠期（Gest，单位为天）。生态学变量包含物种被捕食的程度（Pred）、睡眠时暴露的程度（Exp）和面临的总危险度（Danger）。\n\n1) 以表格形式进行探索\n  主要使用的is.na()和complete.cases()函数。\n\n# 加载VIM包\nlibrary(VIM)\n\n# 加载数据集\ndata(sleep, package = \"VIM\")\n\n\n# 列出没有缺失值的行\nsleep[complete.cases(sleep[, 1:5]), 1:5]\n\n    BodyWgt BrainWgt NonD Dream Sleep\n2     1.000     6.60  6.3   2.0   8.3\n5  2547.000  4603.00  2.1   1.8   3.9\n6    10.550   179.50  9.1   0.7   9.8\n7     0.023     0.30 15.8   3.9  19.7\n8   160.000   169.00  5.2   1.0   6.2\n9     3.300    25.60 10.9   3.6  14.5\n10   52.160   440.00  8.3   1.4   9.7\n11    0.425     6.40 11.0   1.5  12.5\n12  465.000   423.00  3.2   0.7   3.9\n13    0.550     2.40  7.6   2.7  10.3\n15    0.075     1.20  6.3   2.1   8.4\n16    3.000    25.00  8.6   0.0   8.6\n17    0.785     3.50  6.6   4.1  10.7\n18    0.200     5.00  9.5   1.2  10.7\n19    1.410    17.50  4.8   1.3   6.1\n20   60.000    81.00 12.0   6.1  18.1\n22   27.660   115.00  3.3   0.5   3.8\n23    0.120     1.00 11.0   3.4  14.4\n25   85.000   325.00  4.7   1.5   6.2\n27    0.101     4.00 10.4   3.4  13.8\n28    1.040     5.50  7.4   0.8   8.2\n29  521.000   655.00  2.1   0.8   2.9\n32    0.005     0.14  7.7   1.4   9.1\n33    0.010     0.25 17.9   2.0  19.9\n34   62.000  1320.00  6.1   1.9   8.0\n35    0.122     3.00  8.2   2.4  10.6\n36    1.350     8.10  8.4   2.8  11.2\n37    0.023     0.40 11.9   1.3  13.2\n38    0.048     0.33 10.8   2.0  12.8\n39    1.700     6.30 13.8   5.6  19.4\n40    3.500    10.80 14.3   3.1  17.4\n42    0.480    15.50 15.2   1.8  17.0\n43   10.000   115.00 10.0   0.9  10.9\n44    1.620    11.40 11.9   1.8  13.7\n45  192.000   180.00  6.5   1.9   8.4\n46    2.500    12.10  7.5   0.9   8.4\n48    0.280     1.90 10.6   2.6  13.2\n49    4.235    50.40  7.4   2.4   9.8\n50    6.800   179.00  8.4   1.2   9.6\n51    0.750    12.30  5.7   0.9   6.6\n52    3.600    21.00  4.9   0.5   5.4\n54   55.500   175.00  3.2   0.6   3.8\n56    0.060     1.00  8.1   2.2  10.3\n57    0.900     2.60 11.0   2.3  13.3\n58    2.000    12.30  4.9   0.5   5.4\n59    0.104     2.50 13.2   2.6  15.8\n60    4.190    58.00  9.7   0.6  10.3\n61    3.500     3.90 12.8   6.6  19.4\n\n#列出有一个或多个缺失值的行\nsleep[!complete.cases(sleep[, 1:5]), 1:5]\n\n    BodyWgt BrainWgt NonD Dream Sleep\n1  6654.000   5712.0   NA    NA   3.3\n3     3.385     44.5   NA    NA  12.5\n4     0.920      5.7   NA    NA  16.5\n14  187.100    419.0   NA    NA   3.1\n21  529.000    680.0   NA   0.3    NA\n24  207.000    406.0   NA    NA  12.0\n26   36.330    119.5   NA    NA  13.0\n30  100.000    157.0   NA    NA  10.8\n31   35.000     56.0   NA    NA    NA\n41  250.000    490.0   NA   1.0    NA\n47    4.288     39.2   NA    NA  12.5\n53   14.830     98.2   NA    NA   2.6\n55    1.400     12.5   NA    NA  11.0\n62    4.050     17.0   NA    NA    NA\n\n\n  从结果可以看出，完整数据集中有62条observation，其中只有42条不含缺失值，20条含一个或多个缺失值。由于逻辑值TRUE和FALSE分别等价于数值1和0，可用sum()和mean()函数来获取关于缺失数据的有用信息。如：\n\nsum(is.na(sleep$Dream))\n\n[1] 12\n\nmean(is.na(sleep$Dream))\n\n[1] 0.1935484\n\nmean(!complete.cases(sleep))\n\n[1] 0.3225806\n\n\n  结果表明变量Dream有12个缺失值，19%的实例在此变量上有缺失值。另外，数据集中32%的实例包含一个或多个缺失值。另外一种方法是，采用mice包中的md.pattern()函数来生成缺失值矩阵，如下：\n\nlibrary(mice)\nmd.pattern(sleep[, 1:5], plot = FALSE)\n\n   BodyWgt BrainWgt Sleep Dream NonD   \n48       1        1     1     1    1  0\n10       1        1     1     0    0  2\n2        1        1     0     1    0  2\n2        1        1     0     0    0  3\n         0        0     4    12   14 30\n\n\n  结果中，最右侧列显示的是缺失的variable数量，0表示无缺失；最左侧列显示对应的observation数量，如第一行显示，有42条observation在BodyWgt等10个variable中均无缺失；第二行显示，Dream和NonD共2个变量同时有缺失的observation有9条。那么，数据集的20条缺失observation中，共包含缺失值(42 × 0) + (2 × 1) + … + (1 × 3) = 38个。\n2) 以图形探索缺失数据\n  虽然表格的输出也很简洁，但是图形的输出会更直观，此时可以采用VIM包中的aggr()和matrixplot()函数。\n\naggr(sleep, prop = FALSE, \n     numbers = TRUE, \n     labels = TRUE, )\n\n\n\n\n图 3.2: aggr()生成的sleep数据集的缺失值模式图形\n\n\n\n\n  如 图 3.2，左侧的红色条图分别显示了所有variable的缺失值数量，可以清楚的看出Gest变量有4个缺失值等。右侧的图中，红色代表缺失，可以看出无任何缺失的observation共有42条，同时了缺失NonD和Dream的observation共有9条。\n\nmatrixplot(sleep)\n\n\nClick in a column to sort by the corresponding variable.\nTo regain use of the VIM GUI and the R console, click outside the plot region.\n\n\n\n\n\n图 3.3: 按实例（行）展示真实值和缺失值的矩阵图\n\n\n\n\n  matrixplot()函数可生成展示每个实例数据的图形。此处，数值型数据被重新转换到[0, 1]区间，并用灰度来表示大小：浅色表示值小，深色表示值大。默认缺失值为红色。 图 3.3，横轴展示了全部的variable，纵轴显示了全部observation，从图中可以清楚的了解缺失值的分布情况，默认是按照第一个变量BodyWgt从小到大排序过。\n\n\n\n3.4.2 探索缺失原因或模式\n\n3.4.2.1 通过图形捕捉缺失原因或模式\n  通过Fig.2的矩阵图中，可以看出某些变量的缺失值模式是否与其他变量的真实值有关联。此图中可以看到，无缺失值的睡眠变量（Dream、NonD和Sleep）对应着较小的体重（BodyWgt）或脑重（BrainWgt）。此时，可以采用VIM包中的marginplot()函数可生成一幅散点图，在图形边界展示两个变量的缺失值信息，并观察变量之间的关系。\n\nmarginplot(sleep[c(\"Gest\", \"Dream\")], \n           col = c(\"darkgrey\", \"red\", \"blue\"), \n           pch = c(20))\n\n\n\n\n图 3.4: 变量Gest与Deam的散点图及缺失信息\n\n\n\n\n   图 3.4，共显示了两部分信息，（a）散点图（b）边界显示缺失信息，具体为：\n\n横轴为Gest变量，边界的红色boxplot显示的是Dream变量中缺失值对应的Gest值的箱式图，灰色为Dream变量中非缺失值对应的Gest值的箱式图。\n纵轴为Dream变量，边界的红色boxplot显示的是Gest变量中缺失值对应的Dream值的箱式图，灰色为Gest变量中非缺失值对应的Dream值的箱式图。\n灰色散点为Dream与Gest的散点图。\n左下角的蓝色数字表示Dream与Gest共同缺失的observation数量。\n\n  从 图 3.4 可以看出，Dream与Gest成负相关关系，并且，Gest的缺失值对应更大的Dream值，即缺失妊娠期数据时动物的做梦时长一般更长。\n\n\n3.4.2.2 通过相关性探索缺失值\n  图形展示了缺失变量之间可能的相关关系，进一步可用指示变量替代数据集中的数据（1表示缺失，0表示存在），这样生成的矩阵有时称作影子矩阵。求这些指示变量间和它们与初始（可观测）变量间的相关性，有助于观察哪些变量常一起缺失，以及分析变量“缺失”与其他变量间的关系。具体如下：\n\n# 管道函数包\nlibrary(magrittr)\n\n# 将是否缺失的逻辑变量转换为0和1变量的数据框形式\na <- abs(is.na(sleep[, 1:5])) %>%\n     as.data.frame()\n\nb <- apply(a, 2, sum) %>% \n       .[which(. != 0)] %>% \n       names()\n\n# 提取出缺失数据\nc <- subset(a, select = b)\n\n# 对缺失数据进行相关分析\ncor(c, y = NULL) \n\n           NonD     Dream     Sleep\nNonD  1.0000000 0.9071147 0.4862645\nDream 0.9071147 1.0000000 0.2037014\nSleep 0.4862645 0.2037014 1.0000000\n\n\n  此时，你可以看到Dream和NonD常常一起缺失（r = 0.91）。相对可能性较小的是Sleep和NonD一起缺失（r = 0.49），以及Sleep和Dream（r = 0.20）\n\n\n\n3.4.3 缺失值的处理\n  在对缺失数据进行识别和探索后，下一步就是对缺失数据进行填补，但是在进行下一步之前，需要前面的工作弄清楚几个问题，以便选择合理的缺失值处理方式：\n\n缺失数据的比例为多少。\n哪些变量上存在缺失，是否存在某种明显的分布趋势。\n缺失的类型，完全随机（MCAR）、随机(MAR)或非随机(NMAR)。\n\n\n3.4.3.1 缺失值处理方式\n  缺失值的处理通常有以下4种方法，其中前三种处理方式比较常用。除非特别必要，一般情况下较少会选择填补法。另外需要强调的是，若采取删除缺失值的处理方式，通常需要对删除缺失前的数据库和删除后的数据库进行比较，主要采用描述性统计方法检验关键的人口学特征是否存在差异。若不存在差异方可说明采用删除缺失值的方式是可取的，否则就会改变样本的代表性。\n\n不予处理：如果有一小部分数据（如小于10%）随机分布在整个数据集中（MCAR），那么你可以分析数据完整的实例，这样仍可以得到可靠且有效的结果。\n删除变量（列）：如果缺失数据集中在几个相对不太重要的变量上，那么你可以删除这些变量，然后再进行正常的数据分析.\n删除观测（行）：在完整的数据分析中，只有每个变量都包含了有效数据值的观测才会保留下来做进一步的分析。实际上，这样会导致包含一个或多个缺失值的任意一行都会被删除，因此常称作行删除法（listwise）、个案删除（case-wise）或剔除。大部分流行的统计软件包都默认采用行删除法来处理缺失值。如果你平时留心观察过，不管是在Stata或者SAS中，进行crostable、t-test、anova或者regression分析的时候，默认都是剔除了包含缺失值的记录的。 但是，行删除法是有一个前提假定的，即数据是完全随机缺失（MCAR）（即完整的观测只是全数据集的一个随机子样本）。如果不满足此条件，那么删除行是会产生有偏的结果。\n数据填补：根据研究设计不同，填补的方法有多种，如clinical trial中对于重复观察结果采用的末次观测结转(LOCF:Last observation carried forward)，以及平时常用的简单插补法（即用某个值（如均值、中位数或众数）来替换变量中的缺失值）。这里只介绍在面对复杂的缺失值问题时，常用的多重插补法(MI,Multiple imputation)，此处使用前面提到的R语言的mise包，其他软件也有类似的函数或者包。\n\n\n\n3.4.3.2 多重插补法(MI,Multiple imputation)\n原理：\n  MI是将从一个包含缺失值的数据集中生成一组完整的数据集（通常是3到10个）。每个模拟数据集中，缺失数据将用蒙特卡洛方法来填补。缺失值的插补通过Gibbs抽样完成。每个包含缺失值的变量都默认可通过数据集中的其他变量预测得来，于是这些预测方程便可用来预测缺失数据的有效值。该过程不断迭代直到所有的缺失值都收敛为止。对于每个变量，用户可以选择预测模型的形式（称为基本插补法）和待选入的变量。默认地，预测的均值用来替换连续型变量中的缺失数据，而Logistic或多元Logistic回归则分别用来替换二值目标变量（两水平因子）或多值变量（多于两水平的因子）。其他基本插补法包括贝叶斯线性回归、判别分析、两水平正态插补和从观测值中随机抽样。用户也可以选择自己独有的方法。\n语法：\n  函数mice()首先从一个包含缺失数据的数据框开始，然后返回一个包含多个（默认为5个）完整数据集的对象。每个完整数据集都是通过对原始数据框中的缺失数据进行插补而生成的。由于插补有随机的成分，因此每个完整数据集都略有不同。然后，with()函数可依次对每个完整数据集应用统计模型（如线性模型或广义线性模型），最后，pool()函数将这些单独的分析结果整合为一组结果。\n\nm为生成的数据集个数，默认为5个, seed是设定种子数，保证填补结果的可重复性，值可任意设定。\n导出需要的填补完整数据集，action表示选择哪一个。\n\n\nlibrary(mice) \nimp <- mice(df, m, \n            seed = 1234, method = \"pmm\") \n\nsleep_im <- complete(pooled, action = 3) \n\n  method为默认插补方式，pmm为默认方式预测均值匹配（Predictive mean matching）, 还有一些其他methods插补方法:\n\n贝叶斯线性回归（norm）。\n基于bootstrap的线性回归（norm.boot）。\n线性回归预测值（norm.predict）。\n分类回归树（cart）。\n随机森林（rf）。\n\n  使用这些插补方法对数据有严格的要求，比如贝叶斯线性回归等前三个模型都需要数据符合numeric格式，而pmm、cart、rf任意格式都行。\n使用以上模型遇见的问题有：\n\npmm相当于某一指标的平均值作为插补，会出现插补值重复的问题。\ncart以及rf是挑选某指标中最大分类的那个数字，是指标中的某一个数字，未按照规律。\n要使用norm.predict，必须先对数据进行格式转换，这个过程中会出现一些错误。\n\n实例：\n\nimp <- mice(sleep, seed = 1234, method = \"pmm\")\n\n\n iter imp variable\n  1   1  NonD  Dream  Sleep  Span  Gest\n  1   2  NonD  Dream  Sleep  Span  Gest\n  1   3  NonD  Dream  Sleep  Span  Gest\n  1   4  NonD  Dream  Sleep  Span  Gest\n  1   5  NonD  Dream  Sleep  Span  Gest\n  2   1  NonD  Dream  Sleep  Span  Gest\n  2   2  NonD  Dream  Sleep  Span  Gest\n  2   3  NonD  Dream  Sleep  Span  Gest\n  2   4  NonD  Dream  Sleep  Span  Gest\n  2   5  NonD  Dream  Sleep  Span  Gest\n  3   1  NonD  Dream  Sleep  Span  Gest\n  3   2  NonD  Dream  Sleep  Span  Gest\n  3   3  NonD  Dream  Sleep  Span  Gest\n  3   4  NonD  Dream  Sleep  Span  Gest\n  3   5  NonD  Dream  Sleep  Span  Gest\n  4   1  NonD  Dream  Sleep  Span  Gest\n  4   2  NonD  Dream  Sleep  Span  Gest\n  4   3  NonD  Dream  Sleep  Span  Gest\n  4   4  NonD  Dream  Sleep  Span  Gest\n  4   5  NonD  Dream  Sleep  Span  Gest\n  5   1  NonD  Dream  Sleep  Span  Gest\n  5   2  NonD  Dream  Sleep  Span  Gest\n  5   3  NonD  Dream  Sleep  Span  Gest\n  5   4  NonD  Dream  Sleep  Span  Gest\n  5   5  NonD  Dream  Sleep  Span  Gest\n\n\nWarning: Number of logged events: 5\n\nsleep_im <- complete(imp, action = 1)\n\n#查看填补情况\nimp$imp$Dream\n\n     1   2   3   4   5\n1  0.0 0.5 0.5 0.5 0.3\n3  0.5 1.4 1.5 1.5 1.3\n4  3.6 4.1 3.1 4.1 2.7\n14 0.3 1.0 0.5 0.0 0.0\n24 3.6 0.8 1.4 1.4 0.9\n26 2.4 0.5 3.9 3.4 1.2\n30 2.6 0.8 2.4 2.2 3.1\n31 0.6 1.3 1.2 1.8 2.1\n47 1.3 1.8 1.8 1.8 3.9\n53 0.5 0.5 0.6 0.5 0.3\n55 2.6 3.6 2.4 1.8 0.5\n62 1.5 3.4 3.9 3.4 2.2\n\n\n检查插补效果：\n  通过散点图和分布图，检查填补数据的效果，图 3.5 ，红色点表示填补值，蓝色表示非填补值。\n\nlibrary(lattice)\nxyplot(imp, Dream ~ Gest + Span, \n       pch = 18, cex = 1)\n\ndensityplot(imp)\nstripplot(imp, pch = 20, cex = 1.2)\n\n\n\n\n\n\n\n(a) 散点图\n\n\n\n\n\n\n\n(b) 密度图\n\n\n\n\n\n\n\n(c) 分布图\n\n\n\n\n图 3.5: Charts\n\n\n\n填补后数据集应用：\n  通常情况下，填补完成后，通常会进行回归分析，如前所示，MI填补生成了5个填补数据集，此时就会有一个疑问，到底选择哪一个作为最后的完整数据集。mice包提供了一个函数可以很容易的将5个填补数据集分别进行回归，然后将结果合并后返回一个结果。\n\nsleep_im_fit <- with(imp, \n                     lm(Sleep ~ Gest + Span))\nmodelsummary::modelsummary(sleep_im_fit,\n                           stars = TRUE,\n                           output = \"kableExtra\")\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    13.174*** \n  \n  \n     \n    (0.732) \n  \n  \n    Gest \n    −0.016*** \n  \n  \n     \n    (0.005) \n  \n  \n    Span \n    −0.020 \n  \n  \n     \n    (0.035) \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n  处理一般线性回归的lm()函数，其他方法包括广义线性模型glm()函数、广义可加模型gam()，负二项模型nbrm()函数均可实现。\n\n\n\n\nKabacoff, Robert. 2013. R in Action, Data Analysis and Graphics with r. Manning.\n\n\nRon, Cody. 2008. Cody’s Data Cleaning Techniques Using SAS. Cary, NC: SAS Institute Inc.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software 45 (3): 1–67. https://doi.org/10.18637/jss.v045.i03.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n高涛, 肖楠, and 陈钢. 2013. R语言实战. 人民邮电出版社."
  },
  {
    "objectID": "chapter_4.html",
    "href": "chapter_4.html",
    "title": "4  数据的描述",
    "section": "",
    "text": "记不太清楚是从什么时候开始，在很多青年学者的认识里描述性统计工作的重要性逐渐下降，越来越追求包含复杂模型的“高大上”研究，当然也包括我自己。简单分析之后，发现可能是这些年因果推断革命在社会科学领域的兴起，以及学术期刊对于“高大上”研究的偏好所引导的吧。\n  尽管这是现阶段很常见的一种现象，但是这并不代表数据描述在科研研究中就不重要了。通过回归模型获得因果推断证据的首要前提就是在现实世界中这个因果关系是客观存在的，而这就需要进行充分的数据描述和探索性分析去发现这种客观规律，如果数据描述过程中无法显示出较明显的因果关联，那么因果推断模型提示的显著性将值得怀疑。\n  数据的描述，或者是描述性统计分析，包含的内容其实比较简洁，就是数据的集中趋势（Central Tendency）、离散趋势（Variability）和分布（Frequency Distribution）。学习过统计学的人都知道均数标准差，但是想要做好一项描述性统计分析工作确是需要丰富的经验以及一定的耐心和精力，不过高质量的数据描述也会给整个研究带来非常大的收获。因此描述性统计分析也是探索性数据分析（Exploratory Data Analysis）的重要组成部分。\n  当然了，数据描述不单单只是为了弄清楚单个变量的集中、离散和分布情况，而是从杂乱的数据中，找到不同的变量之间的关联性，按照数据科学中的术语即是寻找个体不同属性或特征之间的关联性。尽管相关性不是因果关系，但是相关性往往是因果关系的前提，并且发现事物之间新的相关性也是十分有价值的，比如大家所熟知的“尿布与啤酒”的关系。\n  在本章中，依据数据的展现形式，我将数据描述分为表和图两大部分进行介绍。本来打算将数据可视化（Data Visualization）单独介绍，但是考虑到内容实在太多，精力有限无法面面俱到，又因这方面的数据和学习资料已经十分丰富了，故而放弃了单独介绍的想法。然而，一图胜千言，字不如表，表不如图，图形对于信息的展示能力是明显优于表格的，因此正确掌握可视化方法是描述统计一项重要的技能。\n\n\n\n\n\n\n本章就主要内容：\n\n\n\n\n\n\n数据的表格描述\n数据的可视化\n回归模型表格和图形的输出"
  },
  {
    "objectID": "chapter_4.html#数据的表格描述",
    "href": "chapter_4.html#数据的表格描述",
    "title": "4  数据的描述",
    "section": "4.2 数据的表格描述",
    "text": "4.2 数据的表格描述\n  在描述性统计分析中，表格常用于展示样本的基本信息和社会人口学特征，通常是一篇医学或者社会科学方向学术论文的第一个表格，一般也称之为Table One或者Summary Table。此外，也常用于不同组别之间研究所关注的结局变量分布情况的比较，比如展示参保病人和未参保病人的年卫生花费。\n  这一类表格具有共同的特点，一是展示的信息基本是固定的，对于连续型变量通常展示均数±标准差、中位数（四分位间距）、最大值、最小值等，对于分类变量通常展示频数（百分比）；二是展示的结果通常是需要通过聚合运算之后才能得出；三是重复工作量大。\n  这三个特点就使得完成这部分的描述工作简单但是费时，我想很多人应该都是通过利用统计软件分别计算得出每个需要描述的变量结果之后，然后再在MS Word或者MS Excel中进行手动整理。这样会带来一个明显的弊端，如果数据库发生变化，那么所有的工作均需要重新来过，再就是对于数据清洗和探索性分析时极其不友好1。在本书中就将会介绍如何快速地完成这项工作。\n\n4.2.1 R中表格描述的推荐工具\n  根据我自己的使用经验，整理出了几个在R中进行表格描述非常便利的工具，各个Package的特点可以在官方文档中进行查看，这里不赘述。近期我个人最推荐modelsummary、table1、gtsummary这三个包，以下内容也将利用它们分别进行演示。表 4.1 中统计描述是指支持自动完成变量的描述。\n\n\n表 4.1: R中支持输出符合统计规范表格的第三方包情况一览\n\n\n包名称\n支持功能\n推荐指数\n\n\n\n\nmodelsummary\n统计描述，回归模型\n***\n\n\ngtsummary\n统计描述，回归模型、自定义表格\n***\n\n\ntable1\n统计描述、自定义表格\n**\n\n\nstargazer\n回归模型\n*\n\n\ngt\n统计描述、自定义表格\n**\n\n\nkableExtra\n自定义表格\n**\n\n\nflextable\n自定义表格\n**\n\n\n\n\n\n\n4.2.2 快速生成Summary Table\n  这部分用卫生政策研究中常使用的公开微观数据库CHARLS进行演示，使用的是美国南加州大学社会经济研究中心（CESR）对原始数据进行整理后所提供的Harmonized CHARLS数据库。\n  首先，可以利用 Section 2.2.4 中提到的datadictionary包生成Harmonized CHARLS数据库的数据字典，以便于对数据库的基本信息进行概览。当然了，在实际的分析过程中，数据字典应该是在分析之前就已经准备好了，而数据概览操作通常使用str()函数或者glimpse()函数来进行，在这里用str()函数进行演示。从结果可以看出有50658条记录，19296个受访者。\n\nlibrary(datadictionary)\nlibrary(readr)\n\nharm <- \n  read_csv(\"dataset/harmonized_data_long_sub220522.csv\")\n\nstr(harm[, 1:10])\n\ntibble [50,658 × 10] (S3: tbl_df/tbl/data.frame)\n $ ...1       : num [1:50658] 1 2 3 4 5 6 7 8 9 10 ...\n $ ID         : num [1:50658] 1.01e+10 1.01e+10 1.01e+10 1.01e+10 1.01e+10 ...\n $ householdID: num [1:50658] 1.01e+08 1.01e+08 1.01e+08 1.01e+08 1.01e+08 ...\n $ communityID: num [1:50658] 101041 101041 101041 101041 101041 ...\n $ rdob       : Date[1:50658], format: \"1965-05-01\" \"1965-05-01\" ...\n $ ragender   : num [1:50658] 2 2 2 2 1 2 2 2 2 1 ...\n $ raeducl    : num [1:50658] 1 1 1 1 1 1 1 1 1 1 ...\n $ marriage   : num [1:50658] 1 1 1 1 1 1 1 1 1 1 ...\n $ wave       : num [1:50658] 1 2 3 4 1 1 2 3 4 4 ...\n $ ruralcounty: num [1:50658] 1 1 1 1 1 1 1 1 1 1 ...\n\n\n  这里简单选择4个典型的分类变量（ragender、raeducl、wave、ruralcounty），3个数值型变量（self_health、cesd10、mmse、hosfee）作为示例。由于选择的这四个分类变量在数据库中的类型为numeric2，这一点可以从以上的codebook结果中看出，因此为了方便进行后续的统计描述，建议先将分类变量的类型转换为character，代码如下。\n\nlibrary(dplyr)\nharm <- harm %>%\n    mutate_at(vars(c(\"ragender\", \"raeducl\", \n                     \"wave\", \"ruralcounty\")), \n              as.character)\n\n\n4.2.2.1 仅分类变量\n  只对分类变量进行描述的需求可能在论文撰写过程中不会经常出现，但是在对数据进行概览时，还是会经常需要。 表 4.1 中提到的package基本都具备此功能，但更推荐使用table1和gtsummary包。\n  如果只用于数据概览，不考虑输出成规范的表格，推荐使用modelsummary包中的datasummary_skim()函数，结果见 表 4.2 。\n\nlibrary(modelsummary)\n\ndatasummary_skim(harm[, c(\"ragender\", \"raeducl\", \n                          \"wave\", \"ruralcounty\")], \n                 type=\"categorical\")\n\n\n\n表 4.2:  利用modelsummary包对分类变量进行描述 \n \n  \n      \n       \n    N \n    % \n  \n \n\n  \n    ragender \n    1 \n    12800 \n    25.3 \n  \n  \n     \n    2 \n    37858 \n    74.7 \n  \n  \n    raeducl \n    1 \n    43984 \n    86.8 \n  \n  \n     \n    2 \n    5369 \n    10.6 \n  \n  \n     \n    3 \n    1297 \n    2.6 \n  \n  \n     \n    NA \n    8 \n    0.0 \n  \n  \n    wave \n    1 \n    11005 \n    21.7 \n  \n  \n     \n    2 \n    12102 \n    23.9 \n  \n  \n     \n    3 \n    13601 \n    26.8 \n  \n  \n     \n    4 \n    13950 \n    27.5 \n  \n  \n    ruralcounty \n    0 \n    22517 \n    44.4 \n  \n  \n     \n    1 \n    28141 \n    55.6 \n  \n\n\n\n\n\n\n  如果需要输出符合统计规范的表格，则推荐使用table1或者gtsummary包，结果见 表 4.3 。\n\nlibrary(table1)\nvars <- c(\"ragender\", \"raeducl\", \n          \"wave\", \"ruralcounty\", \n          \"self_health\", \"cesd10\",\n          \"mmse\", \"hosfee\")\n\nlbs <- c(\"性别\", \"受教育程度\", \n         \"访次\", \"城乡\", \n         \"自评健康\", \"认知水平\", \n         \"自我行动能力评分\", \"住院费用\")\n\nfor (i in 1:length(lbs)) {\n    label(harm[[vars[i]]]) <- lbs[i]\n}\n\ntable1(~ ragender + raeducl + \n         wave + ruralcounty, \n         data = harm)\n#---#\nlibrary(gtsummary)\ntbl_summary(harm[, c(\"ragender\", \"raeducl\", \n                     \"wave\", \"ruralcounty\")],\n           missing_text = \"(Missing)\",\n           label = list(ragender ~ \"Gender\",\n                        raeducl ~ \"Education\",\n                        wave ~ \"Wave\",\n                        ruralcounty ~ \"Rural or Not\"),\n           ) %>% as_gt()\n\n\n表 4.3: 利用table1和gtsummary包作仅分类变量描述表\n\n\n\n\n(a) table1包 \n\n\n\nOverall(N=50658)\n\n\n\n\n性别\n\n\n\n1\n12800 (25.3%)\n\n\n2\n37858 (74.7%)\n\n\n受教育程度\n\n\n\n1\n43984 (86.8%)\n\n\n2\n5369 (10.6%)\n\n\n3\n1297 (2.6%)\n\n\nMissing\n8 (0.0%)\n\n\n访次\n\n\n\n1\n11005 (21.7%)\n\n\n2\n12102 (23.9%)\n\n\n3\n13601 (26.8%)\n\n\n4\n13950 (27.5%)\n\n\n城乡\n\n\n\n0\n22517 (44.4%)\n\n\n1\n28141 (55.6%)\n\n\n\n\n\n\n\n\n\n(b) gtsummary包 \n  \n  \n    \n      Characteristic\n      N = 50,6581\n    \n  \n  \n    Gender\n\n        1\n12,800 (25%)\n        2\n37,858 (75%)\n    Education\n\n        1\n43,984 (87%)\n        2\n5,369 (11%)\n        3\n1,297 (2.6%)\n        (Missing)\n8\n    Wave\n\n        1\n11,005 (22%)\n        2\n12,102 (24%)\n        3\n13,601 (27%)\n        4\n13,950 (28%)\n    Rural or Not\n\n        0\n22,517 (44%)\n        1\n28,141 (56%)\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\n\n\n\n\n4.2.2.2 仅数值变量\n  modelsummary、table1、gtsummary均推荐。如果只用于数据概览，不考虑输出成规范的表格，推荐使用modelsummary包中的datasummary()或datasummary_skim()函数，其优点在于可以快速的了解数值型变量的分布情况，这两个函数之间其实是等价的，结果见 表 4.4 。如果需要将输出结果用于报告或者论文写作，方法同分类变量，此处不再举例说明。\n\ndatasummary(self_health + cesd10 + mmse +\n            hosfee ~ Mean + SD + Min + Max + Histogram,\n            data = harm,\n            output = \"default\" )\n\n\n\n表 4.4:  利用modelsummary包作仅数值变量描述表 \n \n  \n      \n    Mean \n    SD \n    Min \n    Max \n    Histogram \n  \n \n\n  \n    self_health \n    2.97 \n    0.97 \n    1.00 \n    5.00 \n    ▁▂▇▃▁ \n  \n  \n    cesd10 \n    8.37 \n    6.38 \n    0.00 \n    30.00 \n    ▇▆▄▃▂▂▁▁ \n  \n  \n    mmse \n    15.43 \n    5.03 \n    0.00 \n    30.00 \n    ▁▂▄▆▇▅▂▁ \n  \n  \n    hosfee \n    1724.53 \n    11988.19 \n    0.00 \n    1400000.00 \n    ▇ \n  \n\n\n\n\n\n\n\n\n4.2.2.3 分类和数值变量混合描述\n  将分类变量和数值变量同时进行描述是在论文写作中最常见的一种需求，推荐使用table1和gtsummary包。\n  table1，结果见 表 4.5 。\n\ntable1(~ ragender + raeducl + \n         ruralcounty + \n         self_health + cesd10 + \n         mmse + hosfee, \n         data = harm)\n\n\n\n表 4.5:  利用table1包生成分类和数值变量描述表（一） \n\n\n\nOverall(N=50658)\n\n\n\n\n性别\n\n\n\n1\n12800 (25.3%)\n\n\n2\n37858 (74.7%)\n\n\n受教育程度\n\n\n\n1\n43984 (86.8%)\n\n\n2\n5369 (10.6%)\n\n\n3\n1297 (2.6%)\n\n\nMissing\n8 (0.0%)\n\n\n城乡\n\n\n\n0\n22517 (44.4%)\n\n\n1\n28141 (55.6%)\n\n\n自评健康\n\n\n\nMean (SD)\n2.97 (0.966)\n\n\nMedian [Min, Max]\n3.00 [1.00, 5.00]\n\n\nMissing\n2440 (4.8%)\n\n\n认知水平\n\n\n\nMean (SD)\n8.37 (6.38)\n\n\nMedian [Min, Max]\n7.00 [0, 30.0]\n\n\nMissing\n3890 (7.7%)\n\n\n自我行动能力评分\n\n\n\nMean (SD)\n15.4 (5.03)\n\n\nMedian [Min, Max]\n16.0 [0, 30.0]\n\n\nMissing\n15173 (30.0%)\n\n\n住院费用\n\n\n\nMean (SD)\n1720 (12000)\n\n\nMedian [Min, Max]\n0 [0, 1400000]\n\n\nMissing\n951 (1.9%)\n\n\n\n\n\n\n\n  gtsummary，结果见 表 4.6 。\n\ntbl_summary(harm[, c(\"ragender\", \"raeducl\", \n                     \"wave\", \"ruralcounty\", \n                     \"self_health\", \"cesd10\",\n                     \"mmse\", \"hosfee\")],\n           missing_text = \"(Missing)\",\n           label = list(ragender ~ \"Gender\",\n                        raeducl ~ \"Education\",\n                        wave ~ \"Wave\",\n                        ruralcounty ~ \"Rural or Not\",\n                        self_health ~ \"Self report Health\",\n                        cesd10 ~ \"CESD 10\",\n                        mmse ~ \"MMSE\",\n                        hosfee ~ \"Hospital expense\"),\n           statistic = list(all_continuous() ~ \n                             \"{mean} ({sd})\")) %>% as_gt()\n\n\n\n\n\n表 4.6:  利用gtsummary包作分类和数值变量描述表（二） \n  \n  \n    \n      Characteristic\n      N = 50,6581\n    \n  \n  \n    Gender\n\n        1\n12,800 (25%)\n        2\n37,858 (75%)\n    Education\n\n        1\n43,984 (87%)\n        2\n5,369 (11%)\n        3\n1,297 (2.6%)\n        (Missing)\n8\n    Wave\n\n        1\n11,005 (22%)\n        2\n12,102 (24%)\n        3\n13,601 (27%)\n        4\n13,950 (28%)\n    Rural or Not\n\n        0\n22,517 (44%)\n        1\n28,141 (56%)\n    Self report Health\n\n        1\n4,626 (9.6%)\n        2\n6,770 (14%)\n        3\n24,569 (51%)\n        4\n9,759 (20%)\n        5\n2,494 (5.2%)\n        (Missing)\n2,440\n    CESD 10\n8 (6)\n        (Missing)\n3,890\n    MMSE\n15.4 (5.0)\n        (Missing)\n15,173\n    Hospital expense\n1,725 (11,988)\n        (Missing)\n951\n  \n  \n  \n    \n      1 n (%); Mean (SD)\n    \n  \n\n\n\n\n\n\n\n4.2.2.4 按变量分组后分类和数值变量混合描述\n  除上述几种情况之外，还有一种使用更频繁场景，那就是分组描述并进行统计检验，推荐使用gtsummary和table1包。\n  gtsummary，默认是进行数值变量进行Wilcoxon rank sum test，分类变量进行Pearson’s Chi-squared test。结果见 表 4.7 。\n\ntbl_mix <- \n    tbl_summary(harm[, c(\"ragender\", \"raeducl\", \n                        \"wave\", \"ruralcounty\", \n                        \"self_health\", \"cesd10\",\n                        \"mmse\", \"hosfee\")],\n                by = ruralcounty,\n                missing_text = \"(Missing)\",\n                label = list(ragender ~ \"Gender\",\n                            raeducl ~ \"Education\",\n                            wave ~ \"Wave\",\n                            ruralcounty ~ \"Rural or Not\",\n                            self_health ~ \"Self report Health\",\n                            cesd10 ~ \"CESD 10\",\n                            mmse ~ \"MMSE\",\n                            hosfee ~ \"Hospital expense\"),\n                statistic = list(all_continuous() ~ \n                                \"{mean} ({sd})\"),  \n            ) \n                     \ntbl_mix %>% add_p() %>% as_gt()\n\n\n\n\n\n表 4.7:  利用gtsummary包作分组分类和数值变量描述表（一） \n  \n  \n    \n      Characteristic\n      0, N = 22,5171\n      1, N = 28,1411\n      p-value2\n    \n  \n  \n    Gender\n\n\n<0.001\n        1\n6,996 (31%)\n5,804 (21%)\n\n        2\n15,521 (69%)\n22,337 (79%)\n\n    Education\n\n\n<0.001\n        1\n17,409 (77%)\n26,575 (94%)\n\n        2\n3,905 (17%)\n1,464 (5.2%)\n\n        3\n1,197 (5.3%)\n100 (0.4%)\n\n        (Missing)\n6\n2\n\n    Wave\n\n\n0.8\n        1\n4,933 (22%)\n6,072 (22%)\n\n        2\n5,386 (24%)\n6,716 (24%)\n\n        3\n6,026 (27%)\n7,575 (27%)\n\n        4\n6,172 (27%)\n7,778 (28%)\n\n    Self report Health\n\n\n<0.001\n        1\n2,263 (11%)\n2,363 (8.8%)\n\n        2\n3,467 (16%)\n3,303 (12%)\n\n        3\n11,280 (53%)\n13,289 (49%)\n\n        4\n3,413 (16%)\n6,346 (24%)\n\n        5\n813 (3.8%)\n1,681 (6.2%)\n\n        (Missing)\n1,281\n1,159\n\n    CESD 10\n7 (6)\n9 (7)\n<0.001\n        (Missing)\n1,970\n1,920\n\n    MMSE\n16.6 (4.8)\n14.3 (5.0)\n<0.001\n        (Missing)\n5,216\n9,957\n\n    Hospital expense\n2,238 (15,889)\n1,317 (7,548)\n<0.001\n        (Missing)\n514\n437\n\n  \n  \n  \n    \n      1 n (%); Mean (SD)\n    \n    \n      2 Pearson's Chi-squared test; Wilcoxon rank sum test\n    \n  \n\n\n\n\n\n  如果想将默认的Wilcoxon rank sum test改成Student’s t-test，可以如下操作，结果见 表 4.8 。\n\nttest_func <- function(data, variable, by, ...) {\n\n    if (is.numeric(data[[variable]])) {\n        t.test(data[[variable]] ~ \n               as.factor(data[[by]])) %>%\n        broom::tidy() %>%\n        select(statistic, p.value)\n    } else {\n        chisq.test(table(data[[variable]], \n                         as.factor(data[[by]]))) %>%\n        broom::tidy() %>%\n        select(statistic, p.value)\n    }\n\n}\n\ntbl_mix %>%\n  add_stat(fns = everything() ~ ttest_func) %>%\n  as_gt()\n\n\n\n\n\n表 4.8:  利用gtsummary包作分组分类和数值变量描述表（二） \n  \n  \n    \n      Characteristic\n      0, N = 22,5171\n      1, N = 28,1411\n      statistic\n      p.value\n    \n  \n  \n    Gender\n\n\n722\n<0.001\n        1\n6,996 (31%)\n5,804 (21%)\n\n\n        2\n15,521 (69%)\n22,337 (79%)\n\n\n    Education\n\n\n3,364\n<0.001\n        1\n17,409 (77%)\n26,575 (94%)\n\n\n        2\n3,905 (17%)\n1,464 (5.2%)\n\n\n        3\n1,197 (5.3%)\n100 (0.4%)\n\n\n        (Missing)\n6\n2\n\n\n    Wave\n\n\n0.997\n0.8\n        1\n4,933 (22%)\n6,072 (22%)\n\n\n        2\n5,386 (24%)\n6,716 (24%)\n\n\n        3\n6,026 (27%)\n7,575 (27%)\n\n\n        4\n6,172 (27%)\n7,778 (28%)\n\n\n    Self report Health\n\n\n-22.9\n<0.001\n        1\n2,263 (11%)\n2,363 (8.8%)\n\n\n        2\n3,467 (16%)\n3,303 (12%)\n\n\n        3\n11,280 (53%)\n13,289 (49%)\n\n\n        4\n3,413 (16%)\n6,346 (24%)\n\n\n        5\n813 (3.8%)\n1,681 (6.2%)\n\n\n        (Missing)\n1,281\n1,159\n\n\n    CESD 10\n7 (6)\n9 (7)\n-37.2\n<0.001\n        (Missing)\n1,970\n1,920\n\n\n    MMSE\n16.6 (4.8)\n14.3 (5.0)\n42.6\n<0.001\n        (Missing)\n5,216\n9,957\n\n\n    Hospital expense\n2,238 (15,889)\n1,317 (7,548)\n7.92\n<0.001\n        (Missing)\n514\n437\n\n\n  \n  \n  \n    \n      1 n (%); Mean (SD)\n    \n  \n\n\n\n\n\n  table1，结果见 表 4.9 。\n\npvalue <- function(x, ...) {\n\n    y <- unlist(x)\n    g <- factor(rep(1:length(x), \n                   times = sapply(x, length)))\n    if (is.numeric(y)) {\n        p <- t.test(y ~ g)$p.value\n    } else {\n        p <- chisq.test(table(y, g))$p.value\n    }\n    c(\"\", sub(\"<\", \"&lt;\", \n              format.pval(p, digits = 3, \n                          eps = 0.001)))\n}\n#---#\ntable1(~ ragender + raeducl + \n         self_health + cesd10 + \n         mmse + hosfee | ruralcounty, \n         overall = FALSE, \n         extra.col = list(\"P-value\" = pvalue),\n         data = harm)\n\n\n\n表 4.9:  利用table1包生成分组后分类和数值变量描述表 \n\n\n\n0(N=22517)\n1(N=28141)\nP-value\n\n\n\n\n性别\n\n\n\n\n\n1\n6996 (31.1%)\n5804 (20.6%)\n<0.001\n\n\n2\n15521 (68.9%)\n22337 (79.4%)\n\n\n\n受教育程度\n\n\n\n\n\n1\n17409 (77.3%)\n26575 (94.4%)\n<0.001\n\n\n2\n3905 (17.3%)\n1464 (5.2%)\n\n\n\n3\n1197 (5.3%)\n100 (0.4%)\n\n\n\nMissing\n6 (0.0%)\n2 (0.0%)\n\n\n\n自评健康\n\n\n\n\n\nMean (SD)\n2.86 (0.940)\n3.06 (0.976)\n<0.001\n\n\nMedian [Min, Max]\n3.00 [1.00, 5.00]\n3.00 [1.00, 5.00]\n\n\n\nMissing\n1281 (5.7%)\n1159 (4.1%)\n\n\n\n认知水平\n\n\n\n\n\nMean (SD)\n7.16 (5.86)\n9.31 (6.61)\n<0.001\n\n\nMedian [Min, Max]\n6.00 [0, 30.0]\n8.00 [0, 30.0]\n\n\n\nMissing\n1970 (8.7%)\n1920 (6.8%)\n\n\n\n自我行动能力评分\n\n\n\n\n\nMean (SD)\n16.6 (4.80)\n14.3 (5.00)\n<0.001\n\n\nMedian [Min, Max]\n17.0 [0, 30.0]\n15.0 [0, 30.0]\n\n\n\nMissing\n5216 (23.2%)\n9957 (35.4%)\n\n\n\n住院费用\n\n\n\n\n\nMean (SD)\n2240 (15900)\n1320 (7550)\n<0.001\n\n\nMedian [Min, Max]\n0 [0, 1400000]\n0 [0, 250000]\n\n\n\nMissing\n514 (2.3%)\n437 (1.6%)\n\n\n\n\n\n\n\n\n  如果进行数据探索，可以使用modelsummary中的datasummary_balance()的函数快速进行组间比较及描述，结果见 表 4.10 。\n\ndatasummary_balance(~ ruralcounty,\n                    dinm = FALSE,\n                    dinm_statistic = \"p.value\",\n                    data = harm[, \n                           c(\"ragender\", \"raeducl\", \n                             \"wave\", \"ruralcounty\", \n                             \"self_health\", \"cesd10\",\n                             \"mmse\", \"hosfee\")])\n\n\n\n表 4.10:  利用modelsummary包作分组后分类和数值变量描述表 \n \n\n\n0 (N=22517)\n1 (N=28141)\n\n  \n      \n       \n    Mean \n    Std. Dev. \n    Mean \n    Std. Dev. \n  \n \n\n  \n    self_health \n     \n    2.9 \n    0.9 \n    3.1 \n    1.0 \n  \n  \n    cesd10 \n     \n    7.2 \n    5.9 \n    9.3 \n    6.6 \n  \n  \n    mmse \n     \n    16.6 \n    4.8 \n    14.3 \n    5.0 \n  \n  \n    hosfee \n     \n    2238.3 \n    15889.1 \n    1316.5 \n    7548.3 \n  \n  \n     \n     \n    N \n    Pct. \n    N \n    Pct. \n  \n  \n    ragender \n    1 \n    6996 \n    31.1 \n    5804 \n    20.6 \n  \n  \n     \n    2 \n    15521 \n    68.9 \n    22337 \n    79.4 \n  \n  \n    raeducl \n    1 \n    17409 \n    77.3 \n    26575 \n    94.4 \n  \n  \n     \n    2 \n    3905 \n    17.3 \n    1464 \n    5.2 \n  \n  \n     \n    3 \n    1197 \n    5.3 \n    100 \n    0.4 \n  \n  \n     \n    NA \n    6 \n    0.0 \n    2 \n    0.0 \n  \n  \n    wave \n    1 \n    4933 \n    21.9 \n    6072 \n    21.6 \n  \n  \n     \n    2 \n    5386 \n    23.9 \n    6716 \n    23.9 \n  \n  \n     \n    3 \n    6026 \n    26.8 \n    7575 \n    26.9 \n  \n  \n     \n    4 \n    6172 \n    27.4 \n    7778 \n    27.6 \n  \n\n\n\n\n\n\n\n\n\n4.2.3 回归模型的输出\n  回归结果的表格输出虽然不属于描述性分析的内容，但是与本节内容关联性较大，因此在此一并介绍了。推荐modelsummary包，相比于另一个用于回归模型输出的stargazer包而言，其支持的模型函数更全面，并且输出的文件类型也更全面，比如html、latex、word等，示例结果如 ?tbl-reg 。modelsummary包不仅仅只是将回归结果输出，也拥有较丰富的自定义功能，比如可以指定输出的回归变量、回归参数等。同时也可以直接通过vcov参数将值传递给sandwich包来实现稳健标准误的计算。关于其支持的模型类型和稳健标准误计算方法，可详见modelsummary 。\n\nlibrary(plm)\nlibrary(modelsummary)\n\ndata(\"Grunfeld\", package=\"plm\")\n\ngrun.fe <- plm(inv ~ value + capital, index=c(\"firm\",\"year\"),\n               data = Grunfeld, model = \"within\")\ngrun.re <- plm(inv ~ value + capital, index=c(\"firm\",\"year\"),\n               data = Grunfeld, model = \"random\")\n\nmodelsummary(list(\"Fixed Effect\" = grun.fe,\n                  \"Random Effect\" = grun.re), \n             vcov = \"robust\",\n             stars = TRUE,\n             output = \"kableExtra\")\n\n\n利用modelsummary包输出回归结果\n \n  \n      \n    Fixed Effect \n    Random Effect \n  \n \n\n  \n    value \n    0.110*** \n    0.110*** \n  \n  \n     \n    (0.012) \n    (0.010) \n  \n  \n    capital \n    0.310*** \n    0.308*** \n  \n  \n     \n    (0.017) \n    (0.017) \n  \n  \n    (Intercept) \n     \n    −57.834* \n  \n  \n     \n     \n    (28.899) \n  \n  \n    Num.Obs. \n    200 \n    200 \n  \n  \n    R2 \n    0.767 \n    0.770 \n  \n  \n    R2 Adj. \n    0.753 \n    0.767 \n  \n  \n    AIC \n    2147.6 \n    2159.0 \n  \n  \n    BIC \n    2157.5 \n    2172.2 \n  \n  \n    RMSE \n    51.16 \n    52.39 \n  \n  \n    Std.Errors \n    HC3 \n    HC3 \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n\n4.2.4 小结\n  如果你和我一样有选择焦虑，可选项多了反而不知所措了，那么就直接看 表 4.11 。\n\n\n表 4.11: 支持输出符合统计规范表格的packages总结\n\n\n需求\n首推荐\n次推荐\n最次推荐\n\n\n\n\n描述表格\ngtsummary\ntable1\nmodelsummary\n\n\n回归结果\nmodelsummary\nstargazer\ngtsummary"
  },
  {
    "objectID": "chapter_4.html#数据的可视化",
    "href": "chapter_4.html#数据的可视化",
    "title": "4  数据的描述",
    "section": "4.3 数据的可视化",
    "text": "4.3 数据的可视化\n  统计图形的种类非常丰富，并且随着可视化工具的极大发展，目前数据可视化的展示方法越来越多样化。这样带来的好处是可以充分发挥出研究者的想象力，不好的地方是眼花缭乱的可视化方式容易造成研究人员选择困难，并且难以找到各类可视化方式的是用场景。在本节中，将进行总结并给出一定的示例，以下示例均基于R的ggplot2包实现。\n  统计图形可以归纳为四大类，这四类图形基本可以涵盖大部分的数据可视化需求。\n\n分布型：Distribution of a single variable\n关系型：Relationship between two variables\n构成型：Composition of a single or multiple variables\n对比型：Comparison between different categories/individuals\n\n\n4.3.1 分布型图形\n\n4.3.1.1 直方图\n  直方图（Histogram）是直观了解连续型变量分布情况的首选和最有效手段之一。一般情况下用于单个变量分布的观察，如 图 4.1 (a) ，但是若需要进行同一个变量不同组间，或是不同变量之间的比较时，亦可在同一个图形中展示，如 图 4.1 (b) 。\n\nlibrary(ggplot2)\n\nset.seed(2022)\ndf <- data.frame(\"value\" = c(rnorm(1000, 6, 2), \n                             rnorm(1000, 8, 4)),\n                 \"group\" = c(rep(\"Rural\", 1000), \n                             rep(\"Urban\", 1000))\n                )\n\n# Univariate\nggplot(df) +\n  geom_histogram(aes(x = value, \n                     y = ..density..),\n                     bins = 50,\n                     fill = \"blue\",\n                     color = \"black\") +\n  geom_density(aes(x = value), \n               alpha = 0.5, \n               size = 1.2,\n               color = \"red\") +\n  labs(x = \"Indicator\",\n       y = \"Density\") +\n  theme_bw()\n\n# Multivariates\nggplot(df) +\n  geom_histogram(aes(x = value, \n                     y = ..density.., \n                     fill = group),\n                     bins = 50,\n                     alpha = 0.5,\n                     position = \"identity\",\n                     color = \"black\") +\n  geom_density(aes(x = value, color = group), \n               alpha = 0.5, \n               size = 1.2) +\n  labs(x = \"Indicator\",\n       y = \"Density\") +\n  theme_bw() +\n    theme(legend.position = c(0.1, 0.85))\n\n\n\n\n\n\n\n(a) Univariate Histogram\n\n\n\n\n\n\n\n(b) Multivariates Histogram\n\n\n\n\n图 4.1: Histogram\n\n\n\n\n\n4.3.1.2 箱图和小提琴图\n  箱图（Boxplot）是另一种观察数据分布情况的统计图形，箱子的上横线表示上四分位数（Q75），下横线表示下四分位数（Q25），中间的横线为中位数（Median），上下延长线的末端表示中位数加减1.5倍的四分位间距，如 图 4.2 (a) ，箱图在识别异常值时可以提供很大的帮助。\n  箱图（Boxplot）的缺点也很明显，就是无法观测到数据的聚集和离散情况，如在哪一区间数据分布较为集中。为了弥补这一不足，可以将小提琴图（Violin plot）或是Jitter plot与箱图结合，也可以单独使用，如 图 4.2 (b) ，图中较胖的区域说明数据分布较为集中。\n\np <- \n    ggplot(df, aes(x = group, y = value, \n                   fill = group)) + \n    geom_boxplot(width = 0.1) +\n    theme_bw() +\n      theme(legend.position=\"none\")\np\n\n#---#\np +\n  geom_violin(trim = FALSE)+\n  geom_jitter(shape = 16, \n              alpha = 0.2,\n              position = position_jitter(0.2)) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(x = \"Group\", \n       y = \"Value\") +\n  theme_bw() +\n    theme(legend.position=\"none\")\n\n\n\n\n\n\n\n(a) Boxplot\n\n\n\n\n\n\n\n\n\n(b) Violin & Box & Jitter plot\n\n\n\n\n图 4.2: 箱图和小提琴图\n\n\n\n\n\n\n4.3.2 关系型图形\n  探索两个因素之间的关联性或是因果关系是社会科学研究中的关键问题，而这一类问题的分析起点就是通过图形探索两个因素之间是否存在相互依存关系，此时关系型图形的作用就非常大。常见的关系型图形包括散点图、线图以及热力图。\n\n4.3.2.1 散点图\n  散点图（Scatter Plot）最长应用的场景就是描述两个连续型变量之间的关系，当然有时也可用于连续型变量与分类变量之间，示例结果如 图 4.3 。\n\ndata(mtcars)\n#---#\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point() +\n  labs(x = \"Horsepower\",\n       y = \"Miles/(US) gallon\") +\n  theme_bw()\n\n#---#\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(x = \"Horsepower\",\n       y = \"Miles/(US) gallon\") +\n  theme_bw()\n\n`geom_smooth()` using formula 'y ~ x'\n\n#---#\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(shape = 15) +\n  labs(x = \"Horsepower\",\n       y = \"Miles/(US) gallon\") +\n  theme_bw()\n\n#---#\n# change shape, color, fill, size\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(shape=23, \n             fill = \"blue\", \n             color = \"darkred\", size=3) +\n  labs(x = \"Horsepower\",\n       y = \"Miles/(US) gallon\") +\n  theme_bw()\n\n\n\n\n\n\n\n(a) 基础散点图\n\n\n\n\n\n\n\n(b) 基础散点图加拟合线\n\n\n\n\n\n\n\n\n\n(c) 散点图改变Shape\n\n\n\n\n\n\n\n(d) 散点图改变Shape、Color等\n\n\n\n\n图 4.3: 散点图\n\n\n\n\n\n4.3.2.2 线图\n  线图（Line Plot）也是最常见的用于观察两个变量之间关系的统计图形之一，例如股市的K线图，用于观察股价与时间的关系。示例结果如 图 4.4 。\n\ndata(Orange)\n\nggplot(Orange) +\n    geom_line(aes(x = age, \n                  y = circumference, \n                  linetype = Tree)) +\n    theme_bw() +\n      theme(legend.position = \"bottom\")\n\n\n\n\n图 4.4: 线图\n\n\n\n\n\n\n4.3.2.3 热力图\n  热力图（Heatmap）是通过颜色的深浅来区分某个连续型变量在两个分类变量之间的集中和离散情况，在生物信息学中应用较为广泛。目前，ggplot2包支持简单的热力图可视化，若需求更高，如需要同时了解不同分类在连续型变量上的聚类情况，可以使用pheatmap包。\n\nlett <- LETTERS[1:20]\nweek <- paste0(\"Week\", seq(1,20))\ndata <- expand.grid(\"lett\" = lett, \"week\" = week)\ndata$value <- runif(400, 60, 100)\n \n# Heatmap \nggplot(data, aes(lett, week, fill = value)) + \n  geom_tile() +\n  scale_fill_distiller(palette = \"RdPu\")\n\n\n\n\n图 4.5: 热力图\n\n\n\n\n\n\n\n4.3.3 构成型图形\n  构成型图形相对比较简单，主要是饼图（Pie plot）。在ggplot2中，饼图的实现方法有两种思路，一是原始数据已经给出了某个分类的构成比，此时可以直接作图，二是需要经过计算之后方可获得构成比，如本书中的示例。一般而言，第二种情况更为常见。示例结果如 图 4.6 。\n\n\n\n\n\n\n好物推荐\n\n\n\n\n\n\n推荐一个支持自动填充符合不同学术期刊配色要求的包ggsci。\n其优点在于可以让你省去不少寻找RGB等配色的时间，其中预设了包括Lancet、JAMA、Nature等期刊常用的配色板。\n尽管未能覆盖全部的应用场景，但是适用面已经相对较广了。\n\n\n\n\n\nlibrary(dplyr)\n\ndata(mtcars)\n\ntable(mtcars$cyl) %>% \n  prop.table(.) %>%\n  as.data.frame(.) %>%\n  ggplot(aes(x = \"\", y = Freq *  100, fill = Var1)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(Freq * 100, \"%\")),\n            position = position_stack(vjust = 0.5)) +\n  labs(x = \"\", y = \"\") +\n  coord_polar(theta = \"y\") +\n  ggsci::scale_fill_jama(name = \"Cyl\") +\n  theme_bw() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n图 4.6: 饼图\n\n\n\n\n\n\n4.3.4 对比型图形\n  对比型图形主要是条形图（Bar Plot），但是实际研究过程中线图和散点图等均可以实现对比的分析需求，这里所讲的对比型图形主要是指在进行数据对比时更为常用和直观的图形。条形图也称作柱状图，其变化形式较多样，根据柱子的不同摆放方式，可以分为堆积条图（Stack），并排条图（Dodge），百分条图（Fill）。\n  三种不同类型的偏好场景略有不同，堆积条图和百分条图常用于展示计数资料，而并排条图既常用于计数变量也常用于计量变量。在ggplot2中，可以通过geom_bar()或geom_col()函数实现。条形图的变化非常多，本书中仅对常见的情况进行演示，其他的希望读者自行探索。\n\nggplot(mtcars, aes(x = factor(cyl), y = ..count..)) +\n  geom_bar(stat = \"count\", fill = \"blue\") +\n  labs(x = \"Cyl\", y = \"Count\") +\n  theme_bw()\n\n#---#\nmtcars %>% group_by(cyl, gear) %>%\n  summarise(mpg_mean = mean(mpg)) %>%\n  ggplot(aes(x = factor(cyl), \n             y = mpg_mean, \n             fill = factor(gear))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = mpg_mean), \n            vjust = -0.8,\n            size = 3,\n            position = position_dodge(0.9)) +\n  labs(x = \"Cyl\", y = \"Mpg\") +\n  ggsci::scale_fill_aaas(name = \"Gear\") +\n  theme_bw()  +\n    theme(legend.position = \"bottom\")\n\n#---#\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(aes(fill = factor(gear)), position = \"stack\") +\n  labs(x = \"Cyl\", y = \"Count\") +\n  ggsci::scale_fill_npg(name = \"Gear\") +\n  theme_bw() +\n    theme(legend.position = \"bottom\")\n\n#---#\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(aes(fill = factor(gear)), position = \"fill\") +\n  labs(x = \"Cyl\", y = \"Percentage\") +\n  ggsci::scale_fill_lancet(name = \"Gear\") +\n  theme_bw() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n(a) 基础条图\n\n\n\n\n\n\n\n(b) 并排条图\n\n\n\n\n\n\n\n\n\n(c) 堆积条图\n\n\n\n\n\n\n\n(d) 百分条图\n\n\n\n\n图 4.7: 条形图\n\n\n\n\n\n4.3.5 分面\n  一般而言，二维平面图形中最多能展示三个属性，假设有4个属性（Age、BMI、Sex、Region）想要一同展示在同一个图形中，常规方法是无法完成的，如横轴为Age，纵轴为BMI，那么只能利用颜色或者形状来区别Sex或者Region。当需要在一个图形中展现四种属性时，就需要利用到分面（Facet）功能，在ggplot2包中，可以利用 facet_wrap()和facet_grid()函数实现，后者多用于多个分面变量，使结果呈现为行列交叉的形式，结果如 图 4.8 。\n\nggplot(mtcars) +\n  geom_point(aes(x = hp, y = mpg, \n                 shape = factor(vs))) +\n  facet_wrap(~ cyl) +\n  labs(x = \"Horsepower\",\n       y = \"Miles/(US) gallon\") +\n  scale_shape_discrete(name = \"Engine\",\n                       breaks = c(0, 1),\n                       labels = c(\"V-shaped\", \n                                  \"Straight\")) +\n  theme_bw() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n图 4.8: 分面图\n\n\n\n\n\n\n4.3.6 空间可视化\n  关于空间可视化的内容十分复杂，其本来也是空间数据分析或者空间统计学的主要内容，想要介绍清楚需要很多的篇幅和精力。在本书中，将仅仅介绍在数据描述过程中最常用的空间填色图。\n\n4.3.6.1 地图素材类型\n  空间可视化的基础素材就是想要研究的地区的地图图层，一般有三种常有类型：shp素材，geojson素材，地图包内置地图素材。\n  shp和geojson格式素材均可转换为以下两种地图数据格式：\n\nsp:SpatialPolygonDataFrame\nsf:Simple feature list column\n\n  这两种格式的数据集所描述的信息差不多是一致的。第一种格式（sp）是R语言绘图比较传统的数据格式，它将地理信息数据分割为两大块：描述层和映射层。在数据存放时，描述层记录各个地理区域的名称、ID、编号、简写、iOS编码，以及其他标识信息和度量变量，描述层是一个dataframe，我们可以用data@data来提取描述层的数据框。而对应的几何映射层，是每一个行政区域的多边形边界点，这些边界点按照order排序，按照group分组。多边形边界点信息是一个多层嵌套的list结构，但是我们仍然可以通过fortity()函数将其转化为数据框。即sp空间数据对象是一个dataframe（描述层）和polygons（几何映射层）两个对象的组合对象。\n  而sf对象将这种控件数据格式件进行了更加整齐的布局，使用st_read()导入的空间数据对象完全是一个整齐的数据框，拥有整齐的行列，这些行列中包含着数据描述和几何多边形的边界点信息。其中最大的特点是，它将每一个行政区划所对应的几何边界点封装成了一个list对象的记录，这条记录就像其他普通的文本记录、数值记录一样，被排列在对应行政区划描述的单元格中。\n\n\n4.3.6.2 地图来源\n  世界地图基本在很多支持空间分析的软件包中内置，虽然方便，但是这些地图基本会存在更新不及时导致的过于老旧问题。获得正确的地图素材是进行空间研究的基本前提。\n\n\n\n\n\n\n关于中国地图需要注意的\n\n\n\n\n\n\n在这里重点要指出的是在适用中国地图时，不论是商业出版还是发表学术成果，必须适用具有官方审图号的地图素材，否在会带来潜在的不必要麻烦。\n目前建议使用民政部网站提供的地图（http://xzqh.mca.gov.cn/map），其审图号为GS（2022）1873号。\n该地图中，县级地图数据不包括香港和澳门特别行政区，市级地图数据不包括台湾省。\n\n\n\n\n\n\n4.3.6.3 地图数据的读入方式\n  在R中目前有三种读取地图的方法，但是第一种方法目前不太推荐。\n\nsp::readShapePoly()\nrgdal::readOGR()\nsf::st_read()\n\n\n\n4.3.6.4 R中空间可视化常见三种方式\n  第一种，geom_polygon()函数。此方法必须通过@data及fortity()函数将描述层与几何层分别读取后，添加需要映射的变量，然后合并后进行画图。\n\nggplot(data = map_data) +\n    geom_polygon(aes(x=long, y=lat, \n               group=group, f\n               ill = fill_var))\n\n  第二种，geom_map()函数。fill_file为包含需要作图变量的数据框，map_file 为地图素材，即通过直接读取的地图文件，使用geom_map()函数不需要进行合并，因为合并的过程由map_id所指定的参数自动进行合并，merge_id为fill_file中的识别变量。示例如 图 4.9 。\n\nids <- factor(c(\"1.1\", \"2.1\", \"1.2\", \n                  \"2.2\", \"1.3\", \"2.3\"))\n    \nvalues <- data.frame(\n      ids = ids,\n      value = c(3, 3.1, 3.1, \n              3.2, 3.15, 3.5)\n    )\n    \npositions <- data.frame(\n      id = rep(ids, each = 4),\n      x = c(2, 1, 1.1, 2.2, 1, 0, 0.3, 1.1, \n          2.2, 1.1, 1.2, 2.5, 1.1, 0.3,\n            0.5, 1.2, 2.5, 1.2, 1.3, 2.7, \n          1.2, 0.5, 0.6, 1.3),\n      y = c(-0.5, 0, 1, 0.5, 0, 0.5, 1.5, 1, \n          0.5, 1, 2.1, 1.7, 1, 1.5,\n            2.2, 2.1, 1.7, 2.1, 3.2, 2.8, \n          2.1, 2.2, 3.3, 3.2)\n    )\n    \nggplot(values, aes(fill = value)) + \n    geom_map(aes(map_id = ids, fill = value),\n           map = positions, color = \"red\") +\n    expand_limits(positions)\n\n\n\n\n图 4.9: 利用geom_map进行空间可视化\n\n\n\n\n\n\n\n\n\n\n注意\n\n\n\n\n\n\nmap_file中必须包含三个变量x或long、y或lat、region或id。\nmap_id指定的merge_id必须是能与region或id变量进行合并。\n\n\n\n\n  第三种，geom_sf()函数，此方法仍然需要一次合并过程。\n\nchina_map <- \n  sf::st_read(\"dataset/MCA_China_province.geojson\")\n\nReading layer `MCA_China_province' from data source \n  `/Users/Chi/Documents/Research/Writing Book/Handbook_of_Health_Management/dataset/MCA_China_province.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 156 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 73.67795 ymin: 3.984257 xmax: 135.2075 ymax: 53.64847\nGeodetic CRS:  WGS 84\n\nchina_line <- \n  sf::st_read(\"dataset/MCA_China_line.geojson\")\n\nReading layer `MCA_China_line' from data source \n  `/Users/Chi/Documents/Research/Writing Book/Handbook_of_Health_Management/dataset/MCA_China_line.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 104 features and 3 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 73.67795 ymin: 3.553559 xmax: 135.2094 ymax: 53.64847\nGeodetic CRS:  WGS 84\n\nchina_map$pop <- \n  runif(length(china_map$NAME), 100, 1000)\n\nggplot() +\n  geom_sf(aes(fill = pop), data = china_map) +\n  geom_sf(data = china_line) +\n  theme_bw() +\n    theme(legend.position = c(0.1, 0.2))\n\n\n\n\n图 4.10: 利用geom_sf进行空间可视化\n\n\n\n\n  关于投影坐标系。前两种方法使用coord_map()函数指定坐标投影系，第三种可以使用coord_sf()函数，其中有参数crs用于指定投影方式，需要使用proj4string格式的字符串。如墨卡托投影用crs = “+proj=merc”。某些地图投影其字符串中包含经纬度等参数，如crs = “+proj=laea +lat_0=35 +lon_0=-100”。其它地图投影见Projection methods。crs本身包含坐标数据的，就不能再增加xlim, ylim等参数。\n\n\n4.3.6.5 R中的tmap包\n  R中支持空间可视化的第三方包选项还是十分丰富的，比如上文演示的ggplot2包，还有latticeExtra包、tmap包、leaflet包等，这些包各有优劣，一般综合起来使用。这里再推荐tmap包，其具有几点优点：\n\n与ggplot包相比，在地理可视化方面其更加专业，可以提供比例尺、指北针等选项。\n与leaflet包相比，其不仅同样支持reveal.js动态交互地图，还能输出符合学术要求的图片格式。\n\n\ntm_shape(china_map) +\n  tm_polygons(col = \"pop\", \n              pal = c(\"white\", \"skyblue\")) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"), \n               width = 0.1) +\n  tm_compass(position = c(\"right\", \"bottom\"),\n             size = 1) +\n  tm_layout(legend.width = 0.1)\n\n\n\n\n图 4.11: “Tmap输出空间填色图”\n\n\n\n\n\n\n\n\n小结\n\n\n\n\n\n  以上示例展示的只是较为常见的统计图形可视化方法，所举示例也是相对比较简单，只是针对数据分析过程中较为常见的场景，并未能覆盖大部分乃至全部需求。数据可视化是一个充满了创新和创意的领域，读者可以根据自身需求和兴趣大胆尝试和探索。"
  },
  {
    "objectID": "chapter_5.html",
    "href": "chapter_5.html",
    "title": "5  因果推断方法",
    "section": "",
    "text": "先说为什么会想着写这本书：\n\n一是因为这么多年学习和研究的领域是卫生管理（Health Management）与卫生政策（Health Policy），在研究方法方面一直是受卫生统计学或者应用统计学的影响比较大，但是近些年随着因果推断革命（Causal Inference Revolution）对各个学科在研究方法上的强烈冲击，特别是社会科学，卫生政策领域的研究在方法学上也受到计量经济学非常大的影响，其中被国内外学者使用最广泛的应该就当属双重差分（Difference-in-Difference）了；\n二是因为我本人的学科背景并非为经济学，所以很多计量经济学的方法都是在当研究需要时一点一点自学的，尽管计量经济学与统计学存在很多的渊源，但毕竟理论体系不太一样，这就导致在碎片化自学过程中难免会犯一些错误和走一些弯路，再加上这些年记忆力不比从前，对”烂笔头”的依赖越来越高，因此就索性把这些年攒下来的一些笔记，加上最近更新的一些关于DID的知识点整理出来了；\n三是最近几年随着微信公众号和知乎等迅速成为各类信息传播的主要平台，国内科研工作者慢慢的很少”逛”论坛、发帖子、写博客了，有的转战公众号开设自己的账号，有的开始团队运营，这些平台给科学研究提供了不少便利性，扩大了知识获取面，降低了知识检索的时间成本，但是其弊端也毕竟明显，一方面是导致很多人对前沿科学咨讯的获取基本来源于公众号，很少阅读原始文献，另一方面是公众号或者知乎平台分享的信息和知识点没有经过同行评议，其实不乏有错误之处，若不加思索的参考或者照搬很容易导致研究方法误用，特别是对于尚处于学术生涯初期的研究生。\n\n时刻更新自己的知识体系是科研工作者日常生活中的一部分，保持对新知识的学习热情应该是一个合格的打工人的基本素养吧。DID早在20世纪80年代就被提出，其基本原理很简单，但是如果据此就认为采用DID识别框架的方法都很简单就错了，随着Joshua D. Angrist、Guido W. Imbens、Gary King、Susan Athey等经济学和统计学家对因果识别方法体系的贡献，时至今日，DID已经衍生出了多种多样的类型来使用不同的研究设计和数据结构，可以说已经是一大方法家族了吧。\n写这本书就是打算能给卫生管理和卫生政策方向的年轻硕士和博士生在研究中以参考，因此这本书注重实践性。在这本书里面打算既介绍DiD的原理也给出完整的分析方法和过程，由于目前我还写不清楚所以不会涉及详细的数理推导。\n本书的主要参考资料如下：\n\n\nAngrist, & JoshuaDavid. (2010). Mostly Harmless Econometrics: An Empiricist Companion. Princeton University Press.\n伍德里奇, & 费剑平校. (2010). 计量经济学导论: 第4版. 中国人民大学出版社.\nScott Cunningham. (2020). Causal Inference: The Mixtape. Yale University Press.\nhttps://lost-stats.github.io/Model_Estimation/Research_Design/event_study.html"
  },
  {
    "objectID": "chapter_5.html#工具变量",
    "href": "chapter_5.html#工具变量",
    "title": "5  因果推断方法",
    "section": "5.2 工具变量",
    "text": "5.2 工具变量"
  },
  {
    "objectID": "chapter_5.html#断点回归",
    "href": "chapter_5.html#断点回归",
    "title": "5  因果推断方法",
    "section": "5.3 断点回归",
    "text": "5.3 断点回归\n\n5.3.1 简介\n  断点回归（Regression Discontinuity）适用于以下情形：人群是否接受干预（Treatment）是依据某一数值变量（rating variable）是否高于或低于某一确定的阈值（threshold）或者分割点（cut-point），例如在研究是否上大学会影响收入时，数值变量（rating variable，也叫 assignment variable，score，running variable，forcing variable， or index）就是高考分数，阈值或者分割点就是本科录取分数线。以下内容主要参考：(Calonico et al. 2022; D. Cattaneo, Idroboy, and Titiunik 2017; Jacob, Zhu, and Marie-Andrée 2012; 谢谦, 薛仙玲, and 付明卫 2019)。\n\n\n5.3.2 发展历史\n  Regression Discontinuity最早由社会学家Thistlethwaite and Campbell 在1960年提出的，用于评估社会项目，但是他们的研究虽然引起一些影响但是没有得到广泛的注意，后来，在1972-2009年期间，被一系列经济学家(Goldberger，van der Klaauw，Imbens and Kalyanaraman等)在方法学方面进行了完善，最终在2008年达到顶峰，标志是在2008年Journal of Econometrics出了一期RD分析的Special Issue，光看期刊名字就知道是经济学顶刊了(Journal of Econometrics是公认的计量经济学顶尖期刊，是教育部认可的12本经济学国际顶级期刊之一 )。\n\n\n5.3.3 特点与分类\nRD有两个特点：\n\n在Rating variable的一个明确点上，outcome出现了跳跃或者不连续(discontinuity at a cut-point)\n可以认为在一个限定的rating variable区间上，个体是服从局部随机的(local randomization)\n\nRD有两种类型：\n\n精确断点(sharp design): 即所有个体(All subject)在明确的cut-point之后全部接受干预(treatment)\n模糊断点(fuzzy design): 即在cut-point前后，存在 no-shows (treatment group members who do not receive the treatment)或者crossovers (control group members who do receive the treatment) ，换句话说就无法找到一个明确的cut-point完全区分干预和对照组。更严格的分法是，fuzzy也可以分成两类：type I是no-shows，只存在处理组有未接受处理的个体，type II是同时存在no-shows和crossovers。实际就是RCT中常说的沾染问题。\n\n\n\n5.3.4 适用RD分析的先决条件(Conditions for Internal Validity)\n  由于RD仍然属于非实验方法，尽管也被成为类实验(quasi-experimental)，但本质还是非实验方法(nonexperimental)， 所以它必须满足一系列前期条件，才能提供无偏估计和更可能的接近RCT的严格情形。\n\n一、 Rating variable (中文的翻译很多，但比较通用的翻译为驱动变量或者配置变量)不能被干预(treatment)所影响，Rating variable 的值必须是在干预(treatment)之前就已经确定了，或者是不可再更改的变量。也就是只能是驱动变量的值决定是否接受干预，不能是干预决定驱动变量的值。比如：高考成绩(Rating variable)是在判断是否能够上大学(treatment)之前就已经确定了，并且分数不会再发生变化，如果存在严重的根据分数线修改高考成绩的情况，则高考分数就不能作为Rating variable。 检验方法是：画Rating variable的频率分布直方图或者密度图，再进行McCrary 检验\n二、 断点(cut-point)是完全外生的，并且干预与否完全取决于驱动变量和断点。即断点不受其他因素影响而改变干预的判断，即把本不该接受干预的对象划入了干预，或者把该接受干预的对象划入了对照。 比如，如果高考录取线(cut-point)的确定是完全已经严格的录取指标划定的， 而不存在为了使得某部分人群能够获得上大学的机会，而修改录取线。\n三、 在驱动变量的cut-point前后，除了干预与否(treatment)是不连续或者跳跃的(比如断点前为0，断点后为1)，其他任何因素都不能出现不连续或者跳跃(Nothing other than treatment status is discontinuous in the analysis interval)，因为要保证outcome的跳跃只能是干预的不连续或者跳跃导致的。比如，如果高考分数线除了决定是否能上大学外，还决定是否享有创业扶持机会，那么如果研究上大学对某种结局的影响，就无法分离出到底是大学教育导致，还是获得创业扶持机会导致的。\n四、 驱动变量(rating variable)与结局变量(outcome)之间的函数关系应该是连续的，严格的说，应该是在进行RD分析的这段区间内(interval)是连续的。换句话说就是，如果不存在干预的情况，驱动变量与结局变量之间不会出现不连续或者跳跃，才能在结局变量出现不连续的情况下，反推出是干预引起的，因为只有干预是不连续的。注意：此条件只需要在选择参数估计方法是要满足(applies only to parametric estimators)\n\n\n\n5.3.5 断点回归的图形分析(Graphical Presentations in the RD)\n图形分析是进行RD的第一步，也是非常重要的组成部分\n通常，进行RD至少需要画4种图：\n\n驱动变量与干预之间的关系图，在这一步可以确定是应该采取Sharp还是Fuzzy断点回归设计\n非结局变量与驱动变量的关系图，可以用来判断是否满足第3条内部有效性(Internal Validity)条件\n驱动变量的密度分布图，是为了判断驱动变量是否连续，以及在cut-point附近是否存在被操控，可以用来判读石佛满足第1条内部有效性(Internal Validity)条件\n结局变量与驱动变量的关系图，可以帮助预估干预效应的大小，以及判断结局变量与驱动变量之间的函数关系。而且必须是结局变量在纵轴，驱动变量在横轴。注意：通常这一幅图是用来初步判断整个研究是否能够获得预期的干预效应的，如果在这幅图中无法观测到明显的jump，基本后续的分析也是徒劳。\n\n\n\n5.3.6 进行RD分析的步骤及示例\n示列软件: R version 4.0.5 (2021-03-31)\n示列分析包：在R语言中一共有三个package可以进行RD分析：\n\nrdd: 由Dimmery在2016年发布，最后一次更新是2016-3-14，貌似目前没有获得积极的更新\nrddtools：由Stigler & Quast最早在2013年发布，最后一次更新是2015-07-27，貌似目前也没有获得积极的更新\nrdrobust：由Sebastian Calonico, Matias D. Cattaneo, Max H. Farrell等在2016年发布的，最后一次更新是2018-09-26，是目前功能最完善的RD分析包，同时也开发了Stata版本。该包的作者同时也是RD方法学上的大佬，多种bandwidth选择方法的发明者，著名的CCT就是。\n\n\nrdrobust简介：目前该包的开发项目可在https://sites.google.com/site/rdpackages/home主页上查看，该包共有三个函数： + rdplot：用来画图 + rdrobust：用来进行局部非参数估计 + rdbwselect：用来进行bandwidth选择 另外还有一个协同的包rddensity，需要从CRAN上单独安装，用来进行对驱动变量是否被操控进行检验，因为rdrobust包未包含McCrary检验\n\n示列数据集：采用rdrobust包中自带的rdrobust_RDsenate数据集：\n\n数据集简介：该数据集包含了1914–2010年间美国参议院选举的数据，可以利用RD的方法分析民主党赢得参议院席位对于在下次选举中获得的相同席位的影响。该数据共包含两个变量：\n> + vote：记录了州级层面下的参议院议席中民主党所占的比例，值从0到100，是RD分析中结局变量 > + margin：记录了民主党在上次选举中获得相同参议院席位的胜利边际，值从-100到100，当大于0时，说明民主党胜利，小于0则输了，是RD分析中的驱动变量\n\n\n5.3.6.1 Step 1: 确定驱动变量(Rating variable)和断点(Cut-point)\n根据研究设计，判断是否存在采用RD进行因果识别或者效应估计的可能，即是否可以找到合适的驱动变量和明确的断点来识别施加干预的状态。\n通常情况下时间（具体到天）、年龄（如研究退休，代表性的文章: “退休影响会健康吗”）、地理距离（研究雾霾，代表性文章: “冬季供暖导致雾霾？来自华北 城市面板的证据”）比较容易作为驱动变量。\n\n\n5.3.6.2 Step 2: 内部有效性(Internal Validity)条件的检验\n1) 条件一：通过画驱动变量的频率分布直方图或者密度分布图，以及McCrary检验\n可以利用graphics包或者ggplot2包画histogram图，也可以利用rddenstiy包中的rddenstiy函数画密度图，也可利用rdd包进行McCrary检验。\nrddenstiy函数的其他参数可以用默认值，输出结果中，最下面一行robust的p值大于0.05，就说明驱动变量未受到操纵。\n\n方法一\n\n\nlibrary(rdrobust)\nlibrary(rddensity)\nlibrary(magrittr)\ndata(rdrobust_RDsenate)\n\nhist(x = rdrobust_RDsenate$margin, \n     breaks = 30, \n     col = 'red',\n     xlab = 'margin', \n     ylab = 'Frequance', \n     main = 'Histogram of Rating Variable')\nabline(v = 0)\n\n\n\n\n图 5.1: Histogram of Rating Variable (1)\n\n\n\n\n\n方法二\n\n\nlibrary(ggplot2)\nggplot(rdrobust_RDsenate) +\n    geom_histogram(aes(x = margin, \n                       fill = margin < 0), \n                       color = 'black', \n                       binwidth = 4, boundary = 4) +\n    geom_vline(xintercept = 0, size = 1) +\n    labs(title = 'Histogram of the Rating varibale',\n         y = 'Number of Observation', \n         x = 'Margin') + \n    scale_fill_manual(name = '', \n                      values = c('red', 'blue'),\n                      labels = c('Control', 'Treat')) +\n    theme_bw() +\n        theme(legend.position = c(.9, .9),\n              legend.background = element_blank())\n\n\n\n\n图 5.2: Histogram of Rating Variable (2)\n\n\n\n\n\n方法三\n\n\nrdplotdensity(rddensity(rdrobust_RDsenate$margin),\n                 X = rdrobust_RDsenate$margin)$Estplot[1]\n\n$data\nlist()\nattr(,\"class\")\n[1] \"waiver\"\n\n\n\n\n\n图 5.3: Histogram of Rating Variable (3)\n\n\n\n\n2) 条件二三四根据研究设计及背景资料进行判断。\n\n\n5.3.6.3 Step 3: 画驱动变量与干预的关系图，判断sharp或者fuzzy类型\n  画驱动变量与干预的散点图，判断是否为sharp或者fuzzy类型。可以看出本例中，干预在cut-point前后，有明确的界限，为sharp类型。\n\nrdrobust_RDsenate$treatment <- \n  ifelse(rdrobust_RDsenate$margin < 0, 0, 1)\n\nrdrobust_RDsenate$color <- \n  ifelse(rdrobust_RDsenate$margin < 0, \n                                  'blue', \n                                  'red')\n\nplot(x = rdrobust_RDsenate$margin, \n     y = rdrobust_RDsenate$treatment, \n     col = rdrobust_RDsenate$color,\n       type = 'p', \n     pch = 16, \n     xlab = 'Margin', \n     ylab = 'Treatment',\n     main = 'Relationship between \n             rating variable and treatment')\nabline(v = 0)\n\n\n\n\n图 5.4: Relationship between rating variable and treatment\n\n\n\n\n\n\n5.3.6.4 Step 4: 画驱动变量与结果变量的关系图，选择合适的bin数量\n一、画散点图\n  横轴为驱动变量，纵轴为结果变量，画散点图，初步判断两者之间关系，但是散点图噪音(noise)太大，不利于发现跳跃点，需要对驱动变量分不同的段(intervals or bin)将使得关系取线更平顺。\n\n# 第一步：散点图\n\nplot(x = rdrobust_RDsenate$margin, \n     y = rdrobust_RDsenate$vote, \n     type = 'p',\n       col = rdrobust_RDsenate$color, \n     pch = 16, \n     cex = 0.8, \n     xlab = 'Margin', \n     ylab = 'Vote')\nabline(v = 0)\n\n\n\n\n图 5.5: 散点图\n\n\n\n\n二、将驱动变量分段的步骤\n  将驱动变量分段后再做关系图，大致有四步：\n\n将驱动变量分成若干宽度(width)相等的区间(bin)，但是需要注意的是不能存在骑跨cut-point的区间， 所以最好从cut-point开始分别向左右两端进行划分，并不一定要求左右两边的bin数量相等。\n\n计算每个bin中的结局变量的平均值、驱动变量的中位数，以及个体的数量(num of observation)\n将驱动变量的中位数作为横轴，将结局变量的平均值作为纵轴，同时用每个bin中个体的数量进行加权\n为了更好的显示两变量之间的关系，最好加上局部加权平滑回归取线(如：lowess: locally weighted regression)\n\n三、确定bins数量的方法\n  如何设定bin的数量或者binwidth的方法比较复杂，bin的数量不宜过多或过少，过少起不到将噪音的作用，过多会导致jump不明显，在Stata和R中有打包的好的方法去判断bin的数量，本例中采用rdrobust包中的rdplot函数进行判断。\nrdplot函数的常用参数有：\n\nc为cut-point，默认为0\np为全局多项式的幂次方，默认为4\nnbins用来设定断点左右的bin的数量，默认为NULL\nkernel用来设定加权方法，默认为uniform，同时还有triangular和epanechnikov方法\nbinselect为判断方法，默认为’esmv’，\n\n判断方法常用一共4种：Evenly-spaced (es)，Quantile-spaced (qs)，以及结合mimicking-variance (MV)的esmv和qsmv，推荐使用qs方法。p和nbin如果不确定也可缺失，让binselect方法自行设定，从结果可以看出总的样本量，以及cut-point左右的样本量，以及设定的bin的数量。\n\nbins <- rdplot(rdrobust_RDsenate$vote, \n               rdrobust_RDsenate$margin, \n               c = 0, p = 4,\n               nbins = c(20, 20), \n               binselect = 'esmv', \n               kernel = 'uniform')\n\nsummary(bins)\n\nCall: rdplot\n\nNumber of Obs.                 1297\nKernel                      Uniform\n\nNumber of Obs.                  595             702\nEff. Number of Obs.             595             702\nOrder poly. fit (p)               4               4\nBW poly. fit (h)            100.000         100.000\nNumber of bins scale              1               1\n\nBins Selected                    20              20\nAverage Bin Length            5.000           5.000\nMedian Bin Length             5.000           5.000\n\nIMSE-optimal bins                 8               9\nMimicking Variance bins          15              35\n\nRelative to IMSE-optimal:\nImplied scale                 2.500           2.222\nWIMSE variance weight         0.060           0.084\nWIMSE bias weight             0.940           0.916\n\n\n\n\n\n图 5.6: 确定bins数量的方法\n\n\n\n\n\n\n5.3.6.5 Step 5: 模型估计效应大小\n1) 效应大小的估计方法一共有两种：\n\n全局参数估计(Parametric/global strategy): 即是利用全部个体的数据，对驱动变量和结局变量拟合线性、二次项、三次项、四次项回归模型，此方法利用的是RD设计的”断点处不连续(discontinuity at a cut-point)“的特性。模型设定如下：\n\n\\[\n\\begin{aligned}\nY_i = & \\alpha + \\beta_0 \\cdot Treatment_i + \\beta_1 \\cdot (rating_i - cutpoint)^k + \\\\\n      & \\beta_3 \\cdot Treatment_i \\cdot (rating_i - cutpoint)^k + \\\\\n      & \\beta_4 \\cdot Confouders_i + \\epsilon_i\n\\end{aligned}\n\\]\n\n式中：\n\nY为结局变量，\\(\\alpha\\) 为常数项，表示在控制驱动变量后干预组结局变量的平均值，k为多项式的幂次，通常为1到4次方，其他参数含义见字面意思，\n\\(\\beta_0\\)就是核心关注的系数，表示干预措施在断点处的边际效应，也就是干预的处理效应。\n\n而如何选择合适的次项数，需要多次尝试，通过比较AIC，选择AIC最小的模型。\n驱动变量减去cutpoint的目的是为了中心化，为了使\\(\\alpha\\)反映的是断点处的平均值，因为差值在断点处为0\nConfouders为混杂因素，也可以加入模型中，但是中RD分析时，混杂因素的控制不是必须的。\n\n局部非参数估计(Nonparametric/local strategy): 即是在cut-point左右选择一段合适的带宽(bandwidth)，在这一段带宽局部范围内， 拟合驱动变量和结局变量的线性或者多项式回归模型，此方法利用的是RD设计的”局部随机(local randomization)“的特性。此方法适用于样本量较大的情况， 因为样本量太少，设定带宽之后样本量将会不足，增加估计误差。前面所提到的R包中均只提供局部非参数估计函数。局部非参数估计的难点在于如何确定最优带宽，常用的方法有交叉验证法(cross validation procedure， CV)、 IK 法和 CCT 法，后两种方法较为推荐\n\n  关于上述两种方法的比较或者权衡，主要是关于精度和误差，全局参数估计因为利用了全部样本，通常对效应的估计精度更高，然而其也存在一个问题，就是驱动变量和结局变量的关系函数在越大的区间下越难以准确识别；而非参数估计可以通过局部拟合加权线性或者多项式回归模型而减少这种偏误。\n  以上方法主要针对sharp类型，fuzzy类型的估计方法见最后一节。\n2) 全局参数估计\n  分为一次线性、一次线性加交互项、以及二次、三次线性及交互项，共6个模型，比较AIC或者回归残差，较小者模型较优。\n\nrdrobust_RDsenate$margin_del <- rdrobust_RDsenate$margin - 0\n\n# linear\nfit_1 <- lm(vote ~ margin_del + treatment, \n            data = rdrobust_RDsenate) \n\n# linear interaction\nfit_2 <- lm(vote ~ margin_del * treatment, \n            data = rdrobust_RDsenate) \n\n# quadratic\nfit_3 <- lm(vote ~ margin_del + \n                   I(margin_del ^ 2) + treatment, \n            data = rdrobust_RDsenate) \n\n# quadratic interaction\nfit_4 <- lm(vote ~ (margin_del + I(margin_del ^ 2)) * \n                   treatment, \n            data = rdrobust_RDsenate) \n\n# cubic\nfit_5 <- lm(vote ~ margin_del + I(margin_del ^ 2) + \n                   I(margin_del ^ 3) + treatment, \n            data = rdrobust_RDsenate) \n\n# cubic interaction\nfit_6 <- lm(vote ~ (margin_del + I(margin_del ^ 2) + \n                    I(margin_del ^ 3)) * treatment, \n            data = rdrobust_RDsenate) \n\n\nlibrary(modelsummary)\n\nmodelsummary(list(\"Linear\" = fit_1, \n                  \"Linear interaction\" = fit_2, \n                  \"Quadratic\" = fit_3), \n              output = \"kableExtra\",\n              stars = TRUE)\n\n\n\n表 5.1:  全局参数估计 \n \n  \n      \n    Linear \n    Linear interaction \n    Quadratic \n  \n \n\n  \n    (Intercept) \n    47.331*** \n    44.904*** \n    45.486*** \n  \n  \n     \n    (0.542) \n    (0.699) \n    (0.616) \n  \n  \n    margin_del \n    0.348*** \n    0.216*** \n    0.285*** \n  \n  \n     \n    (0.013) \n    (0.028) \n    (0.017) \n  \n  \n    treatment \n    4.785*** \n    6.044*** \n    6.663*** \n  \n  \n     \n    (0.923) \n    (0.942) \n    (0.963) \n  \n  \n    margin_del × treatment \n     \n    0.170*** \n     \n  \n  \n     \n     \n    (0.032) \n     \n  \n  \n    I(margin_del^2) \n     \n     \n    0.001*** \n  \n  \n     \n     \n     \n    (0.0002) \n  \n  \n    Num.Obs. \n    1297 \n    1297 \n    1297 \n  \n  \n    R2 \n    0.578 \n    0.587 \n    0.590 \n  \n  \n    R2 Adj. \n    0.577 \n    0.586 \n    0.589 \n  \n  \n    AIC \n    10083.7 \n    10056.7 \n    10049.9 \n  \n  \n    BIC \n    10104.4 \n    10082.5 \n    10075.7 \n  \n  \n    Log.Lik. \n    −5037.848 \n    −5023.354 \n    −5019.946 \n  \n  \n    F \n    886.433 \n    613.586 \n    619.091 \n  \n  \n    RMSE \n    11.77 \n    11.64 \n    11.61 \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n3) 局部非参数估计\n  利用rdrobust包的rdrobust函数进行拟合。由于该包是由Sebastian Calonico, Matias D. Cattaneo and Rocío Titiunik开发，所以该包中所提供的带宽选择方法可认为就是CCT法，因为CCT本来就是Calonico, Cattaneo, and Titiunik三人姓氏的缩写。\n有两种确定带宽的方法：\n\n一是先利用rdbwselect函数计算出最优带宽，然后用rdrobust函数中的h参数手动指定左右的带宽。\n\n二是直接利用rdrobust函数中的bdselect参数指定选择最优带宽的方法。\n\nrdrobust包中一共提供了10种选择最优带宽的方法，如下:\n\n其中，共包含MSE = Mean Square Error和CER = Coverage Error Rate两大类，MSE更适用于进行点估计的带宽选择， 而CER更适合区间估计的带宽选择。\n以rd结尾是表示选择的带宽在cut-point左右相等，而以two结尾是表示选择的带宽在cut-point左右不相等。\nh即为选择的左右最优带宽，而b给出的是用来进行敏感性分析时应该考虑的带宽。\n\n\nrdbwselect(y = rdrobust_RDsenate$vote, \n           x = rdrobust_RDsenate$margin, \n           all = TRUE) %>% summary()\n\nCall: rdbwselect\n\nNumber of Obs.                 1297\nBW type                         All\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  595          702\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nUnique Obs.                     595          665\n\n=======================================================\n                  BW est. (h)    BW bias (b)\n            Left of c Right of c  Left of c Right of c\n=======================================================\n     mserd    17.754     17.754     28.028     28.028\n    msetwo    16.170     18.126     27.104     29.344\n    msesum    18.365     18.365     31.319     31.319\n  msecomb1    17.754     17.754     28.028     28.028\n  msecomb2    17.754     18.126     28.028     29.344\n     cerrd    12.407     12.407     28.028     28.028\n    certwo    11.299     12.667     27.104     29.344\n    cersum    12.834     12.834     31.319     31.319\n  cercomb1    12.407     12.407     28.028     28.028\n  cercomb2    12.407     12.667     28.028     29.344\n=======================================================\n\n\n  虽然在此文中rdrobust: An R Package for Robust Nonparametric Inference in Regression-Discontinuity Designs提到也可以通过bdselect = ’CV’或者bdselect = ’IK’来采用CV和IK法，但是在写这篇笔记时，已无法使用这两种方法。\n输出的结果的最后一张表的第一行Conventional即为干预的处理效应。\n\n\n\n\n# 如果有混杂因素需要控制,可以用covs = c('var1', 'var2')\nloc_fit_1 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin, \n                      c = 0, p = 1,\n                      kernel = 'triangular', \n                      bwselect = 'msetwo') \n\n# c用来指定cut-point，p用来指定局部加权回归的多项式幂次\nloc_fit_2 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin, \n                      c = 0, p = 2,\n                      kernel = 'triangular', \n                      bwselect = 'msetwo') \n\nloc_fit_3 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin, \n                      c = 0, p = 1,\n                      kernel = 'triangular', \n                      bwselect = 'cerrd')\n\nloc_fit_4 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin, \n                      c = 0, p = 2,\n                      kernel = 'triangular', \n                      bwselect = 'certwo')\n\n\nmodelsummary(list(loc_fit_1, loc_fit_2,\n                  loc_fit_3, loc_fit_4),\n             output = \"kableExtra\",\n             stars = TRUE,\n             estimate = \"std.error\")\n\n\n\n表 5.2:  局部非参数估计 \n \n  \n      \n    Model 1 \n    Model 2 \n    Model 3 \n    Model 4 \n  \n \n\n  \n    Conventional \n    1.497*** \n    1.852*** \n    1.680*** \n    2.209*** \n  \n  \n     \n    (1.497) \n    (1.852) \n    (1.680) \n    (2.209) \n  \n  \n    Bias-Corrected \n    1.497*** \n    1.852*** \n    1.680*** \n    2.209*** \n  \n  \n     \n    (1.497) \n    (1.852) \n    (1.680) \n    (2.209) \n  \n  \n    Robust \n    1.759*** \n    2.056*** \n    1.841*** \n    2.282*** \n  \n  \n     \n    (1.759) \n    (2.056) \n    (1.841) \n    (2.282) \n  \n  \n    Kernel \n    Triangular \n    Triangular \n    Triangular \n    Triangular \n  \n  \n    Bandwidth \n    msetwo \n    msetwo \n    cerrd \n    certwo \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n\n\n5.3.6.6 Step 6: 敏感性分析\n  敏感性分析主要用来检验模型估计结果的稳健性，RD分析主要有四种敏感性分析方式;\n\n对前面所述的Internal Validity条件三的检验，即除了干预(Treatment)变量外，其他的任何非结局变量(包括混杂因素在内)都不能在cut-point处出现不连续或称为跳跃，如果存在，则无法推断结局变量的跳跃是由干预(Treatment)引起的，检验方式就是将所有的混杂因素作为结局，利用rdrobust函数进行检验，输出结果的coef应该很小，且p值应该统计学不显著 (Continuity-Based Analysis for Covariates)。\n对cut-point的敏感性分析，即是更换cut-point，检验是否左右还存在处理效应，如果更换断点后，仍然存在处理效应，则无法说明本研究的干预措施是有效的，因为在研究设定的断点处识别到的处理效应，有可能是由其他因素引起的。\n对cut-point附近个体的敏感性分析，在Internal Validity条件一中提到，驱动变量不可被操控，但是这个无法直接检验，因此，换个思路，如果驱动变量被超控，那自然是在cut-point左右离得最近的值被操纵的可能性最大，所以如果将这部分个体剔除，若仍然能观测到处理效应，则说明这种效应是真实由干预所导致。这种方法被称为甜甜圈(donut hole)法，同样也适用于个体过多的堆积与cut-point附近的RD分析。\n对带宽bandwidth的敏感性分析，即是cut-point不改变，而是更换选择的最优带宽，进行多次局部非参数检验，如果更换带宽之后，仍然能在断点处识别的处理效应，说明研究的干预措施是有效的，因为在研究设定的断点处识别到的处理效应不是由于某一特点的带宽下才观测到的，说明处理效应稳健。\n\n1) 对cut-point的敏感性分析\n  选择一个虚拟的断点用来替换真正的断点，但是真实的干预情况不改变。关于虚拟断点如何选择以及选择多少个并没有明确的标准，通常在真实断点的附近，左右对称选取即可\n\n# 除c以外的其他参数应该与前面分析保持一致\nsen_cut_1 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin,\n                      c = 1, p = 1,\n                      kernel = 'triangular', \n                      bwselect = 'msetwo') \n\n# 除c以外的其他参数应该与前面分析保持一致\nsen_cut_2 <- rdrobust(rdrobust_RDsenate$vote, \n                      rdrobust_RDsenate$margin,\n                      c = -1, p = 1,\n                      kernel = 'triangular', \n                      bwselect = 'msetwo') \n\n\nmodelsummary(list(sen_cut_1, sen_cut_2), \n             output = \"kableExtra\",\n             stars = TRUE,\n             estimate = \"std.error\")\n\n\n\n表 5.3:  对cut-point的敏感性分析 \n \n  \n      \n    Model 1 \n    Model 2 \n  \n \n\n  \n    Conventional \n    1.406*** \n    1.614 \n  \n  \n     \n    (1.406) \n    (1.614) \n  \n  \n    Bias-Corrected \n    1.406*** \n    1.614 \n  \n  \n     \n    (1.406) \n    (1.614) \n  \n  \n    Robust \n    1.640** \n    1.865 \n  \n  \n     \n    (1.640) \n    (1.865) \n  \n  \n    Kernel \n    Triangular \n    Triangular \n  \n  \n    Bandwidth \n    msetwo \n    msetwo \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n2) 对cut-point附近个体的敏感性分析\n  对称剔除真实断点左右一定较小范围内的个体。关于范围的大小同样没有明确的标准，通常选取多个范围比较即可。\n\nrdrobust_RD_hole_1 <- subset(rdrobust_RDsenate, \n                             abs(margin) > 0.3)\nrdrobust_RD_hole_2 <- subset(rdrobust_RDsenate, \n                             abs(margin) > 0.4)\nrdrobust_RD_hole_3 <- subset(rdrobust_RDsenate, \n                             abs(margin) > 0.5)\n\n# 除c以外的其他参数应该与前面分析保持一致\nsen_hole_1 <- rdrobust(rdrobust_RD_hole_1$vote, \n                       rdrobust_RD_hole_1$margin, \n                       c = 0, p = 1,\n                       kernel = 'triangular', \n                       bwselect = 'msetwo') \n\n# 除c以外的其他参数应该与前面分析保持一致\nsen_hole_2 <- rdrobust(rdrobust_RD_hole_2$vote, \n                       rdrobust_RD_hole_2$margin, \n                       c = 0, p = 1,\n                       kernel = 'triangular', \n                       bwselect = 'msetwo') \n\n# 除c以外的其他参数应该与前面分析保持一致\nsen_hole_3 <- rdrobust(rdrobust_RD_hole_3$vote, \n                       rdrobust_RD_hole_3$margin, \n                       c = 0, p = 1,\n                       kernel = 'triangular', \n                       bwselect = 'msetwo') \n\n\nmodelsummary(list(sen_hole_1, sen_hole_2, sen_hole_3), \n             output = \"kableExtra\",\n             stars = TRUE,\n             estimate = \"std.error\")\n\n\n\n表 5.4:  对cut-point附近个体的敏感性分析 \n \n  \n      \n    Model 1 \n    Model 2 \n    Model 3 \n  \n \n\n  \n    Conventional \n    1.558*** \n    1.558*** \n    1.591*** \n  \n  \n     \n    (1.558) \n    (1.558) \n    (1.591) \n  \n  \n    Bias-Corrected \n    1.558*** \n    1.558*** \n    1.591*** \n  \n  \n     \n    (1.558) \n    (1.558) \n    (1.591) \n  \n  \n    Robust \n    1.839*** \n    1.860*** \n    1.918*** \n  \n  \n     \n    (1.839) \n    (1.860) \n    (1.918) \n  \n  \n    Kernel \n    Triangular \n    Triangular \n    Triangular \n  \n  \n    Bandwidth \n    msetwo \n    msetwo \n    msetwo \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n3) 对带宽bandwidth的敏感性分析\n  即是更换不同的带宽即可，带宽的变化实际是cut-point区间范围尾部的个体在发生变化，可以通过选取不同的带宽选择方式，真实存在的处理效应不会随着带宽的改变而发生变化。\n\n\n\n5.3.7 关于Fuzzy类型的RD模型估计方法\n  Fuzzy类型的RD可以根据驱动变量与干预的实际情况采取不同的处理方式:\n\n如果只是no-shows的情况，可以结合研究目的，决定是否采取意向性分析法(intent-to-treat)\n采用两阶段回归two-stage least squares (2SLS) ，两阶段模型设定如下:\n\n第一阶段：\n\\[ Treatment_i = \\alpha_1 + \\gamma_0 \\cdot treat-by-cutpoint_i + \\epsilon_i \\]\n第二阶段：\n\\[ Y_i = \\alpha_i + \\beta_0 \\cdot \\dot Treatment + \\mu_i \\]\n\n其中：\n\n\\(Treatment_i\\)为真实的干预情况\n\\(treat-by-cutpoint_i\\)是指驱动变量根据cut-point判断的是否接受了干预，为虚拟变量\n第二阶段模型中的\\(\\dot Treatment\\)将第一阶段模型的预测指yhat\n\n\n\n\n通过两阶段回归，第二阶段中的Standard errors是经过了调整的。在rdrobust包的rdrobust函数中，有一个fuzzy的逻辑参数，用来指定RD类型是否为fuzzy，可以很方便的进行fuzzy类型的模型估计。\n\n\n5.3.8 结束\n  由于真实世界的复杂性，对因果推断的分析方法的要求也越来越高，没有一种方法可以适用所有的情形，因此RD的方法也衍生发展出很多类型，包括多个断点(Multi-Cutoff RD Design)、多个驱动变量(Multi-Score RD Design)、离散驱动变量(Multi-Score RD Design)等等。\n\n\n\n\nCalonico, Sebastian, Matias D. Cattaneo, Max H. Farrell, and Rocio Titiunik. 2022. Rdrobust: Robust Data-Driven Statistical Inference in Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nD. Cattaneo, Matias, Nicolas Idroboy, and Roc Titiunik. 2017. A Practical Introduction to Regression Discontinuity Designs: Volume i. collingwoodresearch.com.\n\n\nJacob, Robin, Pei Zhu, and Marie-Andrée. 2012. A Practical Guide to Regression Discontinuity. MDRC.org.\n\n\n谢谦, 薛仙玲, and 付明卫. 2019. “断点回归设计方法应用的研究综述.” 经济与管理评论 35 (2): 11."
  },
  {
    "objectID": "chapter_6.html",
    "href": "chapter_6.html",
    "title": "6  番外篇（一）：随机化分组",
    "section": "",
    "text": "实验研究作为因果推断效能最高的一种研究设计，被广泛应用于医学及社会科学研究中。在实验研究中又以随机对照试验（Randomised Controlled Trials，RCT）最为常见。RCT研究一个最明显的特点就是随机化（Randomization）施加干预(李立明 2007)，随机化和盲法是RCT研究的基石(Kendall 2003)。随机化干预的重要性在于可以有效避免选择或其他偏倚，提供统一的统计分析方法，以及为精确检验显著性和区间估计提供基础(Roberts and Torgerson 1998; Cox 2009)。\n  在具体的研究中，随机化干预的基础是对干预对象进行随机分组，然而，在现有针对本科生和研究生阶段学习的统计教材中，却缺少对随机分组方法以及其具体实现方法的详细介绍，一定程度上不利于青年研究者将RCT方法应用到具体的研究实践中。尽管已有文献报道了基于SAS和Stata的随机分组方法(胡良平 et al. 2011; 刘玉秀 et al. 2001; 吴春霖, 王镭, and 李卫兵 2013)，但是相较于SAS和Stata而言，R语言安装与使用更为简便，并且由于其开源免费的特性更易于获取。因此，本文基于已有文献，详细介绍三种随机分组的具体方法和操作过程，以及提供基于R语言的实现代码，供其他学者在具体研究中作为参考。"
  },
  {
    "objectID": "chapter_6.html#随机分组方法简介",
    "href": "chapter_6.html#随机分组方法简介",
    "title": "6  番外篇（一）：随机化分组",
    "section": "6.2 随机分组方法简介",
    "text": "6.2 随机分组方法简介\n  随机化的概念最早由Fisher于1926年提出(Aylmer Fisher 1926)，其后被广泛应用于实验研究中，已成为控制干预组和对照组之间混杂偏倚的必要手段。随机化具体指将受试者按照相同的概率分配在干预组和非干预组的过程(Fleiss, Levin, and Paik 2003)，根据实验设计的样本大小以及受试者招募时间长短等因素的不同，随机化分组方法很多种，但较为常用的有三种：简单随机分组（simple randomization）、区组随机分组（block randomization）和分层随机分组（stratified randomization）(Kang, Ragan, and Park 2008)。具体如下：\n\n简单随机分组是指按照一组随机序列将受试者以相同的概率分配至不同的实验组中，最典型的简单随机分组方法是抛硬币（flipping a coin）。简单随机分组操作容易，但是存在一个明显的不足，即当受试者样本量较小时，无法保证纳入不同实验组的受试者数量相等(Kang, Ragan, and Park 2008; Kernan et al. 1999)。一般认为当受试者样本量小于200时不宜采用简单随机分组(Kang, Ragan, and Park 2008)。实际上，在具体的研究过程中（如临床药物试验），通常要求干预组和对照组所分配的受试者数量相等或达到一定的比例，因此即使样本量大于200简单随机分组也不适用。然而，在实际研究过程中，会对简单随机分组方法进行一定的变化以满足研究设计需要，具体见下节。\n区组随机分组是指先根据受试者样本量大小及实验分组数量和组间比例设定好区组（block）的长度和数量，将特征相似或相同（如入组时间、地区来源）的受试者编入同一区组，然后在每个区组内部进行简单随机分组。区组随机分组可以有效保证不同的实验组中所分配的受试者数量严格满足研究设计要求。不仅如此，区组随机分组的另一个优势在于，当入组后的受试者出现某种不满足实验要求而需退出实验时，可以直接将该受试者所在的区组舍弃另外补充新的区组，而不破坏整个实验的随机化。\n分层随机分组是指先按已知的混杂因素对受试者进行分层后，再在各个层内采用简单随机或者区组随机方法将受试者分配至不同的实验组。分层随机分组可以在实验设计阶段就对部分混杂因素进行控制，从而提高不同实验组中受试者的基线均衡性(Kernan et al. 1999)。\n\n  其他随机分组方法还有：协变量自适应随机分组（covariate adaptive randomization）和响应自适应随机分组（ response-adaptive randomization）。需要补充的是，在实际研究过程中，很难一次性招募足够的受试者，然后再为受试者分配随机号进行随机分组工作。通常情况是会设定一段时间（一个月或三个月，视具体情况而定）的招募入组期，当有受试者满足入组条件后就需要进行随机分组，既而接受相应的干预，因此，这就需要在受试者入组之前准备好随机分组规则，受试者根据进入研究的顺序，依据预先准备好的随机分组表进入相应的实验组。"
  },
  {
    "objectID": "chapter_6.html#随机分组的具体过程及r语言代码",
    "href": "chapter_6.html#随机分组的具体过程及r语言代码",
    "title": "6  番外篇（一）：随机化分组",
    "section": "6.3 随机分组的具体过程及R语言代码",
    "text": "6.3 随机分组的具体过程及R语言代码\n\n6.3.1 简单随机分组（simple randomization）\n  简单随机分组的基本步骤一共有五点：1）按照实验设计的受试者样本量生成受试者编号；2）利用随机函数生成与受试者编号数量相等的随机数字；3）对生成的随机数字进行编秩；4）根据实验设计的组数及分组比例，对随机数字的秩序按照大小或者奇偶数进行分组；5）获得符合实验设计要求的随机分组表。本文提供了一段基于R语言的自编函数，能够满足两组及以上分组且不同组间比例的简单随机分组需求，自编函数代码及具体含义如下：\n\nsimple_random <- function(size, grp = 2, T_2_C = \"1:1\"){\n    set.seed(20210412)\n    id_num <- seq(1, size, 1)\n    random_seq <- runif(n = size, \n                        min = 0, max = 1)\n    int_rank <- rank(random_seq)\n    \n    ratio_T <- as.numeric(substr(T_2_C, 1, 1))\n    ratio_C <- as.numeric(substr(T_2_C, 3, 3))\n    \n    if (grp == 2) {\n        group <- ifelse(int_rank <= \n                        size/(ratio_T + ratio_C), \n                        \"T\", \"C\")\n    } else if (grp > 3) {\n        group <- cut(int_rank, breaks = grp, \n                     labels = paste(\"Group\", 1:grp))\n    }\n    \n    df <- data.frame(\"ID\" = id_num, \n                     \"RandomNum\" = random_seq, \n                     \"Rank\" = int_rank, \"Group\" = group)\n    \n    # write.csv(df, \"simple randomization table.csv\", \n    #           row.names = FALSE)\n    \n    return(df)\n}\n\n\n函数名为simple_random，函数共有3个参数，分别为size（样本量大小）、grp（分组数）和T_2_C（干预组与对照组比例），grp和T_2_C为默认参数，分别默认赋值为2和”1:1”；\nset.seed()函数用于设定随机种子数，以便下一步利用随机函数生成的随机数可再现，此步骤在利用计算机进行实验研究的随机化分组时十分关键，因为只有固定了随机种子数，才可以保证实验研究中生成的随机分组表可以复现，这一点对于研究论文的发表和临床药物试验质量稽查十分重要。利用计算机生成的随机数本质为伪随机数（Pseudorandomness），它是通过特定的算法计算所得，其满足随机数的统计特征，在相同的计算平台下可以重复出现。\nseq()函数用于生成从1开始的受试者编号；\nrunif()函数用于生成与受试者人数相等的服从均匀分布的0到1之间随机数字，其中min和max参数可以任意指定，此处也可采用R语言中自带的其他随机函数，如rnorm()等，也可以采用sample()函数直接生成非负随机整数；\nrank()函数用于对生成的随机数字按照升序进行编秩；\nratio_T和ratio_C变量用于提取T_2_C参数的赋值，用于分组比例的计算；\nif条件语句中的内容即是对随机数字的秩进行分组，其规则为：当为两组且组间比例为1:1时，随机数字的秩位于前50%的受试者编号设定为干预组（“T”），后50%对应的受试者编号设定为对照组（“C”），当为两组且组间比例为1:N时，随机数字的秩位于前1/(1+N)的受试者编号设定为干预组（“T”），剩余受试者编号设定为对照组（“C”），当为三组及以上时，本文提供的自编函数仅支持组间比例相等的情况，其分组规则为将随机数字的秩进行等分，与秩对应的受试者编号分别分入”Group 1”、“Group 2”…“Group N”；\ndata.frame()和write.csv()函数用于生成随机分组表并输出为csv格式，随机分组表中包含受试者编号、随机数字、随机数字的秩、分组。\n\n\n实例一：为检验某一治疗高血压新药A的临床疗效，选择已上市B药作为对照组，开展单中心双臂临床试验，需要受试者240名，每组各120名受试者，请按简单随机分组方法制定随机分组表。利用本文提供的自编函数simple_random()则可将参数size设定为240， 参数grp设定为2，参数T_2_C设定为”1:1”（注意此处为英文半角引号），具体如下：\n\n\nres <- simple_random(size = 240, grp = 2, T_2_C = \"1:1\")\n\n  函数输出的随机分组表见 表 6.1 ，其中分组为T和C的受试者分别为120名。\n\nlibrary(knitr)\nkable(res[c(1:12, 229:240),])\n\n\n\n表 6.1: 简单随机分组表\n\n\n\nID\nRandomNum\nRank\nGroup\n\n\n\n\n1\n1\n0.8323749\n198\nC\n\n\n2\n2\n0.9552218\n228\nC\n\n\n3\n3\n0.5978788\n134\nC\n\n\n4\n4\n0.3507679\n75\nT\n\n\n5\n5\n0.4315742\n91\nT\n\n\n6\n6\n0.6332632\n147\nC\n\n\n7\n7\n0.7801558\n189\nC\n\n\n8\n8\n0.4699095\n102\nT\n\n\n9\n9\n0.3853540\n81\nT\n\n\n10\n10\n0.6336118\n148\nC\n\n\n11\n11\n0.7365508\n178\nC\n\n\n12\n12\n0.4967514\n106\nT\n\n\n229\n229\n0.6637343\n156\nC\n\n\n230\n230\n0.9801347\n235\nC\n\n\n231\n231\n0.1659937\n36\nT\n\n\n232\n232\n0.4225586\n90\nT\n\n\n233\n233\n0.3599470\n78\nT\n\n\n234\n234\n0.6065274\n136\nC\n\n\n235\n235\n0.5213893\n111\nT\n\n\n236\n236\n0.8585871\n202\nC\n\n\n237\n237\n0.6808066\n161\nC\n\n\n238\n238\n0.2789617\n63\nT\n\n\n239\n239\n0.9939375\n236\nC\n\n\n240\n240\n0.7866525\n190\nC\n\n\n\n\n\n\n\n\n6.3.2 区组随机分组（block randomization）\n\n6.3.2.1 方法一\n  区组随机分组一般有两种办法，第一种是先根据实验分组情况设定区组长度和区组类型组合，再根据区组数量对区组类型进行随机抽样，从而获得每个区组内部的分组情况(Kang, Ragan, and Park 2008)，其基本步骤为：1）按照实验设计的受试者样本量生成受试者编号；2）设定区组长度和区组组合；3）根据样本量计算区组数；4）根据区组数从区组组合中随机抽取对应数量的区组类型；5）将随机抽取的区组类型顺序连接，从而获得符合实验设计要求的随机分组表。本文同样提供了一段基于R语言的自编函数，但仅能满足组间比例为1:1的区组随机分组需求，自编函数代码及具体含义如下：\n\nblock_random_m1 <- function(size, \n                            block_len, \n                            block_category){\n\n    id_num <- seq(1, size, 1)\n    block <- size / block_len\n    block_set <- c()\n    block_num <- c()\n    for (i in 1:block) {\n        set.seed(20210412+i)\n        block <- sample(block_category, 1, \n                        replace = FALSE)\n        block_set[i] <- block\n        block_num <- append(block_num, \n                            rep(i, block_len))\n        \n    }\n    group <- as.vector(\n                      unlist(\n                            strsplit(\n                                paste0(block_set, \n                                       collapse=\";\"), \n                                       split=\";\")\n                            )\n                      )\n    df <- data.frame(\"ID\" = id_num, \n                     \"Block\" = block_num, \n                     \"Group\" = group)\n    \n    # write.csv(df, \n    #           \"block radomization table 1.csv\", \n    #           row.names = FALSE)\n    \n    return(df)\n}\n\n\n函数名为block_random_m1，函数共有3个参数，分别为size（样本量大小）、block_len（区组长度）和block_category（区组组合）；\nseq()函数和set.seed()作用同上；\nblock变量为根据样本量和区组长度计算的区组数量；\n两组实验中常用的区组长度为4，那么参数block_ken即为4，对应的区组组合共有6种情况（“TTCC”, “TCTC”, “TCCT”, “CCTT”, “CTCT”, “CTTC”），其中T表示干预组，C表示对照组，用参数block_category来指定；\nfor循环中即是依据计算所得的区组数量，利用sample()函数从全部区组组合中随机抽取相应数量的区组组合，其中sample()函数的第二个参数用于指定每次抽取的组合数，本文中采取每次抽取1个，第三个参数replace用于指定是否为有放回抽样，当每次抽取数量大于1时应采取无放回抽样，replace的值应设定为FALSE；\nblock_set变量表示将for循环随机抽取的区组组合以向量的形式存储，以便于后续输出结构化的随机分组表；\nblock_num变量用于记录区组的编号；\nas.vector()函数是将block_set变量中存储的区组组合连接成与样本量相同的长度，以便与受试者编号对应；\ndata.frame()和write.csv()函数用于生成随机分组表并输出为csv格式，随机分组表中包含受试者编号、区组编号、分组。\n\n\n实例二：为检验某一治疗肿瘤新化疗方案的临床疗效，选择常规化疗方案作为对照组，开展单中心双臂临床试验，需要受试者120名，每组各60名受试者，受试者入组期为3个月，化疗期为6个月，请按区组随机分组方法制定随机分组表。利用本文提供的自编函数block_random_m1()则可将参数size设定为120， 参数block_len设定为4，参数block_category设定为c(“T;T;C;C”, “T;C;T;C”, “T;C;C;T”, “C;C;T;T”, “C;T;C;T”, “C;T;T;C”)，具体如下：\n\n\nres_block1 <- \n    block_random_m1(size = 120, \n                    block_len = 4, \n                    block_category = c(\"T;T;C;C\", \n                                       \"T;C;T;C\", \n                                       \"T;C;C;T\", \n                                       \"C;C;T;T\", \n                                       \"C;T;C;T\", \n                                       \"C;T;T;C\"))\n\n  函数输出的随机分组表见 表 6.2，其中共产生30个区组，每个区组内干预组和对照组各2名受试者。在实际的研究中区组长度的设定可适当变化，具体可见本文讨论部分。\n\nkable(res_block1[c(1:12, 109:120), ])\n\n\n\n表 6.2: 方法一区组随机分组表\n\n\n\nID\nBlock\nGroup\n\n\n\n\n1\n1\n1\nC\n\n\n2\n2\n1\nT\n\n\n3\n3\n1\nT\n\n\n4\n4\n1\nC\n\n\n5\n5\n2\nT\n\n\n6\n6\n2\nT\n\n\n7\n7\n2\nC\n\n\n8\n8\n2\nC\n\n\n9\n9\n3\nT\n\n\n10\n10\n3\nC\n\n\n11\n11\n3\nC\n\n\n12\n12\n3\nT\n\n\n109\n109\n28\nC\n\n\n110\n110\n28\nT\n\n\n111\n111\n28\nT\n\n\n112\n112\n28\nC\n\n\n113\n113\n29\nT\n\n\n114\n114\n29\nT\n\n\n115\n115\n29\nC\n\n\n116\n116\n29\nC\n\n\n117\n117\n30\nT\n\n\n118\n118\n30\nC\n\n\n119\n119\n30\nC\n\n\n120\n120\n30\nT\n\n\n\n\n\n\n\n\n6.3.2.2 方法二\n  第二种是先根据受试者数量生成随机数字，然后根据实验分组情况设定区组长度并依次对区组编号，再按区组编号分组对生成的随机数字编秩，最后根据各区组内随机数字秩的大小关系进行分组，其基本步骤为：1）按照实验设计的受试者样本量生成受试者编号；2）设定区组长度并根据受试者数量计算区组数并依次编号；3）利用随机函数生成与受试者编号数量相等的随机数字；4）按区组编号分组计算随机数字的秩；5）在各区组内，按照实验设计的组数及分组比例，对随机数字的秩序按照大小或者奇偶数进行分组；6）获得符合实验设计要求的随机分组表。本文同样提供了一段基于R语言的自编函数，能满足两组及多组且组间比例不同的区组随机分组需求，自编函数代码及具体含义如下：\n\nlibrary(dplyr, warn = FALSE)\nblock_random_m2 <- function(size, block_len, \n                            grp = 2, T_2_C = \"1:1\"){\n\n    id_num <- seq(1, size, 1)\n    block <- size / block_len\n    \n    block_num <- c()\n    for (i in 1:block) {\n        block_num <- append(block_num, \n                            rep(i, block_len))\n    }\n    \n    random_seq <- runif(n = size, \n                        min = 0, max = 1)\n    \n    df <- data.frame(\"ID\" = id_num, \n                     \"BlockNum\" = block_num, \n                     \"RandomNum\" = random_seq)\n\n    df_rank <- df %>% \n               group_by(BlockNum) %>% \n               mutate(Rank = rank(RandomNum))\n    \n    ratio_T <- as.numeric(substr(T_2_C, 1, 1))\n    ratio_C <- as.numeric(substr(T_2_C, 3, 3))\n    \n    if (grp == 2){\n        df_rank$Group <- \n            ifelse(df_rank$Rank <= \n                   block_len/(ratio_T + ratio_C), \n                   \"T\", \"C\")\n    } else if (grp >2) {\n        df_rank$Group <- \n            cut(df_rank$Rank, \n                breaks = grp, \n                labels = paste(\"Group\", 1:grp))\n    }\n    \n    # write.csv(df_rank, \n    #            \"block radomization table 2.csv\", \n    #            row.names = FALSE)   \n    return(df_rank)\n}\n\n\n函数名为block_random_m2，函数共有4个参数，分别为size（样本量大小）、block_len（区组长度）、grp（分组数）和T_2_C（干预组与对照组比例），grp和T_2_C为默认参数，分别默认赋值为2和”1:1”；\nseq()和runif()函数作用同上；\n由于此方法需要进行分组编秩，考虑到R语言自带分组功能使用较为不便，本文在自编函数中引入了第三方程序包dplyr，可通过install.packages(“dplyr”)安装，其中group_by()、mutate()及%>%管道函数为dplyr包中引入的函数;\n变量ratio_T和ratio_C，以及if条件语句中的功能与简单随机分组相同。\ndata.frame()和write.csv()函数用于生成随机分组表并输出为csv格式，随机分组表中包含受试者编号、区组编号、随机数字、分组。\n\n  同样以实例二为例，利用block_random_m2()函数可以获得同表2相似的结果，具体使用方式如下，输出结果见 表 6.3 。\n\nres_block2 <- \n    block_random_m2(size = 120, \n                    block_len = 4, \n                    grp = 2, \n                    T_2_C = \"1:1\")\n\n\nkable(res_block2[c(1:12, 109:120), ])\n\n\n\n表 6.3: 方法二区组随机分组表\n\n\nID\nBlockNum\nRandomNum\nRank\nGroup\n\n\n\n\n1\n1\n0.2355342\n3\nC\n\n\n2\n1\n0.1950892\n1\nT\n\n\n3\n1\n0.7458419\n4\nC\n\n\n4\n1\n0.2197261\n2\nT\n\n\n5\n2\n0.0764369\n1\nT\n\n\n6\n2\n0.6039926\n4\nC\n\n\n7\n2\n0.4317742\n2\nT\n\n\n8\n2\n0.4814425\n3\nC\n\n\n9\n3\n0.7359029\n2\nT\n\n\n10\n3\n0.8289157\n3\nC\n\n\n11\n3\n0.9952405\n4\nC\n\n\n12\n3\n0.5653650\n1\nT\n\n\n109\n28\n0.7804675\n4\nC\n\n\n110\n28\n0.6807008\n3\nC\n\n\n111\n28\n0.3534151\n2\nT\n\n\n112\n28\n0.3243371\n1\nT\n\n\n113\n29\n0.3971316\n2\nT\n\n\n114\n29\n0.5407747\n4\nC\n\n\n115\n29\n0.4979685\n3\nC\n\n\n116\n29\n0.1560650\n1\nT\n\n\n117\n30\n0.6870941\n1\nT\n\n\n118\n30\n0.7521457\n2\nT\n\n\n119\n30\n0.8746505\n4\nC\n\n\n120\n30\n0.7950460\n3\nC\n\n\n\n\n\n\n\n\n\n6.3.3 分层随机分组（stratified randomization）\n  从操作层面而言，分层随机分组的本质是根据混杂因素对受试者进行分层后，再在各个层内采用简单随机或者区组随机的方法将受试者分配至不同的实验组。如在进行多中心肿瘤疾病的临床试验中，研究中心的情况和受试者肿瘤的分型分期会对治疗结局产生明显的影响，但是若采取简单随机或者区组随机分组的方法，很难确保不同研究中心入组的受试者特征相似，以及不同实验组间受试者肿瘤分型分期均衡，因此需要对此类明显可能造成实验结局偏倚的变量进行分层控制。假设存在3个研究中心，肿瘤共分为2型2期，那么就可以设置12个层，再在12个层内分别采用简单随机或者区组随机分组方法生成随机分组表，由于所采用方法同上，此处不再详细叙述。"
  },
  {
    "objectID": "chapter_6.html#讨论",
    "href": "chapter_6.html#讨论",
    "title": "6  番外篇（一）：随机化分组",
    "section": "6.4 讨论",
    "text": "6.4 讨论\n  本文详细梳理了当前阶段实验研究中常用的随机分组方法，并以此为基础，结合现阶段研究领域常用的开源统计软件R语言，详细介绍了实验研究中进行随机分组的具体过程，并通过自编函数的形式提出了集成化解决方案，且通过实例验证了自编函数的可靠性，可为研究者在开展实验研究的随机分组时提供借鉴和参考。有几点需要讨论说明，如下：\n  随机种子数的设定及管理。在RCT研究中，随机化通常和盲法联合使用，而随机分组的结果和规则是盲法的重要内容之一，因此除了需要保证不将随机分组结果透露至研究人员或者受试者外，还需要保证随机分组规则不可轻易被猜出。在利用计算机平台进行随机数生成时，合理设定随机种子数是保证生成的随机数可再现的重要前提。那么随机种子数的设定就十分关键，一般情况下随机种子数不可设置过于简单，如1或者2，亦或者年份。随机种子数应由项目组的统计人员保管，在实验揭盲或者结束之前不可透露给其他研究相关人员。\n  区组随机中区组长度的设定。区组长度由研究者确定，对于实验组为两组时通常取4和6(Kang, Ragan, and Park 2008)。考虑到区组长度较短且单一时，研究人员容易总结发现其中规律，可能会破坏盲法，因此在设定区组长度时可以采用复合方案，即在同一份随机分组表中同时设置4和6的区组长度，从而增加区组的复杂性，减少可预见性。另一方面，由于在具体的实验研究中，受试者脱落或者中途退出的情况并不少见，因此在生成随机分组表时，一般情况下推荐多设置1至2个区组，以备进行补充。\n  除本文中详细介绍的三种常用随机分组方法外，为了应对受试者入组的复杂情况，还有协变量自适应随机分组（covariate adaptive randomization）(Kang, Ragan, and Park 2008)和响应自适应随机分组（ response-adaptive randomization）(刘晓燕 et al. 2008)方法，由于以上方法已有学者开发出了供R语言直接使用的软件包（carat），因此本文中未详细介绍，具体可见 <https://CRAN.R-project.org/package=carat>。\n\n\n\n\nAylmer Fisher, Ronald. 1926. “The Arrangement of Field Experiments.” Journal of the Ministry of Agriculture of Great Britain 33: 503–13. https://doi.org/10.23637/ROTHAMSTED.8V61Q.\n\n\nCox, D R. 2009. “Randomization in the Design of Experiments.” International Statistical Review 77 (3): 415–29.\n\n\nFleiss, Joseph L, Bruce A Levin, and Myunghee Cho Paik. 2003. Chapter 5: How to Randomize in Statistical Methods for Rates and Proportions. Hoboken, N.J.: Wiley-Interscience.\n\n\nKang, Minsoo, Brian G. Ragan, and Jae-Hyeon Park. 2008. “Issues in Outcomes Research: An Overview of Randomization Techniques for Clinical Trials.” Journal of Athletic Training 43 (2): 215–21. https://doi.org/10.4085/1062-6050-43.2.215.\n\n\nKendall, J M. 2003. “Designing a Research Project: Randomised Controlled Trials and Their Principles.” Emergency Medicine Journal 20 (2): 164–68. https://doi.org/10.1136/emj.20.2.164.\n\n\nKernan, Walter N, Catherine M Viscoli, Robert W Makuch, Lawrence M Brass, and Ralph I Horwitz. 1999. “Stratified Randomization for Clinical Trials.” Journal of Clinical Epidemiology 52 (1): 19–26. https://doi.org/10.1016/S0895-4356(98)00138-3.\n\n\nRoberts, C., and D. Torgerson. 1998. “Understanding Controlled Trials: Randomisation Methods in Controlled Trials.” BMJ 317 (7168): 1301–10. https://doi.org/10.1136/bmj.317.7168.1301.\n\n\n刘晓燕, 陈峰, 魏永越, 柏建岭, and 于浩. 2008. “响应—自适应随机化分组方法.” 10.3969/j.issn.1009-2501.2008.06.016. 临床药理学 13 (6): 684–89.\n\n\n刘玉秀, 姚晨, 杨友春, and 陈峰. 2001. “随机化临床试验及随机化的SAS实现.” 10.3969/j.issn.1009-2501.2001.03.001. 中国临床药理学与治疗学 6 (3): 193–95. https://doi.org/10.3969/j.issn.1009-2501.2001.03.001.\n\n\n吴春霖, 王镭, and 李卫兵. 2013. “临床试验随机化分组及其Stata的实现.” 10.7507/1672-2531.20130041. 中国循证医学杂志 13 (2): 242–44. https://doi.org/10.7507/1672-2531.20130041.\n\n\n李立明. 2007. 流行病学. 北京: 人民卫生出版社.\n\n\n胡良平, 关雪, 毛玮, and 高辉. 2011. “各种常见随机化的sas实现.” 中华脑血管病杂志(电子版) 5 (01): 68–76."
  },
  {
    "objectID": "chapter_7.html",
    "href": "chapter_7.html",
    "title": "7  番外篇（二）：主成分分析",
    "section": "",
    "text": "主成分分析（Principal component analysis，PCA）是由Pearson在1901年提出的，后来被Hotelling在1933进行了发展。PCA目的是把多个相关的原始变量降维成少数几个不相关的主成分，其基本原理是原始变量的线性组合。"
  },
  {
    "objectID": "chapter_7.html#进行主成分分析的步骤",
    "href": "chapter_7.html#进行主成分分析的步骤",
    "title": "7  番外篇（二）：主成分分析",
    "section": "7.2 进行主成分分析的步骤",
    "text": "7.2 进行主成分分析的步骤\n\n7.2.1 数据预处理\n  原始数据矩阵或者相关系数矩阵都可以进行分析，需要注意的几点：\n\n原始数据矩阵实际是计算协方差矩阵进行的主成分分析。\n原始数据矩阵只适用于度量单位相同，或者差别不大时进行分析，如果差别大可以进行Z得分转换。\n不论是原始数据矩阵还是相关系数矩阵，都不能有缺失值。\n\n\n\n7.2.2 确定主成分个数\n  在计算主成分得分之前，需要先判断主成分个数，通常有四种准则：\n\n根据先验经验或者理论知识。\nKaiser-Harris准则，只保留特征值大于1的主成分。\nCattell碎石检验，通过绘制特征值与主成分数的图形，只保留在图形变化最大处之上的主成分。\n平行分析，依据与初始矩阵相同大小的随机数据矩阵来判断要提取的特征值，若基于真实数据的某个特征值大于一组随机数据矩阵相 应的平均特征值，那么该主成分可以保留。\n\n  通常需要结合四种准则，综合判断主成分个数。\n\n\n7.2.3 计算主成分\n  在R语言中既可以使用base包中的princomp()函数，也可以使用psych包中的principal()函数。\n\n\n7.2.4 旋转主成分\n  旋转是一系列将成分载荷阵变得更容易解释的数学方法，它们尽可能地对成分去噪。旋转方法有两种：\n\n正交旋转：使选择的成分保持不相关\n斜交旋转：和让它们变得相关\n\n  旋转方法也会依据去噪定义的不同而不同。最流行的正交旋转是方差极大旋转，它试图对载荷阵的列进行去噪，使得每个成分只是由一组有限的变量来解释（即载荷阵每列只有少数几个很大的载荷，其他都是很小的载荷）。\n\n\n7.2.5 解释结果并计算主成分得分\n  对结果进行解释，并计算各个主成分的得分，需要注意的是：使用相关系数矩阵进行主成分分析时，无法直接获得主成分得分，只能得到原始变量的主成分系数。"
  },
  {
    "objectID": "chapter_7.html#主成分分析的示列",
    "href": "chapter_7.html#主成分分析的示列",
    "title": "7  番外篇（二）：主成分分析",
    "section": "7.3 主成分分析的示列",
    "text": "7.3 主成分分析的示列\n  建议在R语言中，使用psych包进行主成分分析。\n\n7.3.1 载入包和数据\n  示列数据集使用的是psych包中自带的Harman23.cor数据集。Harman23.cor是一个list，需要提取出分析使用的cov数据集.\n\nlibrary(psych)\n\ndata(Harman23.cor)\nstr(Harman23.cor) \n\nList of 3\n $ cov   : num [1:8, 1:8] 1 0.846 0.805 0.859 0.473 0.398 0.301 0.382 0.846 1 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:8] \"height\" \"arm.span\" \"forearm\" \"lower.leg\" ...\n  .. ..$ : chr [1:8] \"height\" \"arm.span\" \"forearm\" \"lower.leg\" ...\n $ center: num [1:8] 0 0 0 0 0 0 0 0\n $ n.obs : num 305\n\ndf <- Harman23.cor$cov\nstr(df)\n\n num [1:8, 1:8] 1 0.846 0.805 0.859 0.473 0.398 0.301 0.382 0.846 1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:8] \"height\" \"arm.span\" \"forearm\" \"lower.leg\" ...\n  ..$ : chr [1:8] \"height\" \"arm.span\" \"forearm\" \"lower.leg\" ...\n\n\n\n\n7.3.2 判断主成分个数\n  psych包中给出了一个特别有用的函数fa.parallel(),可以很方便的同时给出Kaiser-Harris准则、Cattell碎石检验、平行分析的结果。由上步骤可以看出，df实际为相关系数矩阵，n.obs是指定原始观测数，n.iter是指定平行分析的随机迭代次数，fa是用来指定进行主成分分析，取值有三类（‘fa’，‘pc’，‘both’）。由 图 7.1 看出，特征值大于1，拐点之上，大于随机的特征值，满足以上三个条件，只有前两个主成分，因此主成分个数为2.\n\nfa.parallel(df, \n            n.obs = 302, \n            n.iter = 100, \n            fa = 'pc', \n            show.legend = TRUE)\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  2 \n\n\n\n\n\n图 7.1: Parallel Analysis Scree Plots\n\n\n\n\n\n\n7.3.3 主成分分析\n  结果中：\n\nPC1和PC2既为2个主成分在各个原始变量上的载荷（loading，也就是相关系数），可以看出系数都较高。\nh2为成分公因子方差，用来说明主成分对每个变量的方差解释度，此值越高说明解释力度越大。\nu2栏指成分唯一性——方差，用来说明无法被主成分解释的比例，h2与u2之和为1。\nProportion Var为每个主成分对整个数据集的解释程度。\nCumulative Var是Proportion Var的累计和，其值至少应该需大于0.80以上。\n\n  可以看出，PC1和PC2在部分变量上都有较高的解释度，不满足各个主成分之间应该正交的条件，需进行旋转。\n\nprincipal(df, \n          nfactors = 2, \n          rotate = 'none', \n          n.obs = 302, \n          scores = TRUE)\n\nPrincipal Components Analysis\nCall: principal(r = df, nfactors = 2, rotate = \"none\", n.obs = 302, \n    scores = TRUE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n                PC1   PC2   h2    u2 com\nheight         0.86 -0.37 0.88 0.123 1.4\narm.span       0.84 -0.44 0.90 0.097 1.5\nforearm        0.81 -0.46 0.87 0.128 1.6\nlower.leg      0.84 -0.40 0.86 0.139 1.4\nweight         0.76  0.52 0.85 0.150 1.8\nbitro.diameter 0.67  0.53 0.74 0.261 1.9\nchest.girth    0.62  0.58 0.72 0.283 2.0\nchest.width    0.67  0.42 0.62 0.375 1.7\n\n                       PC1  PC2\nSS loadings           4.67 1.77\nProportion Var        0.58 0.22\nCumulative Var        0.58 0.81\nProportion Explained  0.73 0.27\nCumulative Proportion 0.73 1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.05 \n with the empirical chi square  46.77  with prob <  1.1e-05 \n\nFit based upon off diagonal values = 0.99\n\n\n  从结果可以看出，PC1和PC2变成了RC1和RC2，即表示经过旋转了，可以很清楚的看出：旋转后，RC1在前4个变量（height – lower.leg）上具有更高的解释度。RC2在后4个变量（weight – chest.width）上具有更高的解释度，说明两个主成分正交（不相关），同时可以发现，旋转后并不改变Cumulative Var\n\ndfp <- principal(df, nfactors = 2, \n                 rotate = 'varimax', \n                 n.obs = 302, \n                 scores = TRUE)\ndfp\n\nPrincipal Components Analysis\nCall: principal(r = df, nfactors = 2, rotate = \"varimax\", n.obs = 302, \n    scores = TRUE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n                RC1  RC2   h2    u2 com\nheight         0.90 0.25 0.88 0.123 1.2\narm.span       0.93 0.19 0.90 0.097 1.1\nforearm        0.92 0.16 0.87 0.128 1.1\nlower.leg      0.90 0.22 0.86 0.139 1.1\nweight         0.26 0.88 0.85 0.150 1.2\nbitro.diameter 0.19 0.84 0.74 0.261 1.1\nchest.girth    0.11 0.84 0.72 0.283 1.0\nchest.width    0.26 0.75 0.62 0.375 1.2\n\n                       RC1  RC2\nSS loadings           3.52 2.92\nProportion Var        0.44 0.37\nCumulative Var        0.44 0.81\nProportion Explained  0.55 0.45\nCumulative Proportion 0.55 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.05 \n with the empirical chi square  46.77  with prob <  1.1e-05 \n\nFit based upon off diagonal values = 0.99\n\n\n\n\n7.3.4 计算主成分得分\n  在principal函数中的scores即是制定是否计算主成分得分的，如果为TRUE，则在dfp中已保存有得分，利用dfp$scores即可以取出得分。但是本示列中使用的是相关系数矩阵，无法计算得分，只能计算得分权重，为：\n\ndfp$weights\n\n                       RC1         RC2\nheight          0.27524417 -0.04748169\narm.span        0.29673051 -0.08005490\nforearm         0.29823990 -0.09158460\nlower.leg       0.28014088 -0.06027214\nweight         -0.06053059  0.33228637\nbitro.diameter -0.07752349  0.32477593\nchest.girth    -0.10366026  0.33763942\nchest.width    -0.03730720  0.27392667"
  },
  {
    "objectID": "chapter_references.html",
    "href": "chapter_references.html",
    "title": "参考文献",
    "section": "",
    "text": "Aylmer Fisher, Ronald. 1926. “The Arrangement of Field\nExperiments.” Journal of the Ministry of Agriculture of Great\nBritain 33: 503–13. https://doi.org/10.23637/ROTHAMSTED.8V61Q.\n\n\nCalonico, Sebastian, Matias D. Cattaneo, Max H. Farrell, and Rocio\nTitiunik. 2022. Rdrobust: Robust Data-Driven Statistical Inference\nin Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nCox, D R. 2009. “Randomization in the Design of\nExperiments.” International Statistical\nReview 77 (3): 415–29.\n\n\nD. Cattaneo, Matias, Nicolas Idroboy, and Roc Titiunik. 2017. A\nPractical Introduction to Regression Discontinuity Designs: Volume\ni. collingwoodresearch.com.\n\n\nDasu, T., and T. Johnson. 2003. Exploratory Data Mining and Data\nCleaning. John Wiley & Sons, Inc.\n\n\nFleiss, Joseph L, Bruce A Levin, and Myunghee Cho Paik. 2003.\nChapter 5: How to Randomize in\nStatistical Methods for Rates and Proportions.\nHoboken, N.J.: Wiley-Interscience.\n\n\nJacob, Robin, Pei Zhu, and Marie-Andrée. 2012. A Practical Guide to\nRegression Discontinuity. MDRC.org.\n\n\nKabacoff, Robert. 2013. R in Action, Data Analysis and Graphics with\nr. Manning.\n\n\nKang, Minsoo, Brian G. Ragan, and Jae-Hyeon Park. 2008. “Issues in\nOutcomes Research: An Overview of\nRandomization Techniques for Clinical\nTrials.” Journal of Athletic Training 43 (2):\n215–21. https://doi.org/10.4085/1062-6050-43.2.215.\n\n\nKendall, J M. 2003. “Designing a Research Project: Randomised\nControlled Trials and Their Principles.” Emergency Medicine\nJournal 20 (2): 164–68. https://doi.org/10.1136/emj.20.2.164.\n\n\nKernan, Walter N, Catherine M Viscoli, Robert W Makuch, Lawrence M\nBrass, and Ralph I Horwitz. 1999. “Stratified\nRandomization for Clinical Trials.”\nJournal of Clinical Epidemiology 52 (1): 19–26. https://doi.org/10.1016/S0895-4356(98)00138-3.\n\n\nRoberts, C., and D. Torgerson. 1998. “Understanding Controlled\nTrials: Randomisation Methods in Controlled Trials.”\nBMJ 317 (7168): 1301–10. https://doi.org/10.1136/bmj.317.7168.1301.\n\n\nRon, Cody. 2008. Cody’s Data Cleaning Techniques Using SAS.\nCary, NC: SAS Institute Inc.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations\nin r.” Journal of Statistical Software 45 (3): 1–67. https://doi.org/10.18637/jss.v045.i03.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n刘晓燕, 陈峰, 魏永越, 柏建岭, and 于浩. 2008.\n“响应—自适应随机化分组方法.”\n10.3969/j.issn.1009-2501.2008.06.016. 临床药理学 13 (6):\n684–89.\n\n\n刘玉秀, 姚晨, 杨友春, and 陈峰. 2001.\n“随机化临床试验及随机化的SAS实现.”\n10.3969/j.issn.1009-2501.2001.03.001. 中国临床药理学与治疗学 6\n(3): 193–95. https://doi.org/10.3969/j.issn.1009-2501.2001.03.001.\n\n\n吴春霖, 王镭, and 李卫兵. 2013.\n“临床试验随机化分组及其Stata的实现.”\n10.7507/1672-2531.20130041. 中国循证医学杂志 13 (2): 242–44. https://doi.org/10.7507/1672-2531.20130041.\n\n\n李立明. 2007. 流行病学. 北京:\n人民卫生出版社.\n\n\n胡良平, 关雪, 毛玮, and 高辉. 2011.\n“各种常见随机化的sas实现.”\n中华脑血管病杂志(电子版) 5 (01): 68–76.\n\n\n谢谦, 薛仙玲, and 付明卫. 2019.\n“断点回归设计方法应用的研究综述.” 经济与管理评论\n35 (2): 11.\n\n\n高涛, 肖楠, and 陈钢. 2013. R语言实战. 人民邮电出版社."
  }
]