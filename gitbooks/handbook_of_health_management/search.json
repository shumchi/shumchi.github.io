[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "卫生政策实证研究手册",
    "section": "",
    "text": "这是一本写给卫生管理与卫生政策专业的硕博士研究生的研究入门工具书，主要是分享一些笔者多年来积累的一些经验，以便帮助后来的青年学者快速提高科研素养，少走一些弯路。\n\n\n\n\n\n\n本书共分为五个章节：\n\n\n\n\n\n\n第一章：科学知识的获取与管理，主要介绍文献的检索与管理、知识谱系的记录与管理。\n第二章：数据库操作（Data Manipulate），主要介绍在实证研究中所涉及的有关数据库的增删改查、结构变换等方法。\n第三章：数据库清洗（Data Clean），主要介绍数据库清洗的步骤和方法。\n第四章：数据的描述，主要介绍如何在统计软件中进行统计描述及推断，并制作符合统计和学术规范的表格。\n第五章：数据的可视化，主要介绍科学研究中常见的统计图及在统计软件中的实现。\n第六章：实证研究中的因果推断，主要介绍在卫生政策研究中前沿的因果推断方法、注意事项及实现过程。"
  },
  {
    "objectID": "chapter_1.html",
    "href": "chapter_1.html",
    "title": "1  文献知识的获取与管理",
    "section": "",
    "text": "“读文献”应该是在科研生涯早期被导师要求进行的最多的科研活动了，足见文献的重要性1，但是很多同学起初对“读文献”是有些抵触的，常认为”读文献“没意思，光说不练，而多热衷于拿数据跑模型看统计显著性，而我们当年还对下现场调研兴趣颇高，不知道近些年是否还是这样。那这样的现象是否正常呢？其实正常也不正常。说正常主要在于读文献需要带着问题去阅读，否则容易觉得枯燥，阅读文献的热情不高自然是正常现象。说不正常在于，若未经过深入的文献研究，直接开始处理数据分析结果，容易出现研究方法误用或是缺少创新性等问题。因此，依然要重视“读文献”，培养良好的文献阅读习惯是科学研究的基本要求，它也是科研工作中的重要组成部分。\n那么，“读文献”的第一步就是检索文献了，如何在数以万计的文献中找到自己需要的研究，是一件具有挑战性的事情。刚刚进入研究生阶段的同学，面对这个问题的时候很多人都很茫然，从哪里检索文献？如何检索文献？哪些文献是应该要阅读的？等等，尽管这些在本科阶段都多少有课程讲解过，但是缺少实践的训练，很多人都忘得差不多了。\n第二步自然就是阅读文献，如何快速地从大量的文献中筛选出高价值的文章，并且从这些文章中获得有用的信息，这些既需要大量的练习也有一些技巧，而刚进入科研领域的研究生在这一方面通常有些欠缺。\n第三步就是管理文献了，少量的文献还可以较为从容进行存储和查找，但是如果当文献数量庞大，研究项目繁多，需要与其他研究组进行合作时，文献的管理工作就不可忽视了。建立良好的文献管理规范，对于研究组而言，是科研质量稳定且可持续性的重要保证2；对于个人而言，如果决定将科研作为自己的事业，那么阅读文献将有可能是伴随一生的工作3，在这个过程中积累下来的文献量将是很庞大，良好的文献管理习惯是发挥这些文献价值的重要途径之一。\n文献检索是文献综述（Literature Review）的重要基础。文献综述的目的简单来讲就是为研究提供背景，对相关研究主题进行概述，了解相关知识点或者认知的前沿在哪里，以及还存在的差距（Research Gap）。文献综述是开展一项研究必不可少的环节，通过文献综述可以：\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n增加对研究主题及其学术背景的熟悉程度\n梳理并总结出研究的理论框架和方法\n掌握目前所开展的研究在文献中的位置\n归纳并提炼出当前开展的研究对相关领域的贡献\n\n\n\n\n文献综述的写作过程大体可以分为以下几个环节，如 图 1.1\n\n\n\n图 1.1: 文献综述的流程图\n\n\n但是，本书中将不会介绍如何撰写文献综述，而是介绍文献综述过程中会涉及的几个重要但容易被忽视的的环节：文献检索、文献阅读和文献管理。"
  },
  {
    "objectID": "chapter_1.html#文献检索",
    "href": "chapter_1.html#文献检索",
    "title": "1  文献知识的获取与管理",
    "section": "1.2 文献检索",
    "text": "1.2 文献检索\n\n1.2.1 文献检索平台\n\n1.2.1.1 中文平台\n中文文献资料检索平台主要有以下三个：\n\n中国知网：综合性数据库，最全面，文献更新最及时，也是应用最广泛的。\n万方：综合性数据库，每月更新一次。\n维普：综合性数据库，每半月更新一次。\n\n\n\n1.2.1.2 英文平台\n英文文献检索的平台就相对丰富，主要各个出版集团都有自己的检索平台。首先简单介绍一下五大英文学术期刊出版集团：\n\n爱思唯尔（Elsevier）：1880年成立，总部在荷兰，现属于英国RELX Group集团，Lancet及其子刊、Cell及其子刊均属于该出版社，是目前出版量排名第一的出版集团，开发有自己的文献管理软件Mendeley。\n斯普林格（Springer）：德国出版集团，在2015年收购了Nature Publishing Group等几家出版社，现在旗下的出版社有：Nature、Springer、BioMed Central，出版量排名第二。\n约翰威立（Wiley）：美国出版集团，创始于1807年，主要集中在科学、工业、医学等学术领域的方向，神刊CA-A Cancer Journal for Clinicians就属于该出版社旗下。\n世哲（SAGE Publishing）：美国出版集团，创始于1965年，是世界第五大学术出版商。\n电气和电子工程师协会（IEEE）：IEEE主要致力于在电气、电子、计算机工程和与科学有关的领域开发和研究。\n\n另外还有几个顶级期刊不属于任何出版集团：\n\nScience：由美国科学促进会（American Association for the Advancement of Science）出版。\nThe New England Journal of Medicine（NEJM）：由在马萨诸塞州医学会的NEJM出版社拥有和出版。\nThe Journal of the American Medical Association（JAMA）：美国医学会会刊，由美国医学会（American Medical Association）出版。\n\n\n\n\n\n\n\n关于开放获取期刊（Open Access）\n\n\n\n\n\n\n开放获取（Open Access）是近些年出现的一种学术期刊类别，是相对于之前closed access而言的。其实本质上是学术出版领域的一种新的商业模式，与pay-for-access model相对。\n以往的学术期刊都是由图书馆和学术机构订阅，每年都需要支付大额的订阅费用，而读者也只有拥有一定的权限才能阅读。而OA期刊是由论文作者支付出版费用，读者可以免费获取，在一定程度上降低了学术论文的获取门槛，有利于研究成果的传播。\n关于OA期刊目前还存在一定的争议，有部分学者认为发表OA期刊是在花钱买论文，而出版集团为了获取利益，存在降低发表论文质量的可能。但是，OA期刊的发展目前来看是一种趋势，许多顶级期刊也开始创办OA期刊，比如Nature Communications、Lancet Public Health、JAMA Network Open等等都是OA期刊。\n\n\n\n\n了解了目前全球主要的学术期刊出版集团后，再介绍一下检索平台/数据库：\n\nWeb of Science（WoS）：WoS是由科睿唯安（Clarivate Analytics）公司开发的信息服务平台，数据来源于期刊、图书、专利、会议录、网络资源（包括免费开放资源）等。WoS所具有的三大引文索引系统（SCIE, SSCI, A&HCI）共收录了全球12400多种权威的、高影响力的国际学术期刊。WoS为非开放获取，提供的是收费服务，通常需要从学校图书馆进行访问。\nScienceDirect：是Elsevier旗下期刊的全文检索平台，为开放查询平台，可以查询Elsevie旗下出版期刊的论文和书籍等。\nScopus：是Elsevier于2004年11月推出的摘要和引文检索平台，虽然不直接提供全文下载服务，但是是目前全球规模最大的摘要和引文（A&I）数据库，涵盖了15000种科学、技术及医学方面的期刊，该数据库完整收录了Elsevier, Springer, Science等来自全球5000家出版社的20500多种经同行评议的出版物。\nWiley Online Library：是Wiley出版期刊的全文检索平台，开放查询。\nPubMed：美国国立医学图书馆国家生物技术信息中心提供的免费MEDLINE检索服务，主要提供免费的生物医学文摘型数据库检索服务。\nSCI-HUB：SCI-HUB当然是不可少的，以上平台提供的都是免费检索服务（WoS和Scopus除外），但是对于非OA期刊的文献下载是需要购买授权的，而SCI-HUB可以提供免费的文献下载服务。当然了，正是因为这一点，SCI-HUB一直在受到几大出版集团的打压。关于SCI-HUB是什么，看一下它的slogan就清楚了：the first website in the world to provide mass & public access to research papers。\n\n\n\n\n\n\n\n关于SCIE和SSCI：\n\n\n\n\n\n\nScience Citation Index Expanded (SCIE，科学引文索引，一般也统称为SCI) 历来被全球学术界公认为最权威的科技文献检索工具, 提供科学技术领域最重要的研究信息。共收录了8600多种自然科学领域的世界权威期刊，覆盖了176个学科领域。\nSocial Sciences Citation Index（SSCI，社会科学引文索引）是一个涵盖了社会科学领域的多学科综合数据库，共收录3000多种社会科学领域的世界权威期刊，覆盖了56个学科领域。\n\n\n\n\n其实，中文也有与SCIE和SSCI相对应的期刊收录库，分别是CSCD和CSSCI，具体如下：\n\n\n\n\n\n\n关于CSCD和CSSCI：\n\n\n\n\n\n\n中国科学引文数据库(Chinese Science Citation Database，简称CSCD)。创建于1989年，收录我国数学、物理、化学、天文学、地学、生物学、农林科学、医药卫生、工程技术和环境科学等领域出版的中英文科技核心期刊和优秀期刊千余种。\n中文社会科学引文索引（Chinese Social Sciences Citation Index，缩写为CSSCI，一般也称作“C刊”或者“南大核心”）。由南京大学中国社会科学研究评价中心开发研制的数据库，用来检索中文社会科学领域的论文收录和文献被引用情况，是我国人文社会科学评价领域的标志性工程。目前收录包括法学、管理学、经济学、历史学、政治学等在内的25大类的500多种学术期刊。\n\n\n\n\n\n\n\n1.2.2 文献检索方法\n检索文献是比较简单的一件事情，无非就是通过关键词查询文章，但是完成一项系统的文献检索工作并不容易，何为系统，那就是快速、准确且全面，想要做到这三点，就需要按照一定的程序进行文献的检索，也就是要制定好检索策略。\n检索策略通常可以分为三个部分：\n\n检索目的：就是确定检索主题和检索范围。\n\n检索主题：通常根据研究问题来确定相关联的文献，比如研究医疗保险支付方式对医生诱导需求行为的影响，那么关联的文献将主要包括“医疗保险支付方式的效果”和“医生诱导需求”两个方面。\n检索范围：通常包括数据库、时间、文献类型（Article或Review等）、文献语言（中文或英文）。通常要对多个数据库检索，如WoS、Pubmed、Science Direct、Wiley等均需检索到，原因其实在前文已经给出，因为不同的数据库收录的文献范围不一样，为了避免遗漏，最后对多个数据库检索。\n\n检索词的选择：检索词可以是标题/摘要关键字、作者、医学MeSH主题词、索引号等。\n\n合理运用AND、OR、NOT、*、？等检索符。\n对于词组的检索，合理使用“”，如Health Policy和“Health Policy”检索得到的结果将不一样，前者得到的是Health OR Policy的结果，后者得到的是Health AND Policy的结果。\n\n检索结果入选和排除标准：也就是文献的筛选标准\n\n数据库收录情况，如SCIE和SSCI收录，通常而言，收录标准较高的期刊，其发表的文章质量更高。\n重视高被引论文。\n尽量选择5年内的研究，如果是对学术史进行梳理，时间范围可以放宽。\n\n检索结果的记录与存档：\n\n做好检索词以及检索时间的记录，以便日后核对和溯源检索结果。\n合理运用数据库的批量导出功能，将不同的检索词得到的结果分类导出。"
  },
  {
    "objectID": "chapter_1.html#文献阅读",
    "href": "chapter_1.html#文献阅读",
    "title": "1  文献知识的获取与管理",
    "section": "1.3 文献阅读",
    "text": "1.3 文献阅读\n文献分为泛读和精读两种策略，因为文献的质量不一样，时间也有限，并非所有的文献都需要从头到尾的仔细阅读。\n\n泛读：顾名思义，就是泛泛而读，初步浏览Abstract、Introduction和Result之后，对研究问题、研究方法和研究发现有了解了，若发现其对自己的研究启示意义不大，即可无需再详细阅读。\n精读：即是对泛读过程中发现的好的文献，进行逐字阅读，除了完全掌握文献中的研究方法和研究结论外，也要对作者的图表展示、行文技巧进行学习，弄清楚作者是如何构思的，以便在自己写作过程中能够有所借鉴。\n文献阅读顺序：一篇科学论文的内容一般是按照Abstract、Introduction、Method、Result、Discussion和Conclusion的顺序印刷的（部分期刊除外），但是阅读时的顺序并非是这样，一般情况下先读Abstract，对全文有一个大致了解，这也是摘要的作用；然后再读Introduction，了解文章的研究问题、研究动机、研究的贡献；接着再需要读的应该是Result，从图表中掌握研究的主要发现，带着问题去看Discussion，了解作者是如何对研究结果进行解释，最后才是Method和Conclusion。"
  },
  {
    "objectID": "chapter_1.html#文献管理",
    "href": "chapter_1.html#文献管理",
    "title": "1  文献知识的获取与管理",
    "section": "1.4 文献管理",
    "text": "1.4 文献管理\n\n1.4.1 文献管理体系\n如前文述，文献管理规范对于保证研究质量十分重要，本节就介绍如何形成个人的文献管理规范，搭建文献管理体系。所谓规范或是体系，其实就是一套可以重复并推广的标准化流程，文献管理体系主要包括两部分：\n\n一是文献及原始论文管理\n二是阅读文献后的笔记管理\n\n这里介绍两种不同的管理文献的思路：\n\n项目式：也就是按照研究项目进行文献和笔记的管理，文献及笔记和其他项目文件存储在一起，这样的好处利于项目文件的管理，但是缺点也很明显，就是容易导致文献重复，且查找效率低。\n集中式：也就是将所有的文献和笔记集中管理（如 图 1.2），建立索引，我个人比较推荐此种方式，因为其具有较高的文献查找效率。\n\n\n\n\n图 1.2: 在Zotero中进行集中式文献管理\n\n\n\n\n\n\n\n\n关于文献的下载格式与命名规则\n\n\n\n\n\n\n建议统一下载为PDF格式，不推荐使用知网的CAJ格式，原因有二，一是CAJ格式需要CAJViewer阅读器才能打开，实在无必要多此一举，CAJViewer并没有比常见的PDF阅读器更优秀；二是CAJ是是中文本土格式，不利于对外进行文件交流。\n下载的文件最好建立起统一的命名规则，便于快速的查找并定位文献，如Author-Year-Short Title。 （Li and Zhou - 2005 - Political turnover and economic performance: the incentive role of personnel control in China）\n\n\n\n\n\n\n1.4.2 文献管理工具\n主流的文献管理工具基本如 表 1.1 :\n\n\n表 1.1: 常用文献管理软件比较\n\n\n\n\n\n\n\n\n\n软件名\n开发商\n中文支持\n推荐指数\n是否收费\n\n\n\n\nEndNote\n科睿唯安（Clarivate Analytics）\n一般\n***\n收费\n\n\nNoteExpress\n北京爱琴海乐之\n优秀\n*\n免费\n\n\nZotero\n开源社区\n一般\n***\n免费\n\n\nMendely\n爱思唯尔（Elsevier）\n一般\n**\n免费\n\n\n\n\n而对于文献笔记，可用的工具就非常多了，如印象笔记、Notebility、Obsidian等，Zotero 6.0版本以后也有非常好的文献笔记体验，推荐在选择文献笔记工具时应至少支持两项功能，一是可以云同步，二是支持Markdown。\n\n\n1.4.3 文献管理软件在论文写作中的应用\n任何科学研究都是建立在前人的研究基础上的，因此论文、报告等学术成果的写作都需要引用参考文献。参考文献的引用本来不复杂，但是由于不同的出版方对参考文献的引用格式各不一样，这就导致引用参考文献这件事情变得特别繁琐。\n对于文献数量较少时，花费一定的时间手动去调整还可以接受，若是文献数量多加之文稿篇幅长，那么如果继续沿用人工手动方式就会占用非常多的时间，比如下面两种场景，但凡想一想就会觉得头大：\n\n科研活动中很常见的一种情况就是论文的投稿，而投稿过程中因为拒稿、撤稿等原因需要更换不同的期刊的情况十分常见，而不同的期刊都有各自的文献引用格式要求，如果每次改投期刊时都去手动进行修改，将会是一件十分费事且低效率的行为。\n对于动辄上百页的学位论文、结题报告等，其参考文献等引用数量都非常可观，而通常都会经历多轮次的修改，若是采用GB-7714的序号引用格式4，当文献引用的顺序在修改过程中被调整，那么就需要对最后对参考文献进行重新排序，那么这也会是一件让人头疼不已的事情。\n\n幸运的是，以上工作可以由 表 1.1 中的文献管理软件完成。但是最好不要把这些软件仅当作文献引用的工具使用，其可以在整个文献知识的获取和管理中发挥非常有用的作用。"
  },
  {
    "objectID": "chapter_2.html",
    "href": "chapter_2.html",
    "title": "2  数据库操作",
    "section": "",
    "text": "在数据的发展历程中有过两次革命。第一次数据革命是近代科学诞生之时，实现了数据与科学研究的融合，数据在科学研究中的基础地位得到确立。对研究过程和结果赋予精确化的诉求，是近代科学的基本特征之一。在以数据为依据的研究范式中，数据的可靠性和准确性代表了研究的精确性，人们甚至将以数据为依据的实证研究作为判断 “科学”与“伪科学”的标准。第二次数据革命发生于因信息技术的发展而导致数据产生的速度和规模急剧发展之时，它不仅改变着科学研究范式，实现社会科学研究的定量化，也将促使经济、社会、军事等所有社会领域产生巨大的变革。1\n因此，掌握数据的分析能力是现代社会科学研究必须要掌握的技能。\n现实社会中产生的数据是纷繁复杂的，有时甚至是杂乱无章的，并非我们所设想的那样拿到数据就可以跑模型，它需要经过一系列的处理过程，这个处理过程一般称作数据操作（Data Manipulate或者Data Wrangle）。在定量实证研究中，接近70%到80%的时间都需要花费在数据的清洗和预处理阶段(Dasu and Johnson 2003)，然而很多青年学者都容易忽视这部分工作的重要性，而现实情况是，数据分析是没有捷径可以走的，机器学习也无法完成这部分工作，数据的质量决定了研究的质量，否则就是”Garbage in, Grabage out“。\n一个基于数据开展实证研究（不论定性或是定量）的基本流程至少都应该包含以下步骤：\n\n\n\n\nflowchart LR\n  A[采集 Collection]--> B[数字化 Digitalization]\n  B --> C{清洗 Clean}\n  C --> D[分析 Analysis]\n  D --> E[展示 Presentation]\n\n\n\n\n\n\n\n\n不过，随着互联网技术的发展，现阶段基本可以采用一些调查工具将采集和数字化两个阶段同时进行，比如最常用的问卷星、腾讯问卷，国外的话有SurveyMonkey和LimeSurvey，也可以基于RedCap搭建自有的调查系统。\n\n\n\n\n\n\n本章就主要介绍数据操作（Data Manipulate）的主要内容和方法，包括：\n\n\n\n\n\n\n数据的采集原则\n数据的批量读取\n数据库转换\n数据整合"
  },
  {
    "objectID": "chapter_2.html#数据的采集原则",
    "href": "chapter_2.html#数据的采集原则",
    "title": "2  数据库操作",
    "section": "2.2 数据的采集原则",
    "text": "2.2 数据的采集原则\n\n2.2.1 数据的分类\n在介绍数据采集原则之前，先说一下数据的分类，当然这里不讨论统计学上对数据的分类（如定性、定量、计数、计量等等），而是按照数据产生的途径将数据进行分类，我个人理解可以归纳为三类；\n\n调查数据（Survey data）：也就是通过针对个人或者机构的调查获得的数据，调查研究是开展社会科学研究的最主要方式，如田野调查、问卷调查、访谈等等。\n行为数据（Behavioral data）：即记录人或者机器行为的数据，如日常的网购数据、出行数据、够药数据、医保账户使用数据等等，最早应用于商业领域的用户行为分析。\n统计数据（Statistical data）：即政府或者机构对于涉及经济、社会和日常职能业务相关的指标进行按时间加总之后的数据，如金融情况、财政情况、教育资源、卫生资源等等，严格来说统计数据是基于调查数据和行为数据进行聚合、统计、校正之后形成的。\n\n我将三种数据的优缺点总结如 表 2.1\n\n\n表 2.1: 三种类型数据优缺点比较\n\n\n\n\n\n\n\n数据类型\n优点\n缺点\n\n\n\n\n调查数据\n1.容易组织，指的是小样本调查； 2.调查内容和质量相对可控； 3.分析简单。\n1.调查成本高，特别是大样本人群调查； 2.回忆偏倚、调查者偏倚、访员偏倚等不可避免。\n\n\n行为数据\n1.样本大，通常为海量数据； 2.时间和空间尺度细腻。\n1.获取较难，通常被机构和平台掌握； 2.对分析技能要求高； 3.通常混有非真实数据，如网购中的刷单，医保中的骗保等。\n\n\n统计数据\n1.较易获取。\n1.真实性难以验证； 2.时间和空间尺度粗糙； 3.指标不够丰富。\n\n\n\n\n\n\n2.2.2 数据库设计\n数据库（Database）设计主要针对的是将调查问卷电子化，虽然这个工作看似简单，但是其对于后续的数据清洗和数据分析十分关键，如果数据库设计得不够合理，会给后续工作带来很多麻烦。很多研究人员往往都忽视了数据库设计的重要性。\n\n\n\n\n\n\n数据库的设计应该至少要做到以下三点：\n\n\n\n\n数据库的设计应该要考虑到数据分析的需求，如变量的设计、变量类型的约束、逻辑核查规则、纵向或横向结构等。\n数据库的设计应该与调查问卷的设计同步进行，也就是当调查问卷定稿后，调查开始之前，就应该将电子化的数据库搭建完成，并在预调查阶段对数据库进行修订。\n数据库的设计应该要重视版本管理，且必须由专人负责管理，这一点对于长期历时多年的追踪调查十分重要，必须要做到问卷版本与数据库版本对应。\n\n\n\n下面针对以上三点分别举几例：\n\n关于数据库中变量设计最常见的就是多选题的变量设置，如对于以下一个多选题：\n\n\n您经常选择就诊的医疗机构有哪些？（多选）：1.村卫生室；2.诊所；3.卫生院；4.县区医院；5.省市医院。假设受访者回答；1，3，4。\n\n那么，数据库中变量设置有两种方法：\n\n设置1个变量（visit_institution），变量类型Char，长度为5，那么以上回答在录入时就是：134.\n设置5个变量（分别为：visit_village, visit_clinic, visit_thc, visit_county,visit_tert），变量类型分别为Num，长度为1，取值限定为0或1，那么以上回答在录入时就是，1，0，1，1，0。\n\n显然方法二对于描述性分析时更为友好。\n\n关于数据库的设计应该与调查问卷的设计同步进行，是为了保证提前对数据库和问卷进行检验和修订，及时对问卷中设置不合理的问题进行调整。若不同步进行就可能会带来一个问题，也就是在调查完成之后再进行数据库的建立，就很有可能在数据录入的过程中发现问卷中的问题难以在数据库中采取合理的方式进行录入。这一点对于采取电子化调查一样适用，因为问卷的形成通常是通过Word先完成的，电子化调查只是将数据调查过程和录入过程进行了合并。\n在说明数据库的版本管理为何重要之前，我们可以假设这样一个场景：\n\n\n有一项调查研究，需要历时5年，每一年进行一次追踪调查，每次调查完成之后进行数据的电子化，但是在第2年和第4年对问卷进行了补充和修订（这样的操作很常见）。\n\n那么，对于这样一个研究有几个问题是无法避免的：1）每次问卷调查人员会有变化；2）数据录入人员会有变动；3）数据库的建立人员会有变动。面对这样的问题，应该如何才能保证数据库管理的工作正常进行呢？我想一时间你很难想到很好的解决办法。\n在这里我给出三点建议：\n\n在问卷的footnote或者header里必须增加这些版本识别信息：起草人、生成日期、核准人、版本号。\n在数据库的footnote或者header里必须增加这些版本识别信息：起草人、生成日期、核准人、版本号、问卷版本号。\n数据的录入工作可以由不同的人完成，但是数据库的建立必须由专人负责，录入人员在录入时不得擅自建立数据库用于录入，且要保证录入问卷的版本号和数据库中记录的问卷版本号对应。\n\n\n\n\n\n2.2.3 变量名命名规范\n变量名的命名是指为变量赋予一个简短的字符，用于在统计软件作为变量的识别符号，一般需要遵循以下几项原则：\n\n尽量通过简短的单词或者符号来反映出变量的含义，推荐用英文单词，比如简单的：编号(id)、姓名(name)、性别(sex)，复杂一点的：家庭年收入(family_income)，这里不提倡用拼音的原因是为了保证研究的国际化，在研究过程中进行国际交流是十分常见的，如果采用拼音会在与国外的研究者进行交流时产生阻碍。同时也不提倡采用var1、var2这种没有规律的命名方式，因为难以对应出变量本身代表的含义。\n变量名不可用数字或者下划线开头，也不可包含. * ？- ！～等特殊字符。\n变量名的长度最好不要超过32个字符，如果过长会在统计分析的coding过程中增加不必要的键盘敲击量，增加额外的负担，影响效率。\n变量名可以采用驼峰命名法(familyIncome)、双峰命名法(FamilyIncome)、下划线法(family_income)等规则，重要的是保持一致，尽量不要在同一个数据库或者项目中混合采用不同的命名方式。\n\n\n\n2.2.4 数据字典\n数据字典（Data dictionary）也称作Codebook或者Specification，是指详细记录数据库中的变量名、变量内容、变量取值、变量标签、变量属性等信息，其作用主要是为了方便统计分析过程中检查和查看变量。\n数据字典可以手动整理，也可以通过统计软件中的函数生成，如Stata中的codebook命令，R中也有codebookr和datadictionary两个package。\n下面给出一个我之前整理的一个codebook的示例，如 图 2.1 和 图 2.2\n\n\n\n图 2.1: Codebook封面信息\n\n\n\n\n\n图 2.2: Codebook内容\n\n\n\n\n2.2.5 数据库建立工具\n\n入门工具：EpiData，这个基本是各个学校开设数据库管理课程都会讲到的一个工具，但是EpiData比较古老了，如果没记错应该是从2008年之后就再未更新过。\n普通线上工具：问卷星、腾讯问卷，能满足基本的数据库建立需求，但是对于较大型的研究一般不推荐。\n专业线上工具：推荐LimeSurvey，有Cloud版和社区版，社区版可以通过自行搭建在云服务器或者本地服务器上。\n综合数据与分析平台：推荐RedCap(Research Electronic Data Capture)，RedCap是由范德堡大学Paul Harris教授团队自2004年开发的一个成熟开源、安全可靠、网络化的在线临床研究和试验数据库管理程序，基本国内外知名高校均搭建有自己服务器，西安交通大学也有，如下：\n\n\n\n\n图 2.3: 西安交通大学开放研究数据平台"
  },
  {
    "objectID": "chapter_2.html#数据的读取",
    "href": "chapter_2.html#数据的读取",
    "title": "2  数据库操作",
    "section": "2.3 数据的读取",
    "text": "2.3 数据的读取\n数据读取是数据清洗和数据分析的第一步，为什么要单独把这一部分拿出来详细说明，是因为数据库管理工具通常与统计分析软件是分割开的，当然这里有一个例外，那就是MS EXCEL，很多人会把EXCEL既当成数据管理工具（虽然不推荐）又当作统计分析软件。\n数据读取虽然是很简单的一个操作，但是通常情况下会由于中文字符编码（Encoding）等问题导致在不同的统计软件之间转换时浪费一些时间和精力，所以有必要拿出来讲一讲。\n尽管现在的数据管理软件，不管是古老的EpiData或是时下流行的RedCap，都可以支持直接导出适用于不同统计分析软件的特定格式，比如dta（支持Stata）、sas7bat（支持SAS）、sav（支持SPSS），但是我个人还是强烈推荐以CSV格式作为中介，因为几乎所有的统计软件都支持CSV数据格式。\n\n2.3.1 统计分析软件\n顺带简单说一下目前在社会科学研究领域主要用的统计软件，如 表 2.2 ，我个人推荐R，因此后文关于coding部分均是基于R的。\n\n\n表 2.2: 社会科学研究中常用统计软件比较\n\n\n软件名\n说明\n推荐指数\n是否收费\n\n\n\n\nSPSS\n入门基础款\n*\n收费\n\n\nStata\n最流行款，学习门槛低，硬件要求低\n***\n收费\n\n\nSAS\n较流行，公共卫生领域研究人员使用多\n**\n收费\n\n\nR\n开源，学习门槛较高，第三方工具丰富\n***\n免费\n\n\nPython\n开源，学习门槛高，第三方工具较丰富\n**\n免费\n\n\n\n\n\n\n2.3.2 单个文件读取\n在R中读取不同的数据格式还是相对比较方便的，主要有两种方式：\n\n基于base包的，可以相对较容易的读取CSV，如read.csv()。\n基于第三方包的，主要涉及readr、readxl、haven，均包含在tidyverse系列中：\n\nreadr：主要用于读取CSV，如read_csv()\nreadxl：主要用于读取EXCEL的xls和xlsx格式，如read_excel()\nhaven：主要用于读取SAS、SPSS和Stata格式数据文件，具体如下，摘自官方文档：\n\n\nSAS: read_sas() reads .sas7bdat + .sas7bcat files and read_xpt() reads SAS transport files (version 5 and version 8).\n\n\nSPSS: read_sav() reads .sav files and read_por() reads the older .por files. write_sav() writes .sav files.\n\n\nStata: read_dta() reads .dta files (up to version 15). write_dta() writes .dta files (versions 8-15).\n\n\n\n\n由于以上package和函数都比较简单，参看官方文档之后就能熟练使用，因此不详细赘述，通过以上tidyverse系列的三个package，在R中基本可以应对绝大多数的数据读取问题。\n另外，对于数据量较大的情况，如百万或者千万级别的数据记录条数时，以上package中的函数会耗时较多，遇到这种情况，推荐使用data.table中的fread()函数处理，尽管data.table是不同于tidyverse的另一套对于数据库的操作体系，但是data.table同时具备data.frame的属性，兼容tidyverse语法。\n但是，fread()函数有一个不足之处，就是字符编码只支持UTF-8和Latin，无法支持中文字符常见的gb18030编码，略显遗憾。\n\n\n2.3.3 批量读取\n批量读取数据是在统计分析过程中很常见的一个需求，具体分为两种情况\n\n2.3.3.1 读取同一个文件夹中的多个文件\n如，在一个名为data的的文件夹中有365个CSV文件，分别命名为china_cities_20210101…china_cities_20211231，分别记录了全国所有地级市每一天的空气质量数据，现在需要对空气质量进行分析，那么第一步就需要将这365个文件读取并整合进一个数据库中，面对这个问题，你首先会想到如何处理呢？\n这里提供两种思路：\n\n根据文件命名，可以发现其中有一定的规律，也就是每个文件名自由最后的数字在变动，并且是按日期累加，那么就可以直接利用循环进行读取，因为思路较简单，代码略。\n以上这种情况并不常见，有时很难从文件名中发现规律，那么就只能先想办法获取所有文件名称，然后进行循环读取。\n\n\n由于文件全部为同一类型，因此可以使用list.files()函数进行获得（代码如下），其中path参数指定的是文件夹路径，pattern参数指定的是文件类型，其中*是通配符。\n剩下的工作就是循环读取了，可以使用if循环，但是不推荐，这里推荐使用purrr包中的map()函数，其实purrr是R自带的apply()函数族的高阶版本，由于是CSV文件，因此map()函数中FUNC参数使用readr包中read_csv()函数。\n最后一步就是将合并为一个data.frame，由于map()函数返回的是一个列表list，因此可以通过do.call()函数对list中的对象进行递归。\n也可以使用 map_dfr()直接省略第三步。\n\n\n#---------Mehotd 1---------------#\ncsv_list <- paste0(dir, list.files(path = dir, pattern = \"*.csv\"))\ndf_list <- purrr::map(csv_list, readr::read_csv, locale = locale(encoding = \"UTF-8\"))\n    \ndf <- do.call(rbind, df_list)\n\n#---------Mehotd 2---------------#\n\ncsv_list <- paste0(dir, list.files(path = dir, pattern = \"*.csv\"))\ndf <- purrr::map_dfr(csv_list, readr::read_csv, locale = locale(encoding = \"UTF-8\"))\n\n\n\n2.3.3.2 读取同一个EXCEL文件中的多个Sheet\n这种情况在现实中更为常见，处理思路基本同上，第一步依然是获取所有Sheet的名称。这里需要使用readxl包中的excel_sheets()函数，具体代码如下：\n\nsheet_name <- readxl::excel_sheets(\"datasets.xlsx\")\n\ndf <- purrr::map_dfr(sheet_name, readxl::read_excel, path = \"datasets.xlsx\")\n\n\n\n\n\n\n\n有两点需要强调一下：\n\n\n\n\n\n\n所有的文件路径中最好不要有中文.\n只有当不同的文件或者Sheet中的数据结构完全一致时，才建议直接使用map_dfr()函数将文件合并，如果不一致，建议使用map()函数先保存为列表list，然后根据不同的分析需求进一步处理。"
  },
  {
    "objectID": "chapter_2.html#整洁数据的含义",
    "href": "chapter_2.html#整洁数据的含义",
    "title": "2  数据库操作",
    "section": "2.4 整洁数据的含义",
    "text": "2.4 整洁数据的含义\n（Tidy data）"
  },
  {
    "objectID": "chapter_2.html#数据库转换",
    "href": "chapter_2.html#数据库转换",
    "title": "2  数据库操作",
    "section": "2.4 数据库转换",
    "text": "2.4 数据库转换\n这一节的内容其实和 Section 3.2 关联比较紧密，通常会用在Tidy Data的整理过程中。数据库转换其实就是对数据库的行（Row）和列（Column）进行的一系列操作，可以对数据库的结构进行改变。\n\n2.4.1 长宽类型转换\n也称作数据库的重构（Reshape），本质是对长数据（Long）和宽数据（Wide）形式之间进行转换。\n先简单说明一下长数据（Long）和宽数据（Wide）：\n\n长数据（Long）：不是指行比列多，虽然通常情况下这种数据结构确实是行比列多，但本质是指同一个个体2有多条观测值，比如面板数据（Panel data）就是长数据。英文表述可能更清楚：A “long” dataset contains more than one row per subject, and uses a unique ID to identify each subject.\n宽数据（Wide）：与长数据相对的形式，比较好理解了，就是每个个体只有一条观测值。\n\n从网上借了一张图，很清晰的说明了长款数据之间的差异，如 图 2.4：\n\n\n\n图 2.4: 长款数据示意图，数据来源：https://www.statology.org/long-vs-wide-data/\n\n\n在数据分析过程中，进行长宽数据之间的转换是经常会有的需求，这时你可能就会问了，比如呢！\n比如 表 2.3，一个微观调查的数据库，当在回归中需要控制时间固定效应时，你肯定希望数据是 表 2.3 (a) 形式，而当需要计算两次wave调查的income的差值时，你肯定希望数据是 表 2.3 (b) 形式。\n\n\n表 2.3: 长宽数据转换需求示例\n\n\n\n\n(a) 长数据示例\n\n\nID\nwave\nincome\n\n\n\n\n001\nWave 1\n1500\n\n\n001\nWave 2\n2000\n\n\n002\nWave 1\n2200\n\n\n002\nWave 2\n2800\n\n\n\n\n\n\n(b) 宽数据示例\n\n\nID\nwave1\nwave2\n\n\n\n\n001\n1500\n2000\n\n\n002\n2200\n2800\n\n\n\n\n\n\n常见的统计软件也都提供了数据库长宽转换的方法，只是易用性方面有所不同。比如，Stata中有reshape命令，而在R中，目前用得最为广泛的是tidyr包中的pivot_longer()和pivot_wider()函数，而之前这两个函数的功能是由另外两个函数：gather() 和 spread()提供的，由于后两个函数不容易被记住（这是Hadley Wickham的原话），所以Wickham就重写了这两个函数。\npivot_longer()和pivot_wider()函数的功能在官方文档中解释得非常清楚了，直接搬运如下：\n\npivot_longer() makes datasets longer by increasing the number of rows and decreasing the number of columns.\npivot_longer() is commonly needed to tidy wild-caught datasets as they often optimise for ease of data entry or ease of comparison rather than ease of analysis.\n\n以上两个函数的使用方式都特别简单和直观，参考官方文档即可，在此不再赘述，仅提供一个例子作为演示，具体如下。\n我们还是使用 表 2.3 的例子\n\n构造数据库\n\n\nsurvey <- data.frame(\"ID\" = c(\"001\", \"001\", \"002\", \"002\", \"003\", \"003\"),\n                     \"wave\" = rep(c(\"Wave 1\", \"Wave 2\"), 3),\n                     \"income\" = c(1500, 2000, 2200, 2800, 3000, 2400)                    \n                     )\n\n\nknitr::kable(survey)\n\n\n\n表 2.4: Dataframe of demo\n\n\nID\nwave\nincome\n\n\n\n\n001\nWave 1\n1500\n\n\n001\nWave 2\n2000\n\n\n002\nWave 1\n2200\n\n\n002\nWave 2\n2800\n\n\n003\nWave 1\n3000\n\n\n003\nWave 2\n2400\n\n\n\n\n\n\n\n长数据转换为宽数据\n\n\nlibrary(tidyr)\n\nsurvey_wide <- pivot_wider(survey, \n                           names_from = \"wave\", \n                           values_from = \"income\")\n\nknitr::kable(survey_wide)\n\n\n\n表 2.5: Dataframe of demo (Long to Wide)\n\n\nID\nWave 1\nWave 2\n\n\n\n\n001\n1500\n2000\n\n\n002\n2200\n2800\n\n\n003\n3000\n2400\n\n\n\n\n\n\n\n宽数据转换为长数据\n\n\nsurvey_long <- pivot_longer(survey_wide, cols = c(\"Wave 1\", \"Wave 2\"),\n                            names_to = \"wave\", \n                            values_to = \"income\")\n\n\nknitr::kable(survey_long)\n\n\n\n表 2.6: Dataframe of demo (Wide to Long)\n\n\nID\nwave\nincome\n\n\n\n\n001\nWave 1\n1500\n\n\n001\nWave 2\n2000\n\n\n002\nWave 1\n2200\n\n\n002\nWave 2\n2800\n\n\n003\nWave 1\n3000\n\n\n003\nWave 2\n2400\n\n\n\n\n\n\n\n\n2.4.2 数据库的连接\n数据库的连接分为两大类：\n\n横向连接，一般称为merge或者join，指的是对列（Column）的扩展。\n纵向连接，一般称为append或者concat或者union，指的是对行（Row）的扩展。\n\n\n2.4.2.1 横向联接\n在统计软件中一般使用merge()函数的较多，比如Stata和R中都具备此命令或者函数，不过现在使用join系列函数的比较多，可能更多的是为了和SQL语言统一，比如R中dplyr包提供了join_**()系列函数。\n横向连接一共分为六种情况，这里就用从网上借的一张 图 2.5 来说明:\n\n内联（inner join）\n左联（left join）\n右联（right join）\n全联（full join）\n外联（anti join）\n半连（semi join）\n\n从图中可以很清晰的看出不同联接方式的差异，dplyr包中的join_**()系列函数也十分友好，查看官方文档之后就可以很快熟练使用，在此不再举例说明。\n\n\n\n图 2.5: dplyr包中的join系列函数，数据来源：https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\n\n\n\n\n\n\n\n\n使用merge或者join时应该注意的问题\n\n\n\n\n在联接之前，应该先对需要进行合并操作的数据库结构有熟悉的了解，特别是联接键（Identify variable）的情况，是否有重复？两个数据库之间是何种关系？这些情况都需要提前掌握。\n在进行联接操作之前，应该已经对联接后的结果有一个基本的预期和判断，这样才能保证连接操作的正确性。\n\n\n\n\n\n\n\n\n\nOne more thing: 提供两种一次联接多个数据库的方法\n\n\n\n\n方法一：利用管道函数（%>%）实现\n\n\ndf <- df1 %>%\n        left_join(df2, by='a') %>%\n        left_join(df3, by='a')\n\n\n方法二：利用purrr包中的reduce()函数\n\n\ndf <- purrr::reduce(list(df1, df2, df3), left_join, by = \"a\")\n\n\n\n\n\n2.4.2.2 纵向联接\n纵向联接也有多种方法如R自带base包提供的rbind()函数，功能相对比较基本，以及dplyr包中的系列函数，如下：\n\nunion()：获得两个数据库行（Row）的并集，但会剔除重复的行。\nunion_all()：获得两个数据库行（Row）的并集，但会保留重复的行。\nintersec()：获得两个数据库行（Row）的交集，也就是只保留重复的行。\nsetdiff()：获得两个数据库行（Row）的差集，也就是只保留第一个数据库中与第二个数据不一样的行。\n\n具体示例如下：\n\n构造数据库\n\n\ndf1 = data.frame(\"id\" = c(1:6), \n                 \"product\" = c(rep(\"Oven\", 3), rep(\"Television\", 3)))\ndf2 = data.frame(\"id\" = c(4:7),\n                 \"product\" = c(rep(\"Television\", 2), rep(\"Air conditioner\", 2)))\n\n\nknitr::kable(df1)\nknitr::kable(df2)\n\n\n表 2.7: 示例数据集\n\n\n\n\n(a) 1\n\n\nid\nproduct\n\n\n\n\n1\nOven\n\n\n2\nOven\n\n\n3\nOven\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n6\nTelevision\n\n\n\n\n\n\n(b) 2\n\n\nid\nproduct\n\n\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n6\nAir conditioner\n\n\n7\nAir conditioner\n\n\n\n\n\n\n\n\n纵向联接\n\n\n#---#\ndf_union <- dplyr::union(df1, df2)\nknitr::kable(df_union)\n#---#\ndf_union_all <- dplyr::union_all(df1, df2)\nknitr::kable(df_union_all)\n#---#\ndf_ints <- dplyr::intersect(df1, df2)\nknitr::kable(df_ints)\n#---#\ndf_setdiff <- dplyr::setdiff(df1, df2)\nknitr::kable(df_setdiff)\n\n\n表 2.8: 纵向联接操作\n\n\n\n\n(a) union\n\n\nid\nproduct\n\n\n\n\n1\nOven\n\n\n2\nOven\n\n\n3\nOven\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n6\nTelevision\n\n\n6\nAir conditioner\n\n\n7\nAir conditioner\n\n\n\n\n\n\n(b) union_all\n\n\nid\nproduct\n\n\n\n\n1\nOven\n\n\n2\nOven\n\n\n3\nOven\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n6\nTelevision\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n6\nAir conditioner\n\n\n7\nAir conditioner\n\n\n\n\n\n\n\n\n(c) intersect\n\n\nid\nproduct\n\n\n\n\n4\nTelevision\n\n\n5\nTelevision\n\n\n\n\n\n\n(d) setdiff\n\n\nid\nproduct\n\n\n\n\n1\nOven\n\n\n2\nOven\n\n\n3\nOven\n\n\n6\nTelevision\n\n\n\n\n\n\n\n\n\n\n2.4.3 筛选观测值\n筛选观测值，或者称作筛选行（Row），是非常常见的数据库操作，但是也比较简单，主要是和条件语句（Condition）结合使用。\n在Stata里就是通过keep、drop、in等命令进行行筛选操作。\n在R里主要有两种方法：\n\nbase包自带的函数：如subset()函数，或者切片等方法。\ndplyr包里的filter()函数。\n\n\n\n2.4.4 筛选变量\n筛选便利，也称作筛选列（Column）或者筛选变量（Variable），同样是非常常见的数据库操作，一样也很简单。\n在Stata里也可以通过keep、drop等命令进行列筛选操作，是的你没看错，这两个命令可以同时进行行和列筛选。\n在R里主要有两种方法：\n\nbase包自带的函数：如subset()函数，或者切片等方法，是的你还是没看错，此方法可以同时进行行和列筛选。。\ndplyr包里的select()函数，特别可以注意一下与starts_with()、ends_with()、contains()等函数联用3，可以极大提高变量选择的效率。另外，select()函数现在有三个拓展函数select_all()、select_at()、select_if()，针对不同的使用场景，如下：\n\nselect_if()：如果需要一次性筛选所有的数值型变量，那么可以df %>% select_if(is.numeric)\nselect_at()：如果需要对符合特定条件的变量进行某种操作，那么可以select_at(vars(-contains(\"ar\")), toupper)"
  },
  {
    "objectID": "chapter_2.html#数据整合",
    "href": "chapter_2.html#数据整合",
    "title": "2  数据库操作",
    "section": "2.5 数据整合",
    "text": "2.5 数据整合\n（Aggregating DataFrames）\n分组计算\n\n\n\n\nDasu, T., and T. Johnson. 2003. Exploratory Data Mining and Data Cleaning. John Wiley & Sons, Inc."
  },
  {
    "objectID": "chapter_3.html",
    "href": "chapter_3.html",
    "title": "3  数据的清洗",
    "section": "",
    "text": "正如前文所述，数据清洗（Data Clean）和数据准备（Data Preparation）工作通常会占掉整个数据分析过程70%到80%的工作量，这一点基本被很多学者说认可，因此，应该清楚地认识到数据清洗工作的重要性。然而，很多学者，特别是还处在科研早期阶段的硕博士研究生，十分热衷于跑模型（Run model），拿到数据既不详细摸索数据结构，也不观察变量分布情况，简单的描述一下数据后，就开始跑各种回归，目的只在发现统计显著的回归系数，这是一个不太好的研究习惯。"
  },
  {
    "objectID": "chapter_4.html",
    "href": "chapter_4.html",
    "title": "4  数据的描述",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "chapter_5.html",
    "href": "chapter_5.html",
    "title": "5  数据的可视化",
    "section": "",
    "text": "一图胜千言\n文不如字，字不如表，表不如图"
  },
  {
    "objectID": "chapter_6.html",
    "href": "chapter_6.html",
    "title": "6  因果推断方法",
    "section": "",
    "text": "先说为什么会想着写这本书：\n\n一是因为这么多年学习和研究的领域是卫生管理（Health Management）与卫生政策（Health Policy），在研究方法方面一直是受卫生统计学或者应用统计学的影响比较大，但是近些年随着因果推断革命（Causal Inference Revolution）对各个学科在研究方法上的强烈冲击，特别是社会科学，卫生政策领域的研究在方法学上也受到计量经济学非常大的影响，其中被国内外学者使用最广泛的应该就当属双重差分（Difference-in-Difference）了；\n二是因为我本人的学科背景并非为经济学，所以很多计量经济学的方法都是在当研究需要时一点一点自学的，尽管计量经济学与统计学存在很多的渊源，但毕竟理论体系不太一样，这就导致在碎片化自学过程中难免会犯一些错误和走一些弯路，再加上这些年记忆力不比从前，对”烂笔头”的依赖越来越高，因此就索性把这些年攒下来的一些笔记，加上最近更新的一些关于DID的知识点整理出来了；\n三是最近几年随着微信公众号和知乎等迅速成为各类信息传播的主要平台，国内科研工作者慢慢的很少”逛”论坛、发帖子、写博客了，有的转战公众号开设自己的账号，有的开始团队运营，这些平台给科学研究提供了不少便利性，扩大了知识获取面，降低了知识检索的时间成本，但是其弊端也毕竟明显，一方面是导致很多人对前沿科学咨讯的获取基本来源于公众号，很少阅读原始文献，另一方面是公众号或者知乎平台分享的信息和知识点没有经过同行评议，其实不乏有错误之处，若不加思索的参考或者照搬很容易导致研究方法误用，特别是对于尚处于学术生涯初期的研究生。\n\n时刻更新自己的知识体系是科研工作者日常生活中的一部分，保持对新知识的学习热情应该是一个合格的打工人的基本素养吧。DID早在20世纪80年代就被提出，其基本原理很简单，但是如果据此就认为采用DID识别框架的方法都很简单就错了，随着Joshua D. Angrist、Guido W. Imbens、Gary King、Susan Athey等经济学和统计学家对因果识别方法体系的贡献，时至今日，DID已经衍生出了多种多样的类型来使用不同的研究设计和数据结构，可以说已经是一大方法家族了吧。\n写这本书就是打算能给卫生管理和卫生政策方向的年轻硕士和博士生在研究中以参考，因此这本书注重实践性。在这本书里面打算既介绍DiD的原理也给出完整的分析方法和过程，由于目前我还写不清楚所以不会涉及详细的数理推导。\n本书的主要参考资料如下：\n\n\nAngrist, & JoshuaDavid. (2010). Mostly Harmless Econometrics: An Empiricist Companion. Princeton University Press.\n伍德里奇, & 费剑平校. (2010). 计量经济学导论: 第4版. 中国人民大学出版社.\nScott Cunningham. (2020). Causal Inference: The Mixtape. Yale University Press.\nhttps://lost-stats.github.io/Model_Estimation/Research_Design/event_study.html"
  },
  {
    "objectID": "chapter_6.html#断点回归",
    "href": "chapter_6.html#断点回归",
    "title": "6  因果推断方法",
    "section": "6.2 断点回归",
    "text": "6.2 断点回归"
  },
  {
    "objectID": "chapter_6.html#工具变量",
    "href": "chapter_6.html#工具变量",
    "title": "6  因果推断方法",
    "section": "6.3 工具变量",
    "text": "6.3 工具变量"
  },
  {
    "objectID": "chapter_3.html#整洁数据的含义",
    "href": "chapter_3.html#整洁数据的含义",
    "title": "3  数据的清洗",
    "section": "3.2 整洁数据的含义",
    "text": "3.2 整洁数据的含义\n整洁数据（Tidy Data）是由Posit公司（之前名称为Rstudio）的首席科学家Hadley Wickham提出的，初衷是为了简化数据清洗的流程，提高数据清洗效率。Wickham (2014) 认为Tidy Data更易于操作、建模和可视化，并且应该具备三种特征：\n\nEach variable is a column.\nEach observation is a row.\nEach type of observational unit is a table.\n\n打眼一看，你可能觉得上面三句话说的是废话，心里想着大家不是都是这么建数据库的吗？其实不然，比如可以看下面的例子。\n如 表 3.1 是日常工作和研究中非常常见的建数据库方式，但是这样的方式对于统计分析来说并不友好，比如当你想要知道每一科课考试的最大值最小值时，你通常需要重复同样的操作。\n\n\n表 3.1: 常见数据结构示例一\n\n\n姓名\n数学\n语文\n英语\n\n\n\n\n张三\n80\n90\n95\n\n\n李四\n88\n95\n97\n\n\n\n\n再比如，当有多次考试的成绩需要记录时，很多人会选择 表 3.2 这样的方式，这样的结构在数据分析时会更不友好。\n\n\n表 3.2: 常见数据结构示例二\n\n\n姓名\n第一次模拟考试\n\n\n第二次模拟考\n\n\n\n\n\n\n\n数学\n语文\n英语\n数学\n语文\n英语\n\n\n张三\n80\n90\n95\n85\n92\n97\n\n\n李四\n88\n95\n97\n87\n90\n98\n\n\n\n\n符合Tidy Data要求的建立数据库的方式应该如 表 3.3 所示，在这种结构下，结合Group by等操作，会给数据分析带来很大的便利。\n\n\n表 3.3: 常见数据结构示例三\n\n\n姓名\n考试\n学科\n得分\n\n\n\n\n张三\n第一次模拟考\n数学\n80\n\n\n李四\n第一次模拟考\n数学\n88\n\n\n张三\n第一次模拟考\n语文\n90\n\n\n李四\n第一次模拟考\n语文\n95\n\n\n张三\n第一次模拟考\n英语\n95\n\n\n李四\n第一次模拟考\n英语\n97\n\n\n张三\n第二次模拟考\n数学\n85\n\n\n李四\n第二次模拟考\n数学\n87\n\n\n张三\n第二次模拟考\n语文\n92\n\n\n李四\n第二次模拟考\n语文\n90\n\n\n张三\n第二次模拟考\n英语\n97\n\n\n李四\n第二次模拟考\n英语\n98\n\n\n\n\n关于Tidy Data的内容还有很多，具体可以详细参考 Wickham (2014) 的文章，这里就不进行搬运工作了。在设计数据库的时候，只需要记住不要把数据展示形式当成数据库的结构形式就可以。\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "参考文献",
    "section": "",
    "text": "Dasu, T., and T. Johnson. 2003. Exploratory Data Mining and Data\nCleaning. John Wiley & Sons, Inc.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "chapter_3.html#section-3-2",
    "href": "chapter_3.html#section-3-2",
    "title": "3  数据的清洗",
    "section": "3.2 整洁数据的含义",
    "text": "3.2 整洁数据的含义\n整洁数据（Tidy Data）是由Posit公司（之前名称为Rstudio）的首席科学家Hadley Wickham提出的，初衷是为了简化数据清洗的流程，提高数据清洗效率。Wickham (2014) 认为Tidy Data更易于操作、建模和可视化，并且应该具备三种特征：\n\nEach variable is a column, , and that column contains one “type” of data.\nEach observation is a row.\nEach type of observational unit is a table.\n\n打眼一看，你可能觉得上面三句话说的是废话，心里想着大家不是都是这么建数据库的吗？其实不然，比如可以看下面的例子。\n如 表 3.1 是日常工作和研究中非常常见的建数据库方式，但是这样的方式对于统计分析来说并不友好，比如当你想要知道每一科课考试的最大值最小值时，你通常需要重复同样的操作。\n\n\n表 3.1: 常见数据结构示例一\n\n\n姓名\n数学\n语文\n英语\n\n\n\n\n张三\n80\n90\n95\n\n\n李四\n88\n95\n97\n\n\n\n\n再比如，当有多次考试的成绩需要记录时，很多人会选择 表 3.2 这样的方式，但是这样的结构在数据分析时会更不友好。\n\n\n表 3.2: 常见数据结构示例二\n\n\n姓名\n第一次模拟考试\n\n\n第二次模拟考\n\n\n\n\n\n\n\n数学\n语文\n英语\n数学\n语文\n英语\n\n\n张三\n80\n90\n95\n85\n92\n97\n\n\n李四\n88\n95\n97\n87\n90\n98\n\n\n\n\n符合Tidy Data要求的建立数据库的方式应该如 表 3.3 所示，在这种结构下，结合Group by等操作，会给数据分析带来很大的便利。\n\n\n表 3.3: 常见数据结构示例三\n\n\n姓名\n考试\n学科\n得分\n\n\n\n\n张三\n第一次模拟考\n数学\n80\n\n\n李四\n第一次模拟考\n数学\n88\n\n\n张三\n第一次模拟考\n语文\n90\n\n\n李四\n第一次模拟考\n语文\n95\n\n\n张三\n第一次模拟考\n英语\n95\n\n\n李四\n第一次模拟考\n英语\n97\n\n\n张三\n第二次模拟考\n数学\n85\n\n\n李四\n第二次模拟考\n数学\n87\n\n\n张三\n第二次模拟考\n语文\n92\n\n\n李四\n第二次模拟考\n语文\n90\n\n\n张三\n第二次模拟考\n英语\n97\n\n\n李四\n第二次模拟考\n英语\n98\n\n\n\n\n关于Tidy Data的内容还有很多，具体可以详细参考 Wickham (2014) 的文章，这里就不进行搬运工作了。在设计数据库的时候，只需要记住不要把数据展示形式当成数据库的结构形式就可以。\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "chapter_3.html#sec-3-2",
    "href": "chapter_3.html#sec-3-2",
    "title": "3  数据的清洗",
    "section": "3.2 整洁数据的含义",
    "text": "3.2 整洁数据的含义\n整洁数据（Tidy Data）是由Posit公司（之前名称为Rstudio）的首席科学家Hadley Wickham提出的，初衷是为了简化数据清洗的流程，提高数据清洗效率。Wickham (2014) 认为Tidy Data更易于操作、建模和可视化，并且应该具备三种特征：\n\nEach variable is a column, , and that column contains one “type” of data.\nEach observation is a row.\nEach type of observational unit is a table.\n\n打眼一看，你可能觉得上面三句话说的是废话，心里想着大家不是都是这么建数据库的吗？其实不然，比如可以看下面的例子。\n如 表 3.1 是日常工作和研究中非常常见的建数据库方式，但是这样的方式对于统计分析来说并不友好，比如当你想要知道每一科课考试的最大值最小值时，你通常需要重复同样的操作。\n\n\n表 3.1: 常见数据结构示例一\n\n\n姓名\n数学\n语文\n英语\n\n\n\n\n张三\n80\n90\n95\n\n\n李四\n88\n95\n97\n\n\n\n\n再比如，当有多次考试的成绩需要记录时，很多人会选择 表 3.2 这样的方式，但是这样的结构在数据分析时会更不友好。\n\n\n表 3.2: 常见数据结构示例二\n\n\n姓名\n第一次模拟考试\n\n\n第二次模拟考\n\n\n\n\n\n\n\n数学\n语文\n英语\n数学\n语文\n英语\n\n\n张三\n80\n90\n95\n85\n92\n97\n\n\n李四\n88\n95\n97\n87\n90\n98\n\n\n\n\n符合Tidy Data要求的建立数据库的方式应该如 表 3.3 所示，在这种结构下，结合Group by等操作，会给数据分析带来很大的便利。\n\n\n表 3.3: 常见数据结构示例三\n\n\n姓名\n考试\n学科\n得分\n\n\n\n\n张三\n第一次模拟考\n数学\n80\n\n\n李四\n第一次模拟考\n数学\n88\n\n\n张三\n第一次模拟考\n语文\n90\n\n\n李四\n第一次模拟考\n语文\n95\n\n\n张三\n第一次模拟考\n英语\n95\n\n\n李四\n第一次模拟考\n英语\n97\n\n\n张三\n第二次模拟考\n数学\n85\n\n\n李四\n第二次模拟考\n数学\n87\n\n\n张三\n第二次模拟考\n语文\n92\n\n\n李四\n第二次模拟考\n语文\n90\n\n\n张三\n第二次模拟考\n英语\n97\n\n\n李四\n第二次模拟考\n英语\n98\n\n\n\n\n关于Tidy Data的内容还有很多，具体可以详细参考 Wickham (2014) 的文章，这里就不进行搬运工作了。在设计数据库的时候，只需要记住不要把数据展示形式当成数据库的结构形式就可以。\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "chapter_2.html#数据聚合",
    "href": "chapter_2.html#数据聚合",
    "title": "2  数据库操作",
    "section": "2.5 数据聚合",
    "text": "2.5 数据聚合\n（Aggregating DataFrames）\n分组计算\n\n\n\n\nDasu, T., and T. Johnson. 2003. Exploratory Data Mining and Data Cleaning. John Wiley & Sons, Inc."
  }
]